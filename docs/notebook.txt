Calibration todo:
 -- Rotate source 45 degrees so that our workspace is more centered on
    the source field strength.  Wondering if the reason we have poor
    accuracy at -Y extreme is just low signal strength, and perhaps
    also unfavorable coupling configuration.


Input board design/capture tasks
 -- input network (x)
 -- amplifier (x)
 -- antialias (x)
 -- balancer +
 -- reference buffer (x)
 -- digital driver/receivers
 -- local power conditioning.  Get 3.3V logic supply from 5V via a
    LDO.  We also want passive filtration on the 5V supply to
    keep digital noise from the main board out.  Likewise on the
    analog supplies, maybe 3-wire CM filter?
 -- connector

Rough component placement to get idea of board areas:
 -- chassis ground input network
 -- input channel: protection, amp, antialias
 -- input channel: balancer, ADC, reference driver
 -- board entry filtering for power and reference
 -- digital tx/rx

Board floor plan w/ preliminary sizes, connector locations

Layout tasks:
 -- Power and ground layout
 -- lay out a channel
 -- common support on main ground region
 -- duplicate channels
 -- extra test features on channel 1

Hardware todo:
 -- Prototype antialias?
 -- Prototype reference buffer?
 -- Build prototype reference, used until main board is complete.

Eval box issues:
 -- Rework for input board testing in advance of main board.
 -- Set up for 3 DAC outputs with LPF.
 -- Test for EMI issues, power adapters coupling into signal wiring.

Other hardware issues:
 -- Re-wind Z source coil
 -- Buy spare amp and supply for driver box?  Parts express has driver
    still , but Q(@#*&$*( tax exemption needed.
 -- Stage mount reinforcement
    - Bolt/glue/mortar
 -- Stage refinements: Ry axis, slider mod for Rx axis.

Don't forget:
 -- Some scaling problem in the spectrum window when looking at "Chart high"?
    The plot moves way up and down depending on the FFT parameters, window,
    etc., and seems basically too low.
 -- Could probably reduce high rate noise about 3 dB by only using the
    in-phase component of the signal, then adjusting the phase at low
    rate to allow for actual phase shift.
 -- What effect does placing the output filter resonance at the
    carrier frequency have on driver noise?  Does this help or hurt?

________________
Levels, etc:

The present theory re. output levels is that the overall gain from digital
code to coil voltage should be the same across channels, and also this level
should be set by thermal limits (and near DAC full scale).  Turn off high
carriers and then measure voltage at coil as 1.15 V RMS (with 0.05 peak low
carrier amplitude).  See [26 Sep 19]

I have been setting the UR44 DAW output levels at -6 dB, but with
drivers set for this level the PCM1794A board doesn't have enough
output.  So you have to readjust the gain pot on the driver board when
you switch between.  To switch, you can just set the current readback
to 3.275 A.

Standardize noise floor measurements with carrier at -4 dBFS at 120mm.
Set the X-X gain to get the -4 dB, then switch to the sensor loopback
cable with HP353A attenuator to set all the gains equal.  Use the raw
data time domain RMS because the spectral measurements are iffy due to
unsynch issues.

To minimize unsynch errors in spectral measurement, set spectrum controls to
flat-top order 1 blocking 2 averaging 0, and then show the levels for "Trace
display", not "Carrier levels" (which uses fixed FFT parameters).

Because of time skew issues you really need to evaluate the test box
in "TCP only" mode, otherwise you get gratuitous LF noise, wrong
spectral peaks, etc.  This means you have to use "X X no-sense" mode,
and re-calibrate the distance (which is different than in vector
mode).  I am going to test the UR44 in this same mode for a
head-to-head comparison.  The noise is somewhat lower in vector mode
because of averaging across carriers.

With UR44, set the YZ carriers to zero.  This better matches what "TCP
only" mode is doing (and doesn't include any source channel
interaction effects that might exist, but are not there in TCP only
mode).



________________
Send ADC data to labview:
# Has laptop IP address wired in, may change
~/labview_connect &
________________
Install new FPGA bit file:

On windows:
cd Documents/Work/microzed/verilog/vivado/xillydemo.runs/impl_1
scp xillydemo.bit ilemt@192.168.1.15:/tmp/

On ilemt:
sudo mount /dev/mmcblk0p1 /mnt/card
cd /mnt/card
sudo cp /tmp/xillydemo.bit .
sudo reboot
________________
VNC access:
# ilemt.local works on home net
ssh -L 5905:127.0.0.1:5901  ilemt@192.168.1.15

on ilemt:
vncserver -compatiblekbd
password BogPickle
________________


[8 Dec 19]

 -- OPA2210 (dual), super beta, En=2.2 nV/rtHz
  ==> Woah, this is super-cheap compared to LT1028, etc.  2.2 nV/rtHz,
      but the lower current noise.  Not super fast, more like
      OP27. $4.56 in quantity 20, and a dual!
 -- OPA1612, $5.76 quantity 10, dual, 1.1 nV/rtHz.  Current noise is a
    bit high at 1.7 pA/rtHz
 -- AD8599 (dual), low distortion, En=1 nV/rtHz, but current noise is
    about double LT1028. Less than 1/2 price of LT1028 per-amp.  $7.40
    quantity 10.
 -- OPA1656 (dual), CMOS, En=2.9 nV/rtHz @ 10 kHz, 1/f corner ~2 kHz,
    OPA2156 similar. Even more cheaper, $2.60
 -- ADA4004-2, available in 1,2,4, $6 in quantity 10. 1.8 nV, 1.2 pA.
    Probably too slow, doesn't have a distortion spec.
 -- LM4562, dual, 2.7 nV, 1.6 pA. $1.52 @10.  Strong drive
    capabilities, low distortion.  Open loop Zout 10 ohms.
 -- NJM4580, dual, $0.59 in quantity 10.  Seems to be about 3 nV/rtHz,
    with 1/f corner ~20 Hz.  Audio, no bias current compensation,
    which is likely a win for current noise, but no spec on that.  No
    max noise spec.
 -- NJM2068, $0.66 @ 10, weighted input noise is about 1/2 of NJM4580,
    bias current is higher, with wide upper bound.  Seems aimed at <
    1K source impedance.  Not clear what is going on with the max
    noise.  The NJM2608MD ("D" suffix) is noise tested, but number is
    confusing because of different test condition.  "D" is available
    from Mouser, price is lower there, $0.54 @ 10.


The NJM2114 is obsolete.

Also, these are all available in SOIC-8

It seems like there are enough dual options in the right ballpark that
this is the way to go.  Also, prices are much lower for everything
that is not the LT1028.  We can save space and go down 4x in
price/amp.

I was wondering if it might make sense to not use a differential amp
to drive the ADC.  Probably doesn't make that much difference one the
way or the other, but it seems that in the 1 to 2 mode the difference
amps don't have super low distortion in comparison to high-end audio
opamps.  And it might be a bit cheaper.  It would require more passive
components.  I think it would be best to use a DC level shift rather
than AC coupling, since this can be done at a low impedance node.
Just use inverting and non-inverting amp driven by the antialias
output.  The non-inverting needs a voltage divider at the input to get
gain 1/2.





[7 Dec 19]

Summary:
 -- Input overload issues.  Input overload can happen very easily and
    for a long time.  All you have to do is put the sensor right next
    to the source.  Power dissipation in resistors, limiting at ADC
    inputs and need for high sinking capacity in reference driver.
    +9/-4 supplies seem like a good idea for the ADC driver to
    minimize current limit and power dissipation 
 -- We really do need pretty much +/- 15 supplies to swing 1K to +/-
    10V.  Non RRO parts like the LT1028 run out of gas 3V from the
    rail, and I imagine distortion may start deteriorating before
    then.
 -- More on fully-differential amps, comparing noise and distortion.
 -- Built-in self test scheme.


During input overload the input and feedback resistors can see
possibly arbitrarily long application of nearly the supply voltage.
Especially with the matched 0.1% resistors, we want to make sure we
are not degrading the accuracy.  These resistors do not see the full
30V rail-to-rail voltage, only the 15V single rail.

15V swing is a worst-case, because we are not using rail-to-rail
outputs. Looking at eg. the LT1028, at 600 Ohm load, the typical swing
is +/- 11V, and they only guarantee +/- 9V over temperature.  However
at the first stage output we are also only normally getting +/- 5V
swings on each arm (10V differential).  With >= 2K load, the minimum
swing is 11.5 V.  (But you can see we really do need 15V supplies to
get the 10V swing.)

For eg. 1K:
    13^2/1K = 169 mW

The first stage resistors are not critically matched for CMR, but
drift is a gain error.  If we use 12V, which seems somewhat
conservative for the LT1028 with a 500 Ohm load, then the power
reduces to 0.288W, which is still greater than by 1/4, but not vastly
greater.

0603 resistors are typically rated for 0.1 or 0.125 watts.  You can
get thick film resistors in 0603 rated at up to 1/2 W, but these may
not be ideal for precision.

For >= 1/4 watt as standard, in a 1% resistor, it seems that 1206 is
the sweet spot.  You do not get any higher rating at 0805.  It does
seem that 1206 is a good idea for the 500 Ohm resistors in the first
stage.  For the second stage, I'm not sure.  We do want to preserve
matching, and the 0.1% 1206 array I picked is rated 1/8 W. Using 4
discrete 1204 chips probably degrades initial matching a bit, and
takes up more space.

A related concern in the ADC driver is what happens at the output on
input overdrive.  The LTC2500 has clamp diodes on the analog inputs
between ground and VREF.  These are spec'd as being able to handle 100
mA without latchup.  The absolute max ratings only include 0.3V
outside these rails, but I think it likely these are PN diodes, which
will not conduct significantly until 0.7V.

Also, the absolute max on the reference input is 6V.  We need to make
sure the reference voltage does not run away if one or more drivers
are dumping their current limit into the ADC input.  If wen dump the
current somewhere else, it seems like a clamp on VREF would be a good
idea.

Fig 27 on the THS4131 datasheet is a bit puzzling.  It seems that,
especially on 15V supplies, the output current current starts bogging
down when you are still well away from the rail. (?)  One thing is
that the feedback resistors is given as 1K, so that is always present
as a load, in parallel with the external load resistance.  I guess
this is not so different from the LT1028 though, we reach about 12-13V
with 1K load.  It's just that at light loads it can pull 1-2 volts
more.

It does seem that the output is not as stiff at +/-5.  At the same
load resistance the swing is lower, even at small swings. But I guess
this just reflects that the buffer starts to run out of gas 3V from
the rails, which is only 2V at 5V supply.  Performance specs like
distortion are noticeably better at +/- 15, but the difference is not
huge, 1-4 dB.  There are actually a lot of distortion plots for
single-to-double operation, which is our use case.

So, I should be saying LTC2512-24, but it seems that the noise and
distortion specs are almost identical to the LTC2500-32 with flat
filter and decimation in the 4-32 range covered by the LTC2512.

The THS4131 distortion performance is seems weak compared to what is
attainable with the LTC2500-32.  Instead of -120 dB THD, we have maybe
-100, or as low as -95 at 10V differential output.  If I extrapolate
the line down to 10 kHz, we might reach -110 dB.
 ==> The LTC2512 distortion spec is at 2 kHz, where it is -120 dB, but
     at 10 kHz it is -110 dB, not really any different from the
     THS4131. 

Because the ADC does not (AFAICT) have active circuits in the signal
path it may be that distortion is limited mainly by the driver, even
in the ADC datasheet spec.  In audio terms, -100 dB is 0.001%
distortion, which is not too shabby.

With the recommended LTC6363, comparing the two plots in the LTC2512
datasheet, it does not seem that the 2.9 nV/rtHz noise is limiting
wrt. broadband noise floor, which is pretty much what you would expect
given that the ADC RTI noise is 30 nV/rtHz.  If you squint you can
perhaps see the high 1/f noise corner, which is ~20 kHz!  While the
pinout is the same, it does not come in SOIC, only MSOP.  Possibly
this would be a reason to use the MSOP package for second source.

There's also a possibly thermal concern in the driver if it is stuck.
The short-circuit current specs give a somewhat alarming view, since
the currents can be pretty high, and do not have any specified max.
eg. with THS4131, with 7 Ohm load, the typical output currents are 55
mA @ +/-5 supply and 85 mA at +/- 15V.  85 mA @ 15 V is 1.3 W!

But the swing plot gives a different story.  At +5V we can swing maybe
a 150 Ohm resistor with +/- 15 supplies.  This is 30 mA, not 85.
However, in the negative direction, with +/- 15 supplies, our negative
swing is zero, and we might well see the higher current.

The thermal performance in VSSOP with EP is at least 50% better than
the SO, and is 1.7W at 25C ambient, which actually tolerates the short
circuit current.  One would hope that they set the current limit to
not smoke the part under typical thermal conditions.

It might make sense to use reduced supplies for the driver because
this would definitely reduce the power dissipation, and may also
reduce the maximum output current.  On the LTC2512 datasheet they
suggest -3/+8, or 10V total with 3V headroom.  The lower supplies do
also open out the part selection a bit.  The supplies on the eval
board are also lower, though I no longer recall if they are asymetric.
There would not be too much complexity for the special supplies
because there would be just one instance per board, in common across
the 3 channels.

I guess we may be OK without it, but I am kind of inclined to add a
diode clamp on the driver output, rather than relying on the ADC
internal diodes.  In conjunction with the series resistors this would
reduce the current into the ADC to a quite small value.  If the upper
limit is set by a zener clamp then we would avoid dumping large
amounts of current in to VREF.  eg. if three channel drivers were
saturated then we could be dumping 150 mA into VREF, which should in
no case swing above 6V.

We want some sort of clamp on VREF anyway to prevent VREF from going
too high under startup or fault conditions.  This is a bit mushy with
standard zener tolerances, though.  A 5.1V zener may have a threshold
as low as 4.8V, which seems bad.  But a 5.6V zener may have a
threshold as high as 6V.  And the voltage is going to be a lot more at
higher currents, though this is not characterized.  Going off of TVS
specs, it seems likely that we are talking 1-2 more volts. So a zener
clamp we can't guarantee a safe condition when current is sourced into
VREF, even if located at the ADC.  It is still is worth throwing a
5.6V zener or 5V TVS on the reference buffer output, for eg. startup.

We could put eg. BAV99 at the buffer outputs, connected to VREF, which
would also keep the driver current limit out of the ADC, but then we
are still sourcing into VREF.  This does not seem necessary according
to the 100 mA note on the ADC, but is simple and cheap.
 ==> I just realized that overdrive on all channels is easy to
     get in normal operation, just move the sensor close to the
     source.  So this is not an obscure fault condition at all.  We
     want to make sure that nothing is damaged or overheats if someone
     leaves the sensor sitting on top of the powered-up source for a
     month.

This seems like a good reason to use both reduced driver supplies and
clamp diodes.  I think it is OK to have the driver sitting in current
limit as long as the dissipation is not excessive.  The external clamp
will keep us from sourcing large current into the ADC, which might
cause long-term issues such as electromigration, and the heating could
possibly affect performance as well.  If each input is getting 50 mA @
0.8V, that's 80 mW.

Unassisted, the reference buffer can only sink maybe 20 mA, so I added
a class-A transistor buffer.  It seems that TO-252-3 is the most
common package in this range, with many options.  MJD32CGOS-ND is
electrically similar to the TIP32, but is much smaller, a 1/4"
package, and is rated for 1.5W.  5V @ 150 mA is 0.75W, so this is not
very conservative.  However we expect the current to be rather less
than this, especially with the reduced rails.  At 4V from the rail the
THS4131 is typically more like 20 mA, reading from the datasheet
plots.


I was unable to stop myself from coming up with a scheme for built-in
test.  We would want an inexpensive scheme for introducing a signal at
the input which does not compromise the performance during operation.
This is a bit tricky because we do not want to introduce more than pF
of capacitance or any signficant series or shunt resistance.  The idea
is to have diode-resistor in series going to each input, both
polarities.  So 4 diodes and 4 resistors per channel (2 on each line).

When not testing these are driven into reverse bias at the opposite
rail, so there is only the reverse bias capacitance and leakage
current.  The leakage is not a big deal because our impedance is low.

The resistors are set to provide a nice current at 10V, maybe 10K?
Then we can test with the sensor present, and in doing so we check the
continuity.  If we use a square edge we should also get an initial
shoot up toward the applied voltage, because of the sensor inductance.
(Antialias filter will limit our ability to see this.)  Given that the
sensor impedance is known, we can also check input calibration even
with the sensor attached.

Given that the resistor approximates a current source, where 10V may
create a 50 mV swing, the any diode drop error is attenuated 200x.

We could just run these BIST lines off the board (with filtering), but
the diode drop is an error term.  We can largely compensate this by
having onboard a "precision rectifier" style driver.  This will
completely cancel the forward drop on one channel, and the others will
match as well as the diodes are matched.  A load resistor can pull the
diodes all the way to the rail when the BIST is off.  (Want to filter
those rails well to avoid coupling in noise via the diode
capacitance.)

The BIST lines can be driven by 4 DACs on the main board, also with
precision rectifier buffers, to drive to the DAC voltage or the
opposite rail.  The buffer ground reference (eg. +in for inverting)
can be offset slightly so that we can use a 0..5V DAC, with a zero
code causing the buffer to flip "off".  Then the card buffers are
unity gain.  We can drive in an arbitrary signal, though there would
be fairly nasty crossover distortion on passing through zero because
the BIST line has to swing back from the rail.  But that is avoided if
you don't cross zero, keeping one line active.

This should make it possible to do calibration to perhaps 0.1%, with
even better for gain matching on the same board.

The DAC should be driven from the master reference, but we should also
have a check ADC for the reference to see if it is basically sane.
This can be a low resolution I2C part.



[5 Dec 19]

I tweaked the reference buffer compensation for the highest
disturbance rejection.  This pushed out the disturbance rejection out
> 2 kHz.  The transient response to step disturbance is also hugely
speeded up and attenuated.  Now the feedback cap is 47 nF and the
decoupling resistor is 100 mOhm.  This is a much smaller decoupling
resistor than I have ever used in this sort of setup; it will be
interesting to see if it actually works.  I am guessing this works
because the capacitive load is so huge, so even a relatively small
slew generates a considerable signal, and also the frequency is quite
low, so that we have loads of gain bandwidth.

In the current single opamp setup this also unavoidably increases the
feedforward bandwidth, which is now 2.8 kHz.  I say unavoidable,
because to cancel the disturbance the buffer needs to know what the
reference should be, and there is not currently any "memory" on the
board.  It would be possible to separate the buffer and diff amp
functions in two, then give the diff amp a low bandwidth.  But this is
so far as we know unnecessary, so might as well keep it simple.  The
differential reference distribution should tend to minimize any ELF
noise pickup.






[4 Dec 19]

Summary:
 -- How reference noise spectrum creates measurement spectrum.
 -- X7R capacitor microphonics
 -- Self-resonance in reference decoupling caps
 -- IR drops in main board reference bus.
 -- Total analog power use.  Not much idea, maybe 150 mA per input
    card?  That's 4.5W, but we are plugged into the wall, and already
    have to be thermally prepared for a power amp in the box.
 

Think I understand now how reference noise looks in the ADC output spectrum. 

If you have a DC signal then the extra noise spectrum looks just like
the reference noise.  It appears additive except that it scales with
the signal. You have 1/f in the output for the reference 1/f, and
broadband for the reference broadband.

But if you have narrowband AC signals, then the reference noise
appears as mirror image broadening around each carrier. The 1/f is
close in, then going down to a broadband floor.

This only holds for ref noise in the Nyquist bandwidth. Any reference
noise in the digital filter stopband should have minimal effect, but
there is sensitivity again from 900 kHz to 50 MHz or so. The HF noise
would presumably show up as broadband floor, unless it is modulated or
coherent with the SAR cycle.

The broadband contribution will only be flat if the reference noise is
flat. Either for AC or DC this is unlikely, it will be rolling
off. So, well away from the carrier the reference contribution will be
nil. Because we have guard channels there will be no effect on the
adjacent carrier. The thing is, we care mainly about the reference
noise up to our measurement bandwidth, 500 Hz.  And at low frequency,
below say 50 Hz, the noise doesn't matter so much because it will be
in common between source and sensor. But we would like the ref noise
to be well below the ADC noise floor by 50 Hz.

Why 50 Hz? This is an octave below the reference distribution
bandwidth, so by then all the reference noise should be common mode on
the source and sensor sides.

50 Hz is also 6x lower than the low carrier. Since the low carrier is
very narrow, a given peak broadening is relatively bigger. But the low
carrier signal is also a small fraction of full scale, so reference
noise should not be such a big deal.

The REF102 does not have a broadband noise spec, but from app info it
seems to be about 133 nV/rtHz. This is only 7x above the negligible
level 20 nV/rtHz. Also we have the divide by 2. Anyway, a single pole
at a few Hz should do it. But 2 poles would give faster rolloff
without excessive startup settling time.


Hmmn, X7R caps do have quite significant microphonics it seems. This
is where a reference buffer with low impedance into the kHz range
could be useful. We don't have a lot of choice for the reference
bypass at the ADC, but we could use smaller caps, perhaps with an
aluminum electrolytic or os-con.

For the high impedance locations, the reference buffer feedback and
the master reference filter, we should avoid the ceramics. Tantalum is
not a win for noise, though. And electrolytic leakage currents are a
problem for DC accuracy. That means film. 1 uF at 16V is not too
expensive in 1210 SMT, but there are a lot more through hole
options. 10 uF or larger film capacitors are feasible for the master
reference. 47 uF is about the largest sensible film cap.


Rather than grounding big reference bypass cap to a via, ground it to
a top layer pour that connects directly to the ADC ground pins. This
pour is then tied to the ground plane at single point. This keeps any
LF/HF ground bounce from coupling into the reference input.

The bypass self resonance is not at good as it may seem, because the
"inductor" is the local ground system, which has various loops and
center taps. We don't want to resonantly build up a lot of energy in
the ground system. It may be good that the SRF is below the sampling
rate, because there is much less excitation there.

Need to look at the details of the eval board layout at the
ADC. Should follow that closely. Don't want to go compromising the
bypass performance to deal with a hypothetical microphonics
problem. But jack up the buffer disturbance rejection bandwidth? Sure.


IR drop in reference bus on the main board is at a
possibly-significant level due to load from the diff amps. But not
using the diff amp is even worse because we affected by ground drops
due to supply current, etc.

Bit nervous about how much power we are going to draw on the analog
rails, but it is still going to be small compared to the source
driver. We want a fan already because of that.


[3 Dec 19]

Summary:
 -- Antialias filter design largely settled; it was a big mistake to
    have used Chebyshev, since this creates noise peaking.
 -- Detailed consideration of how the SAR charge redistribution
    architecture determines the demands on the signal and reference
    input driver.
 -- Input board reference distribution converging.  I got a fairly
    detailed spice simulation going.  It seems it will work to just
    have a single buffer, with a small 270 nH inductor decoupling each
    channel to avoid HF crosstalk through the reference.  This buffer
    also implements a diff-amp input from the main board reference
    distribution bus.

Think I have an antialias filter design now.  We will see in reality,
but what I had been doing, using a 15 kHz Chebyshev 1 dB filter was a
major blunder.  This creates quite a bit of noise peaking in our
passband.  With the 16 or 32 oversample, we don't run into aliasing
until nearing the high-rate sampling Fs (~ 1 Ms/s).  And with the flat
filter in the ADC, we are -80 dB from 2/3 the output rate (Fo) until
Fs - 2/3 Fo.  So I went for a 30 kHz order 3 Butterworth, which gives
around 85 dB of attenuation at 900 kHz.  This does not have the noise
peaking at all (Butterworth characteristic I guess).

I could reduce the corner lower, but it isn't really necessary, and
one plus is that the phase response is going to be much lower in our
passband, -30 degrees at 13.5 kHz vs. -130 degrees, and this will
presumably be more predictable across component variation and time.

For the noise analysis in simetrix I used the parameteric opamp model,
which I am figuring is noise-free, then adding a resistor to simulate
the voltage noise.  The noise does seem to be in the right ballpark.
Whatever LT1028 model comes with simetrix is crazy wrong for noise
(>1000x too high).


I learned the basic operation of a charge redistribution SAR ADC,
which has shed a lot of light on the input and reference drive
characteristics. The input and the reference are driving the same ~45
pF capacitance (though the reference drives portions up to 1/2).
During the acquire cycle the converter is idle and the capacitor
charges through the input.  The cap starts discharged, so it has
considerable settling to do.  You'd think this would not be a problem
because the cap is so small and the resistance of the input path
is 10's of Ohms (sample switch itself is 40 Ohms).

But in the standard setup the sample cap is swamped at least 100x by a
capacitor at the input.  This reduces the the sampling glitch at the
input 100x, but also increases the settling time constant ~100x
because the driver has nonzero output impedance and a decoupling
resistor.  We do not have as far to settle, but the settling is much
slower.  This is a net loss for settling accuracy because the glitch
reduction is only linear with increasing capacitance, while the loss
of settling is exponential.

Why is the capacitor there, then?  Two things I can see:
 1] It prevents the ADC from having excess noise bandwidth.  At the
    output of the buffer there is broadband noise present, out to 10's
    of MHz at least.  This noise is from the driver itself, as well as
    any upstream noise.  This noise will be aliased.
 2] As noted below, the RC serves as a lowpass filter on the current
    load directed back at the buffer.  This may reduce the need for
    buffer bandwidth.


Re [1], you'd think that you'd want a much lower filter corner.  They
have the input filter set with a corner approximately equal to the
sample rate, rather than well below the Nyquist frequency.  Why is
this?  Well, you want the drive impedance to be low, because there is
sampling current, whether you look at it as a glitch, or averaged.  So
you don't want to lower the corner by increasing the R.  But lowering
the corner by increasing C results in excessive load on the driver
when there is an AC signal.  As I had found out the hard way, you can
go into current limit, but even well before then the driver distortion
is going to increase.

Now, depending on the situation, it might be reasonable to lower the
corner by increasing both R and C so that the impedance at the input
remains low, without becoming excessively low from the viewpoint of
the driver.

But also, maybe the driver broadband noise is not such a problem?  It
depends on the magnitude of the driver aliased noise vs. the intrinsic
ADC noise.  This could indeed be a reason to make sure that the driver
has much lower noise than the ADC, instead of 3x lower, 10x to 30x
lower.

I think one confusion or subtlety comes from the application notes
desire to give general advice that applies across a wide range of
applications, DC, 1 kHz or 100 kHz.  The basic story about settling
only applies in a straightforward way to a nearly static signal.  We
want to settle to within the resolution during the acqusition period.
But with a dynamic signal, this signal varies a considerable fraction
of full scale during the acqusition period.  Eg. with 13.5 kHz and 400
ns, with full scale swing, our peak slew is pi*13.5 kHz or 42e3
FS/sec.  That gives 17e-3 re. FS change during the acqusition.  This
is *huge* in comparison to the converter resolution.

You come up with similar numbers if you consider the attenuation of a
1 MHz LPF at the 13.5 kHz passband.  This is 13e-3, give or take
sqrt(2) or something.

I think that the basic takeaway is that in AC applications, you have
to assume there is going to be frequency response deviations which are
far larger than the resolution, and this has to be calibrated out
somehow.  Then the issue becomes whether the response deviations are
reproducible between units, and whether there is drift or 1/f noise.

Does this mean we don't have to worry about the ADC kickback?  Well,
it still happens, superimposed on the "tracking error" introduced by
the frequency response of the LPF.  If we have 6.8 nF, and the
sampling cap is 45 pF, then our glitch is 7e-3 re. FS, which is maybe
1/2 of the AC effect.  If the time constant of the input network is
well less than the acqusition time, then the kickback error becomes
quite small indeed.

Note also that the magnitude of the kickback error is probably pretty
well behaved, determined mainly by the capacitance ratio and the
filter time constant.  I think it doesn't really create any
nonlinearity or HF response issues either.  The step is a fraction of
the input amplitude, so is linear, an attenuation.  And the step
happens at the beginning of the acqusition, so any HF ringing in
parasitic inductances has time to damp out before the end of
aqusition.

The impedance of 6.8 nF at 13.5 kHz is 1.7 kOhm. We probably would not
want to increase the capacitance more than 2x or 4x to avoid loading
down the driver.  Their standard input filter has to allow for the
possibility of higher bandwidth than we need.  But this will tend to
push us from "fully settled" in to "partially settled" operation.
Even so, I think the "error" due to the frequency response of the
reduced LPF corner remains large in comparison to the settling error.

Note that the dev board for the LTC2508-32 (with only high decimation
options) has 4.7 uF caps.  Despite being intended for near-DC
applications where frequency response errors can be neglected, and the
resolution is very high (high decimation), in this configuration they
have entirely given up on the idea of settling during the acqusition.
Instead, they are swamping the sampling glitch down to 10 ppm re FS.
This is the "fully averaged" regime.

We do want to avoid major ringing in the buffer response, since that
gives a sampling effect that may be more drift or noise prone, since
it depends on buffer characteristics such as the output impedance.
Any rapid slewing toward the end of the acqusition is undesirable
because it exaggerates clock jitter effects.


On the reference input, despite it driving the same capacitor, on
similar time scales, the situation is quite different.  The big thing
is that the reference does not have to slew, so it can be swamped with
capacitance. On the dev board there are two 47 uF caps, which would
bring the glitch down to about 1 ppm.  The two capacitors may be more
for low inductance than bulk capacitance, because the reference input
gets a burst of load spikes at the SAR cycle rate, here maybe 50x the
sample rate (24x for SAR cycles, but also convert is only 1/2 of the
sample period).  I don't think that their buffered reference is going
to be doing much with 50 MHz and harmonics.  It's really the
capacitors that are doing all the work, so far as preventing the bit
comparisons from interfering with each other.
 ==> Note that at this frequency range the reference bypass caps are
     above self-resonance.  The inductance of a 1210 is about 1
     nH, so 47 uF would be no higher than 730 kHz, maybe 400 kHz with
     2 nH other layout inductance, vias, etc.  This does not mean the
     bypass is ineffective, but the impedance at 50 MHz is more like 1
     Ohm than 100 micro-Ohms.

What the reference buffer does need to do is to keep the average
reference voltage constant from one sample to the next, despite
variations in reference load as the input signal varies.  At the
signal input, droop proportional to the signal is just a gain error.
But at the reference input droop proportional to the (input) signal is
a nonlinearity.  Perhaps even moreso than with the signal input, we
really don't want any fast overshoot on the reference transient
response.  But this will tend to be swamped by the very large
capacitance, or pushed down to a low frequency ringing which may not
get excited.


I expect that it is because of this high frequency crud that they say
you should use separate references (or buffers) for each converter.
The thing is, any noise on the reference is presented directly to the
comparator.
 ==> The ADC input is *disconnected* during conversion, whereas the
     reference *is* connected.  So only the reference is a noise
     conduit.

When a single converter is running the most sensitive
comparisons (for the LSBs) are fairly well separated in time from the
MSB sampling, which draws the biggest currents.  But multiple
converters could interact.  My guess is that this is more a matter of
inductance in the loops involving the two ADC reference currents,
rather than simple charge redistribution.  Each ADC is a noise source,
and you are tying them together through a fairly low impedance
connection (a trace).


I wonder if inductive decoupling between ADC references might work
well?  The LC resonance would need to be damped though.  16 uH gives
100 ohms at 1 MHz.  This gives a LC resonance of 4 kHz with 100 uF.
Z0 is 0.4 ohms, so it would only require an Ohm in series with the
inductor to damp the resonance.  A low frequency ferrite bead might do
the job.

I guess the problem with the inductive decoupling is that it means
there is not a particularly low impedance at eg. 1 MHz, so the
cycle-to-cycle smoothing of the reference voltage may not be so ideal.
But, given that our signal is high SNR and low frequency, I think we
are OK as long as the reference impedance is low at the highest
frequency of interest, say 30 kHz.  There is a 30x span between 30 kHz
and 1 MHz.  But to put the resonance at 30 kHz we only want 280 nH.
This is low, but still more than 10x greater than the expected trace
inductance (about 10 nH/in).  This is in the ballpark of what you
could build with a planar inductor, but I suspect building a loop
antenna into your board is not a great idea.

It may be that we really want a lossy ferrite rather than a high-Q
inductor.  But we don't need too much loss to damp the LC resonance,
since inductor chips in this range are designed for high Q and 10's of
MHz.  For example, Bourns CS160808-R27K,
    Inductance uH	Q	Test Freq. 	SRF MHz	 DCR mOhm	I rms mA
    0.27 ±10		30	25 		190	 500 (max)	200

But Z0 for 300 nH/100 uF is 55 mOhm, so 500 mOhm ESR is well more than
enough to damp this.  This part maintains impedance > 100 Ohm out past
1 gHz, but Q is very low past 100 MHz.  Spec'ing an inductor makes it
clearer what we are doing, it isn't just "a chip bead".  We have
similar impedance at 100 MHz, but the impedance at LF/HF is
characterized as inductive (and resistive).

With this low an inductance, it is not going to serve to decouple the
reference buffer output from the capacitive load, so it would still be
necessary to have the usual C-load decoupling resistor/feedback
arrangement.  We would want this anyway because we want to locate the
inductor at the ADC to E-field pickup and emission on the distribution
trace. Then we have another big capacitor at the buffer output to
thoroughly decouple any ADC interaction.  This creates a tee LC filter
between the converters.

So, at the LC resonance (and below) the impedance at the reference
input is entirely dominated by resistance, the inductor ESR 500 mOhm,
the trace resistance ~50 mOhm, and whatever the buffer output
impedance is.  To ballpark reference buffer output impedance, suppose
the decoupling resistor is 20 Ohms, and the GBW is 30 MHz.  Then the
loop gain at 30 kHz is 1000, so the effective output impedance would
be about 20 mOhm, which is well less than the other resistances.  It
might be several times that, depending on the open-loop output
impedance.

Another way to look at the reference distribution is that the RC pole
(2.3 kHz) is well below the LC pole pair (29 kHz).  Because of the
dominant RC effect, the design intent for the reference distribution
to be coupled at eg. 13.5 kHz is not achieved.

We can push down the resistance a lot by switching to a wirewound
power inductor, like 587-4123-1-ND, 17 mOhm nominal.  (Most of the
power inductors are much lower ESR.  There is a hole in the ESR range
as you go from signal to power parts.)  This part is an odball
single-source extra-small inductor though, 3x3 mm.  Power inductor
footprints are nonstandard, unfortunately, but digikey has 3 that are
approximately 7x7, eg. 553-3412-1-ND.  The specs are ludicrous for a
small signal use, 16A to 21A current rating, 3-4 mOhm ESR.  The
footprints are rather similar, so they could be substituted.  These
parts are not specified for Q or SRF, but the larger 11x10mm
SRP1038C-R30MCT-ND has a SRF of 99 MHz, so it is a pretty safe bet the
smaller parts would be at least this high.  They should be nicely
lossy at VHF also.  SRF of 100 MHz implies C=8.5 pF.

With these parts the resistance is going to be dominated by the trace
resistance and the buffer output impedance.  With 70 mOhm resistance
instead of 700 we have a much crisper rolloff, and are -0.25 dB at
13.5 kHz, rather that -15 dB.  To achieve this resistance you need
somewhat wide traces, eg. 20 mil width at 2" is 40 mOhm.  Want to make
sure the resistance is similar across the different length runs.

See ilemt_hw/simetrix/reference_buffer.wxsch

Uh, OK, I've got it to where I am simulating the coupling magnitude in
response to the sampling capacitor disturbance, and it isn't clear
that the low ESR is even better.  Pushing out the RC pole does mean
that there is less self-coupling a low frequency, but as it is set up
with resonance at 30 kHz, this effect doesn't develop until below our
high carrier passband.  And for the ADC cross-coupling, at 13.5 kHz
the isolation is 27 dB higher because it starts improving at the RC
corner.  Given the smaller, cheaper, and easier sourcing for the chip
inductor, this seems like a no-brainer.  The thing is, the
self-isolation is flat at the capacitor ratio until below the RC pole,
and this is also very fine level of attenuation, -125 dB re. FS.  So
pushing down the RC pole does not really do any harm.

This is another way of saying that the buffer output impedance does
not matter that much, except perhaps at DC for DC accurate
applications.  Above the decoupling self-resonance there is certainly
a concern with self-coupling, but this is completely unaffected by
anything other than the inductance of the reference bypass (and not
by the driver).  eg. at 20 MHz the bypass impedance is only 188 mOhm,
and at 1 Mhz (where cycle-to-cycle ripple would happen), it is only 9
mOhm.  There is no way an opamp buffer is going to meaningfully
contribute. 

There is however a resistive voltage drop at DC, with 700 mOhm, at a
reference current of 1 mA, this is 0.7 mV, or 140 ppm.  I don't think
this is a big concern for us, since it seems unlikely there is much
1/f fluctuation in the reference current.  The low ESR setup would
reduce this drop by an order of magnitude, though.

If we use 445-174189-1-ND, this is 270 nH and 286 mOhm.  This would
give about 1/2 the DC voltage drop compared to a 600 mOhm part.  It
maintains high Q out to 80 MHz, which is perhaps not desirable, but
doesn't seem terrible.  In 0603 package, much cheaper.

To recap, even after modelling trace inductance, the decoupling
inductor creates a >40 dB improvement in cross-coupling starting at
100 kHz, but this is perhaps only significant above 8 MHz where the
cross coupling goes above -120 dB.  At 50 MHz it's -156 dB vs. -103
dB.  Also, having eg. 600 mOhm resistance without the inductance does
not do it.  We need distribution line inductance to swamp the
inductance of the bypass cap at the reference buffer output.

I feel a little silly spending most of a day analyzing the reference
decoupling, but given that I have never seen anyone suggest anything
like the inductive decoupling, it seemed like a good idea.  I am
unclear on why what info is out there is so favoring the idea of a
wide bandwidth buffer.  I wonder if part of the issue is that we did
not have small cheap 47 uF ceramic caps 20 years ago?  It seems to me
that the passive output is favorable from a noise perspective.

What you get with the opamp driving a big cap is the RC from the
decoupling resistor and the output cap.  But with the 100 uF bypass
caps, whether we allow for any reduction of the buffer AC output
impedance using feedback does not really make any difference above say
1 kHz.  The thing is that for the AC disturbance rejection, we've got
a capacitive divider (below the bypass SRF), so the response is flat
at -120 dB, which is just fine.  We do not even need any lower.

The only reason to maintain the feedback is for DC accuracy.  From a
feedforward noise perspective, the sooner we start rolling off the
feedback, the better.  If we set the forward response to -3 dB at say
10 Hz, then we are 64d dB down at 10 kHz.  This reduces in-band
reference noise, and yet has minimal effect on the cross-coupling.  So
we do not need an opamp with particularly low broadband noise or high
bandwidth.  The OP07 or OP27 would be fine.  Audio opamps are more
questionable because 1/f noise is not such a big concern in that
application.

Simetrix says that if we use an 80 Ohm decoupling resistor and 47 uF
|| 1 kOhm feedback then we get a feedforward corner of 10 Hz.
Allowing for the effect that all of the ref bypass caps are in
parallel near DC (376 uF), the capacitance takes over from the 80 Ohm
decoupling resistor at 5 Hz.  At 300 Hz the reference network
impedance would be 1.4 Ohms (purely due to capacitance).  While we
could reduce the impedance at LF using the opamp, at 10 kHz the
capacitors are going to take over, no matter what.  And, as I said,
passing through more reference noise is bad.
 ==> Eh, I guess it's kind of pointless to make the bandwidth that
     Low.  80 Ohms is underdamped and very slow to settle.  Lower
     values are stable with that setup, down to 100 mOhm.  It is
     kind of pointless rolling off the DC feedback so low, since all
     you are doing is compromising the disturbance rejection without
     attenuating the feedforward.  Maybe 2 Ohm decoupling, 47 uF ||
     200 Ohm feedback.  This way our feedforward is 3 dB down at about
     300 Hz, and we have some disturbance rejection.  We don't want
     the feedback R to be too high when using a bipolar amp, since max
     bias current is 80 nA.

IDK, anything in this range would work.  But I'm liking it at 300 Hz.
The master reference is already going to be heavily filtered for
broadband noise, so we don't really need to attenuate that much at
this point.

Need to think a bit about the differential reference distribution,
though.  If we use a diff amp, then we don't want the feedback
resistance too low, since this loads down the main board reference
distribution.  But we are going for gain of 1/2, which helps a bit.

OK, it works.  Now I am still using 2 Ohm decoupling, but with 4K/2K
feedback resistors and 1 uF cap.  The feedback resistors should be
0.1% for accurate gain 1/2 as well as CMR.

OK, lets look at the feedforward seperately from the disturbance
rejection.  With the 4K/2K feedback, but now 1 Ohm decoupling.  The
feedfoward from the reference starts rolling off at 90 Hz, but the
disturbance rejection goes out to about 300 Hz.

Because of the load from the diff amps we want to make sure that there
is a fat low-resistance reference bus.  This is better than a star
distribution because we want to keep the reference voltages equal.  We
aren't particular worried about the common impedance since the load
should be extremely constant.  But there will be per-channel gain
compensation anyway, so it doesn't really matter that much.  Also,
total load on the reference is only about 2 mA per card.

I think I've got a reference buffer design, and one that is a lot
simpler and cheaper than I had been thinking.  


[26 Nov 19]

Summary:
 -- GBW of >20 MHz is useful in the ADC driver because the ADC input
    network corner is 2 MHz, so we want the buffer to reject output
    disturbance from there on down.  The LT6202 on eval board is 100
    MHz.  ADC sample capacitor is 45 pF.
 -- As well as the obvious (but secondary) function of lowpass
    filtering the input signal, the ADC input network can been seen as
    a lowpass filter for the sampling *current* into the buffer
    output.  The corner frequency is the same in this direction.
 -- We could increase the ADC input cap 2-3x if this does not require
    a larger driver output isolation resistor for stability.
 -- There does seem to be a defacto standard pinout for F-D amps.
 -- Fully differential amps do depend on component matching for CMR.
 -- Making a F-D active filter is a thing, but is going to have
    weak CMR because of need for capacitor matching.
 -- Looked at balanced antialias filters more, but it doesn't really
    make sense.  However we can avoid a buffer after the 3'rd pole if
    we build it into the F-D amp balancer/driver.
 -- Puzzled by noise formula for F-D amp.
 -- The usefulness of CMR in the amp is bounded by the imbalance of
    the input network, which is about -50 dB.  So we might aim for -60
    dB.  The amp first stage at Av=20 gives us 26 dB, so we only need
    34 dB downstream.  Unmatched 0.1% resistors would be fine.

The 10 Ohm/6.8 nF network recommended on the ADC inputs is described
as filtering the sampling current spikes to prevent the input driver
from going nonlinear in response.  Seen as a lowpass filter it does
filter the buffer noise somewhat (~2 MHz corner).  But with R so low,
the buffer output impedance may contribute significantly.  R is likely
set largely to give a stable response with the C load, rather than so
much as an LPF.  The placement of R is wrong for lowpass filtering
voltage at the ADC input, but what we are really doing is filtering
the sampling *current*, where it does function as an LPF, with the
usual corner frequency.  Above the corner, most current flows into the
capacitor, below that, most goes into the buffer.

The sample capacitor is described as 45 pF.  The capacitive divider
between 6.8 nF and 45 pF is -44 dB.  (Ignoring diff/CM issues.)  We
could increase the capacitance somewhat because our bandwidth is not
wide, but we don't want to load down the buffer.  6.8 nF is 1.7 kOhm at
13.5 kHz, so we wouldn't want to go up more than 2-3x.  Buffer
stability may also be affected.  We don't want to have to increase the
output isolation resistor.

I'm not clear on whether the input is sampled just once per (high
rate) conversion, or once per SAR cycle, so 24x higher rate (24 MHz).

Because of the LPF aspect from the viewpoint of the buffer, it would
see most of the load ripple at the sample rate, but higher frequency
components would be significantly attenuated.

The recommended LT6202 is a 100 MHz GBW part, which should give decent
suppression of the ~1 MHz sample rate ripple.  The output impedance
Av=1 at 1 MHz is shown as 1 Ohm, and the open loop impedance seems to
be about 50 Ohms, which is rather low for a rail-to-rail output.
There must be internal feedback.  It seems the capacitive load
response is overdamped with 10 Ohm/6.8 nF.


Fully differential amps:

I notice that the recommended LTC6363 diff amp has a higher output
decoupling resistor, 30 Ohms.  This part is $6.  The noise performance
is not so great, the 1/f corner is in the kHz range, and the quoted
spot noise of 2.9 nV/rtHz is at 100 kHz.  As a "LTC" part it would be
CMOS in some way, but from the bias current spec the input stage must
be bipolar (and with no bias current cancellation).  I'm not sure why
the LF noise is so crappy, maybe the CMOS part.

There are fixed gain versions, where the noise spec includes the
internal resistors.  The RTI noise goes up as the gain goes down.
That makes sense, sort of.  You see this in instrumentation amps, for
example, where there is the diff amp stage with a fixed output noise
outside of any overall feedback, RTO noise.  So low RTI noise is only
achieved at high gains.  But the fully diff amp is inside overall
feedback, so output noise should be rejected.  Maybe the difference is
just in the builtin resistor noise?  This is relevant because I was
planning to use a gain 1/2 diff amp as the ADC driver.

Wondering if the fully-differential amp is such a good thing.  But the
TI THS4131 looks plausible from a noise perspective. 1.3 nV/rtHz at 10
kHz, and the 1/f corner seems to be about 1 kHz, with less than 2x
increase by 300 Hz.  Digikey price is $3.30.  It is aimed at roughly
this application, Msps ADC driving.  It runs on up to 33V supply, and
is not unreasonably fast.

But this is the only part TI has like it, the other parts are aimed at
> 10 MHz apps, and I can't even tell the 1/f corner.  It does seem
that the F-D ADC driver is a category that exists with other vendors,
though (like the LTC6363, more or less).

TI LMP8350 is another ADC driver 4.6 nV/rtHz @ 10 kHz, 1/f corner is
also around 1 kHz.  Input bias current 2 uA.  It seems roughly similar
to the LTC6363, but with less LF noise.

The THS4561 is not so low noise, but says it has a 50 Hz 1/f corner.
There is also quite a bit of application info in the datasheet, such
as the input referred and output referred noises vs. gain.  The RTI
spot noise does increase greatly at low gains, but this seems to be
mainly due to increasing resistance at the input nodes, which could be
reduced by decreasing the feedback resistor.  I don't understand their
noise analysis, though.  Why doesn't the gain resistor appear in the
formula?  It does appear implicitly in the noise gain NG.

The THS4551 is somewhat similar to the THS4561, but has only 5V supply
(really undesirable for us).  But it has similar bandwidth to the ADC
drivers, 3.3 nV/rtHz noise, and low 1/f corner.

Need to look at how much noise we can tolerate in the ADC driver.  On
[12 Aug 19] I reckon that the ADC input noise is 33 nV/rtHz, so less
than 11 nV/rtHz is small.  This is the RTO noise of the driver,
though.

You can make a differential active filter in the MFB topology using a
F-D amp.  But with F-D the CMR is still dependent on the component
matching, and this is at least a 10x worse problem when you add
capacitors to the resistors.  This does make sense in some cases.  A
CMR only 30 dB is not a big deal when you are being driven by a
relatively clean source such as a DAC.

I'm pretty sure I can make an active GIC differential filter which has
no CMR but also does not rely on component matching to prevent
CM-to-diff conversion.  I kind of like this idea because it allows the
signal to stay differential.  But not clear that it would reduce
complexity.

Our "plan A" is:
    2 opamp stage 1
    1 opamp difference amp
    1 opamp single-ended MFB LPF, order 3
    1 opamp buffer [optional for order 2]
    F-D amp balancer gain 1/2 and level shift

For "plan B":
    2 opamp stage 1
    2 opamp differential GIC LPF, order 3
    [build in AC coupling and gain of 1/2 here somewhere]
    2 opamp buffer/driver

Plan B is not really simpler, but stays differential (might help with
internal signal pickup) and might have better noise.  Replacing a F-D
amp with an opamp might be considered a plus.  You would need two AC
coupling capacitors.  This scheme requires the ADC to do CMR, though,
and runs the CM signal through the filter, so CMR is probably not as
good.

Hmmn, how about plan A2:
    2 opamp stage 1
    1 opamp difference amp
    1 opamp single-ended MFB LPF, order 2
    F-D amp balancer/driver gain 1/2, level shift, and third pole

Unlike if it were an active filter, adding feedback capacitors to the
balancer won't compromise the output impedance at all, and won't add
noise gain.  The feedback capacitor can't attenuate the noise of the
F-D amp itself, but it will reduce noise and interference in the 100
kHz to MHz range which may leak through the antialias filter and get
aliased.


I still don't have a quantitative story on to what degree CMR is
limited by the input imbalance.  Past that point there is no need to
worry about CMR downstream.  The problem is that this depends on the
imbalance in wiring parasitics in the sensor cable coupling with the
interference, so is hard to quantify.  But if we assume the CM signal
is balanced then we can get an upper bound on the CMR.
 ==> Let's say that our imbalance is 1%, 40 dB, divided by the
     balancing effect of the diff mode cap, 10 dB, or -50 dB.  Then
     there would not be much benefit of downstream CMR greater than
     say 60 dB.  But if we run the first stage at gain 20, then that's
     26 dB of CMR right there.  So we only need 34 dB of CMR
     downstream; there is little point in matching the diff amp
     resistors more than 0.1% and even 1% is not terrible.

In classic instrumentation amp usage, near DC, there is benefit in
keeping a high CM input impedance.  If you can make it much higher
than the source imbalance then that imbalance doesn't cause a diff
mode conversion.  (Assuming also no imbalance in the amp itself.)  But
for us, with a capactive CM coupling mode, the reverse is true.  Since
there is no way we can make our capacitance small wrt the tiny noise
capacitance (and its imbalance), we might as well just swamp the noise
capacitance.  Then we are limited mainly by our input imbalance.  How
much capacitance we can add is limited by the diff mode
characteristics of the sensor (in this case the resonance).
     

Note also that we get some CMR from the first stage, which limits the
need for CMR after that.

Another limiting factor on the benefit of CMR is the usage situation:
 -- The probe is insulated along its length, so there is no firm
    remote ground we are dealing with.
 -- Because there is no DC CM circuit, at our operating frequency
    there is no magnetic (or EM) means for creating common mode.  More
    importantly, at the end of the cable is a magnetic antenna!  This
    is going to quite nicely pick up any in-band magnetic fields as
    differential mode.
 -- So the only CM coupling means is capacitive.  But our cable is not
    long, and can have a shield.
 -- The input network attenuates capacitive
    coupling somewhat through the input capacitance, which is likely
    to be at least several times larger than the interference coupling
    capacitance.  This exploits the sensor's low diff-mode output
    impedance (low in comparison to the interference capacitance).
 -- Because of the low sensor impedance, imbalance in the coupling
    capacitance (eg. between lines) tends to be converted to common
    mode. 

Common mode pickup of RFI is certainly a big concern, but this is way
out of band, and is not going to see much attenuation from CMR of the
differential path.  But CM RFI rectification would to some degree
appear as common mode.

The problem is that there is not an analytic answer to the value of
CMR; it depends on the insults in the application situation.  It would
sometimes work to just ground one side of the sensor, but I would
certainly use a balanced differential input.  The only question is how
much effort and $$$ we should devote to maximizing the CMR.  Given
that we won't know the interference that will be seen, it makes sense
to got to reasonable effort to maximize the CMR.


[25 Nov 19]

Summary:
 -- Effect of input bias resistors on noise.  We should be using a
    much larger value than 20K, like maybe 200K, to minimize current
    noise.  Because the sensor impedance is so much lower this noise
    is almost purely common mode.  This is true even for the DC offset
    due to amp bias.  Also, in-band input impedance is capacitive
    dominated.  A shunt resistor would help damping, but is too much
    noise.  The snubber resistor in the diff mode cap is not a big
    noise source, with current component values (nor is it damping
    that much).  The Johnson noise of the dipole-approximating coil is
    significant.
 -- Likely CMR will be limited by input impedance effects, so a fancy
    ratio-matched resistor array may not be entirely necessary for the
    difference amp.  Even if the amp were perfectly balanced,
    the CM noise will not be.  The diff mode capacitors for RFI
    injection make the input impedance moderately low wrt capacitive
    coupled CM noise, so it will load down the higher impedance line
    and convert to differential.
 -- It seems that if the 0.1 Hz to 1 kHz p-p reference mismatch between
    source and sensor is less than 6 uV p-p then we are good.  This is
    not extremely demanding, and could probably be achieved even with
    a non-common reference.
 -- It is the LF reference noise that is the main problem, not the
    reference noise in the carrier bands. (After lowpass filtering,
    1/f amplitude is bigger, and also second-order effects from
    narrowband signal.)
 -- We can halve any error in reference distribution by using a 10V
    master reference.
 -- Possibly thermal differential noise eg. on thermocouple potentials
    could be a significant noise source.  (But probably not.)  This
    could be reduced by diffential reference distribution, which
    would not be too hard.
 -- Reference buffer selection.  There are chopper amp solutions which
    are not too expensive and can run on +/- 15V, but a low noise
    bipolar might work better, and could avoid the composite buffer.
    Chopper LF noise is only good in comparison to non-chopper CMOS
    amps.  This is because we do not care about noise/drift below 0.1
    Hz.  Some chopper amps are fast enough that a simple buffer might
    also be reasonable.  It is not clear that we even need a fast
    buffer.
 -- The master reference will want to use a chopper amp so that it can
    use a high resistance in the noise filter.

See also [12 Aug 19].


re. effect of input bias resistors:

With 1+1 nF and 20 KOhm on each input, the CM response to the bias
resistor noise starts to roll off at 5 kHz.  If we made the bias
resistor larger, say 50K, this would be pushed down even lower.

My feel is that this is true, but also that it doesn't help with the
amp current noise.  But I can't see why that would be so.  The amp
current is being injected at the same node.  Oh, I guess it's that the
CM noise is attenuated, but the diff mode not so much?  The sensor is
exposed to the current noise of the bias resistors, and develops a
diff mode voltage in response.  So I think it's true that the CM noise
will roll off.  Indeed, assuming a very high source impedance for the
amp current noise, it would be approximately an integrator.  But in
the passband the sensor has a much lower impedance than the input
caps, so most of the diff current noise is going to flow in the
sensor.  This does have the effect of keeping the noise mostly CM, but
also converts a fraction into diff mode noise.

eg. at 13.5 kHz:
    20 mH = 1.7 kOhm, 4 mH = 340 Ohm
    2 nF = 3.9 kOhm

So, for example, with the lower inductance sensor, ~90% of the current
noise is going to be diff mode.  But the fraction that is CM is
looking into a much higher impedance, and the CM signal developed is
going to partly convert into diff mode signal due to imbalance.  My
suspicion is that input impedance imbalance will contribute much more
than the amp CMR itself, since the CM impedance is high.

An important question for the noise analysis is how the magnitude of
the noise contribution from CM response to the current noise compares
to the sensor's diff mode response, and to the amp voltage noise.

I guessed that this tradeoff would determine the size of the bias
resistors. But note that even with 20 kOhm the input caps Z << bias Z,
so the input network is responding to a current noise, and we can
neglect any divider effect from the bias resistor or amp input load.
Then, somewhat nonintuitively, a bigger bias resistor is always better
wrt noise contribution.  So the only limit on the bias resistor value
is going to be the DC offset resulting from the drop across the
resistors.

The LT1028 input bias current is spec'd as +/- 180 nA max.  That's 3.6
mV across 20 kOhm, or 72 mV after 20x gain.  Assuming worst case
mismatch between the + and - input amps, this could be doubled.  I
think an output offset of even 1V would be tolerable, so we could go
up 10x in resistance to 200k.  This gives 0.28 pA/rtHz, which is ~3x
less than the LT1028 input current noise, so would contribute
negligibly.  Maybe 100K would make sense with the LT1028, since there
is likely hidden current noise due to correlation, and the bias
current is relatively large.  Higher value might make sense for amp
with lower current noise and bias current.
 ==> But diff mode bias current is shunted by the sensor, low Z at
     DC.  So the bias current sees little gain.  See below.  As long
     as we stay in the common mode range there is little problem.

It's also the case that the current noise contributes insignificantly
wrt. the voltage noise when using the lower inductance sensor.

Back to the input network response to the current noise.  I think what
it comes down to is that the CMR seen by the intrinsic CM noise is
going to be the same as the rejection of external CM noise because the
CMR in practice is in response to capacitively coupled currents
interacting with the amp imbalance.  In both cases it is the imbalance
in response to CM current.  This means that the CM noise will see
considerable attenuation, at least 40 dB with 1% capacitors, probably
more like 50 dB allowing for the diff mode cap.  This CM current noise
is shunted by 2 nF (4 nF?) of capacitance, which would be 10 kOhm at
7.5 kHz.  So:
    1.4e-12 * 10e3 / 330 = 42 pV/rtHz

This is way way below both the voltage noise floor and the sensor
inductance contribution from current noise.  Even though the sensor
impedance is lower, the inductive drop is pure diff mode, so sees no
CMR.  The imbalance would have to be quite dreadful in order for this
effect to matter.

 ==> Note that currently the input diff mode capacitance is
     considerably lower (330 pF). The CM voltage would be higher, but
     the imbalance should also be lower because the diff mode cap is
     more dominant, so probably a wash.

The input imbalance is a worse-case for the CMR, because CM signals
will have some degree of balance, if for no other reason than that
they are shunted by the sensor.  Since the sensor cable is short we do
expect that whatever CM capacitance there is in the passband will be
dominated by the input network caps.  So in practice the main input
balance would come from the sensor impedance. This would be somewhere
in the 3x-30x ballpark.  So CMR might be 60 dB to 80 dB, considering
input network alone.

The amp CMR itself should be rather better than this, so we would
likely end up dominated by the input network.  But in a CMR test with
the +/- inputs shorted we would see the amp contribution alone
(unrealistic).  A better test would be injecting through two smallish
capacitors that are matched or slightly mismatched.  Note that an
input capacitive balance trim would only help CMR when the input
imbalance is worse than the CM interference imbalance.  That may be an
unrealistic assumption, especially if the amp input is balanced to 1%.

However the *diff* mode capacitor helps to maintain balance even for
the external CM source impedance.  This works because the diff mode
source impedance is much lower than the CM source impedance, so we can
balance the CM input without greatly attenating the diff mode signal.
So a lower sensor impedance helps with CMR, especially if we reduce
the diff mode filter impedance to match.  But a higher inductance
sensor also likely has higher sensitivity, which will offset this
benefit.

Hmmn, just realized it could be an interesting part of the input
network to have a shunt resistor R12 directly across the amp inputs.
This can damp the diff mode sensor resonance, and improves the input
balance, but is also a noise source.  For that matter, the snubber
resistor R6 is also a noise source.

If R6 << reactance of C5, then we can model this as a voltage noise
driving C5, generating a diff mode current.  So, with C5 = 3.3 nF,
R6 = 200 Ohms:
  Johnson noise = 1.8 nV/rtHz
  Xc = 3.6 kOhm
  Induced current noise = 0.5 fA/rtHz

That is a small contribution.  But at values where R12 has much effect
on damping and AC balance, it is also a considerable noise source.
For example, current noise of 2 kOhm is 2.8 pA/rtHz, >> the amp
current noise.  Higher values of R12 have minimal AC benefit, but do
improve the DC balance wrt. the bias current.

Whether R13/R14 or R12, the input shunt resistors also improve the
balance at very low frequencies, like 60 Hz.  If R12 dominates, then
the matching of R13/R14 becomes relatively non-critical.  With the
diff mode shunt impedance of 100k then (neglecting the shunt effect of
the sensor) the lowpass rolloff starts at 1.6 kHz.  But we shouldn't
ignore the sensor, which dominates the impedance in the LF range, low
carriers and below.  eg. the 20 mH dipole sensor reactance is only 38
Ohms at 300 Hz (the resistance is 180 Ohms).

  ==> Hmmn, the 180 Ohm sensor resistance has 6 dB damping effect with
      20 mH sensor.  And is of course also a Johnson noise source.
      1.7 nV/rtHz.  Another reason why LT1028 might not be the best
      amp for this sensor, especially considering price.

  ==> Oh, Duh!  The bias current is also shunted by the sensor, so at
      DC the diff mode voltage due to bias is quite small.  Likewise
      for the idea that R12 improves the DC balance


Starting to think about part choice for the reference buffer and the
ADC driver.

I see that the V+/V- supplies are asymmetrical on the DC2222A eval
board for LTC2500-32.  +6.6/-3.3 This makes sense given the 0-5 swing
at the ADC inputs.  I was specifically thinking about this for the
reference buffer, but the DC2222A is not directly relevant, because it
uses a standalone reference driven off of the unregulated 10V nominal
supply.  Given that we have 12-15V supplies, it would be an extra
nuisance to have to regulate them down just for the reference buffer.
But the reference doesn't need to swing anywhere near ground, so it
could be run off of say 3.5, 7.5V rails, where the negative rail can
be a zener to ground.  Or if the amp tolerates 12-15V, off of just the
+15V rail.

Chopper amps:	pp noise (uV)	0.1% settling 	En nV/rtHz	Supply max V	Street price 
    OPA189	0.1 		1.1 us		5.2 		36		$2.50
    OPA180	0.25		30 us		10		36		$1.79
    OPA188 similar to OPA180, a bit lower En
    ADA4522-1	0.12		12 us		5.8		45		$2.83
    LT1028	0.035		slew 15 V/us	0.87				$11.50
    OPA27	0.09		25 us 0.01%	3.3				$3.76
    
How low does the p-p noise *need* to be?  The LTC2500-32 input noise
is 6 uV RMS at 16x oversample.  These are not exactly commensurate
because the reference noise is not additive, and the reference noise
matters down to very low frequency.  A gain modulation at or below the
broadband noise floor will not be visible, but the p-p value of the
broadband noise is 6x.  But also the reference gets 2x gain allowing
for the differential input.  So if the reference p-p noise equals the
RMS ADC noise (6 uV), then the reference will contribute negligibly.

Note that the *total* noise at the reference input from 0.1 Hz to 1
kHz needs to be < 2 uV RMS to contribute negligibly, not just the 0.1
Hz to 10 Hz.  But swamping the reference buffer with output
capacitance will help with the broadband contribution.  I had been
thinking that the in-band broadband reference noise (eg. at 10 kHz)
was the problem, but that is not true, or is at least not the whole
story.

I'm not sure how the in-band reference noise contributes.  The
reference LF noise eventually shows near DC after demodulation, but if
we look at the effect on the ADC output, what we see is
base-broadening of the carrier.  The reference noise has no
contribution when there is no carrier.  What's going to happen is that
the signal will intermodulate with the full bandwidth of the reference
noise.  For the white part of the noise, this is going to be spread
uniformly in the ADC noise bandwidth (which inevitably exceeds the
signal bandwidth to some degree, and will get aliased).  Since the SNR
is very high, the input spectrum is strongly concentrated in the
signal channels, and the broadband reference noise intermodulation
with the signal will be spread, reducing its in-band density somewhat.
The LF noise is going to be a lot higher in amplitude anyway, due to
1/f and lower attenuation of LF broadband noise.  So I think it's OK
to just consider the baseband noise in the reference.

To first order, the noise of the common reference (pre-distribution)
does not matter at all.  But we might as well start with a low 1/f
noise reference, and heavily filter to minimize contribution from
white noise in the baseband range.  Not having high frequency noise
will presumably help with the second order effects of eg. frequency
response mismatch between the source and sensor paths.

Reference 1/f noise is much higher than low noise opamps, in the 1-5
uV range for 0.1 to 10 Hz.  This gets up into the range where it might
not be negligible wrt. ADC noise, if we did not have a common
reference.  The LTC6655 does seem to have lower 1/f noise that than TI
REF102 and REF5010. The TI datasheets a bit unclear, because the
reference noise seem to go up proportional to voltage, and the REF5010
comes in several voltages.  Assuming that the 3 uV noise spec is for
the 10V version, then it's in the same ballpark as the LTC6655.  The
TI parts are cheaper, especially in the less precise version, and have
a 10V version, unlike the LTC6655.  Price is not a huge deal, since we
only need one for the whole instrument.  The LTC6655 specs the noise
as 0.25 ppm, which would be 2.5 uV at 10V.


 ==> Thermocouple noise due to eg. air circulation might be pretty
     significant down in that range. OPA27 datasheet has example of
     0.5 uv p-p noise from "normal lab bench-top air currents".
Differential reference distribution could help with thermocouple noise
if the routing is well matched.  I had been thinking of that mainly
for HF rejection, and so moved away from the idea because HF noise can
be filtered out with capacitance, and each processing stage adds
noise.  We would not need a very fancy differential amp, just a
difference amp with no high matching.  Assumption is that thermocouple
noise would mostly be related to the main board, connector, etc.

I suppose the reference could be distributed at eg. 10V, which would
reduce any distribution noise by half.  It's easy to make a gain 1/2
difference amp.

Note that bipolar amps have even lower p-p noise, as well as low
broadband noise.  They just have larger offset and more thermal drift
(which are not so critical in our application).  Also, the bipolars
have much bigger bias current, which can affect accuracy considerably
with high impedance RC filters for broadband noise.  A bipolar with
low 1/f noise might be fine for the ADC buffer and any distribution
difference amp, but would not be so good for the master reference
buffer, which wants to have a really low corner to minimize broadband
noise.  The various bipolar audio opamps might be suitable, but aren't
going to have a 0.1-10 Hz noise spec.  A low 1/f noise corner is a
good clue, like the NJM2114.

It seems like a single fast bipolar could be a reference buffer
solution.  Also, it isn't entirely clear why the buffer even needs to
be fast.  When discussing the input driver, for the "fully averaged"
operating mode discussed in the LTC2500-32 datasheet, the downside is
a gain error, but this comes from the series R in the input decoupling
RC filter.  The problem with reference buffer settling with burst
conversion is not an issue because we will convert at a steady rate.
The reference current is up to 1 mA at 1 Ms/s.

The reference buffer cost and footprint is much more important than
master reference because there is only one master reference, whereas
there is a buffer per ADC.

The settling spec seems like a good speed spec related to the output
disturbance rejection, though it isn't directly relevant, since it is
a 10V feed-forward step (I think this mostly reflects slew rate).
Possibly with the OPA189 you could skip the high-speed composite
buffer.


Some fully-differential amps (for ADC driver) are also +/- 5V (like
most high speed amps).  The audio fully differential amps are an
exception.


[27 Nov 19]

Scatter plot pose error vs. norm of coupling.

Double check that the "true pose" kinematics are correct.

In high residue points in calibration optimization, look at the
predicted vs. measured coupling.

Can also find the partial derivatives of mismatch vs. small change in
the pose. This lets us see which pose component is causing the
problem. (From the error vector plot, there is a big interaction
between the extra angles and Y translation.)

Another idea is to do linear regression to see what pose elements or
coupling elements predict calibration mismatch or pose error.

What happens if we solve for the pose of a 2 angle set using 1 angle
calibration? This will tell us how the fixturings disagree. Similarly,
if we calibrate using only 2 angle data, how does this disagree with
the 1 angle calibration.


For the gain 1/2 balancer, feedback R is say 250 Ohms, then input
  resistor is 500. This gives 10 mA peak output current, and Johnson
  noise of 1.6 nV/rtHz.  This is in the ballpark of negligible wrt the
  ADC 5 nV/rtHz (if we have lower noise F-D amp). Not sure whether the
  balancer RTI noise is 1/2, if it is we are extra good.

 ==> NOT! ADC noise RTI is more like *30* nV/rtHz.  So noise after
     first stage can be a lot higher. Allowing for gain 1/2, total
     noise of stage 2 and antialias would be negligible at 20 nV/rtHz.

But either way the ADC noise is 10 nV/rtHz when referenced back to the
balancer input. This gives us some slack in the diff amp and
antialias, tho we don't reach negligible. The diff amp has noise gain
of 2, and something like 2 nV/rtHz Johnson noise at the input. Say
maybe 5 nV/rtHz , if the opamp noise is 1 nV/rtHz. But referenced to
amp input this gets divided by stage 1 gain of 10-20, so perhaps 0.25
to 0.5 nV/rtHz, < 1/2 of the stage 1 input noise.

The noise of the ADC itself RT amp input is 5/gain, where gain is
perhaps 1-15x (including the 1/2 in the balancer).  At gain 1 ADC
noise alone is only mildly dominant.  At higher gains we maximize the
SNR, but at the cost of reduced dynamic range for distance variation.
Gain of 5 or 10 is probably where we want to be. This depends on
sensor sensitivity vs. ambient EMI and also the application need for
low noise vs. workspace span. If EMI is high wrt amp noise, then might
as well run at unity gain (tho if that is the norm then we could
increase amp noise).

A 2x noise reduction gets us 4'th root more range, or 20%.  Eg 240 mm
instead of 200. But the workspace volume increase is linear with
1/noise.

Since we are going from 20 VPP to 10, this is actual gain 1/2.  I
think. Anyway, we can fix resistor values later, thing is to get the
board going.



[25 Nov 19]

Not so sure the GIC filter is a good idea. It's partly because of the
4'th order that the noise gain is so high in current filter. Also, the
filter is running on a 4x lower signal than we plan.

But the multiple feedback architecture has better HF stopband
attenuation than Sallen-Key. Was playing with the idea you could make
a balanced GIC filter, but then you are probably depending on common
mode rejection of the filter and so. Third order is recommended, but
then you are back to high output impedance.



[22 Nov 19]

Summary:
 -- Use the LTC2512-24 instead of the LTC2500-32.  It's 4x cheaper,
    pin compatible, and performs the same at our low oversampling.
    The LTC2380-24 is interesting because it comes in a leaded
    package, but is nearly as expensive as the LTC2500-32.
 -- We do not have very many digital I/Os with the main board, 3 out
    and 2 in.  So 1 driver and 1 receiver chip.  Connector needs maybe
    42 pins, with 1/2 of the pins grounds.
 -- Ongoing calibration saga.

Was starting to look at BOM for in input board, and got some sticker
shock on the LTC2500-32, it's $60 each.  But they are doing some price
discrimination and there are similar parts available at much lower
cost.  In particular, the LTC2512-24 is pin-compatible with the
LTC2500-32, and seems to have about the same dynamic range for our
purposes.

It only has the 'flat' filter, and doesn't output as high a resolution
at the SAR stage, 14 bits vs. 24, but it has a higher sample rate, 1.6
Msps vs 1.0 Msps.  The higher SAR resolution does not have that much
advantage, though, since at the higher sample rate we can 32x
oversample instead of 16x, at which point the dynamic range is the
same.  For the LTC2500-32, the ENOB of the full rate output is only 17
bits, so those 24 bits are somewhat illusory.  

There is no SNR spec for the for the full-rate ouput on the
LTC2512-24, which might be concealing something.  It is possible that
the LTC2512-24 might have higher SAR resolution internally.

[26 Nov 19, block diagram on datasheet shows 2512 has 24 at the SAR
output bits, likely the exact same front end as the 2500-32.  Also,
this part has no SPI input, is configured for decimation by SEL0/SEL1
pins, 4, 8, 16, 32.]

The LTC2380-24 is also quite interesting, partly because it comes in
MSOP as well as DFN.  It does not have the full rate ouput, and is 16
pin vs. 24.  And it runs at up to 2 Msps.  The 2380 only has internal
averaging, no FIR filter.

Possible averaging is actually good, though.  There are various ways
we could compensate the passband droop, either just sampling higher
and then doing more points in the FFT (perhaps tweaking the FFT window
to flatten the response), or using a small FIR filter.  The averaging
is still nice because it lets us operate with a lower bit clock.  An
advantage of the averaging mode over the fancy digital filters is that
it lets us pick any convenient oversample, not just powers of 2.  I
think the 2512 has optional averaging output also.


It seems maybe "transition noise" is a good spec for comparing the
noise levels:
  LTC2380-24: 13.6 @ N = 16, fSMPL = 2Msps
  LTC2512-24: 10.5 @ DF = 16 (?? 1.6 Msps)
  LTC2500-32: ??? no spec, working from dynamic range maybe 5??

Lets look at dynamic range specs:
  LTC2380-24: @ 16x, 2 Msps, (SINC1 only) 113 dB
  LTC2512-24: @ 16x, 1.6 Msps, (flat only) 114 dB
  LTC2500-32: @ 16x, 1 Msps, SINC1: 115.5 dB, flat: 116.8 dB

It seems there is roughly no difference in the noise in our operating
regime after we allow for the the sample rate variation.  So I'm
thinking the LTC2380-24 unless some gotcha shows up on closer
examination of the datasheet.  Avoiding the QFN is a really big plus
because I am much more confident on soldering an MSOP.
 ==> Hmmn, the price difference is not as much as the AD budgetary
     pricing would suggest, $56 vs $63.  Oh, it's the 2512 which is
     cheaper, $19 on DigiKey.  Hmmn, I guess I could try the QFN, and
     switch if I can't get it to solder. 

THD and SFDR comparison is a little confusing because of varying
conditions and specs.  Possibly distortion is up to 10 dB better on
2500.  But all are around -117 to -128 dB, which is not a big deal for
us; we have a lot more THD elsewhere, and the distortion filter works
well.

The AD7768-1 Sigma-delta is also in a similar noise ballpark.

Uh, there's also the LTC2508-32, which is the one I had remembered,
which only supports DF > 256, which is no good for us.  This is at a
similar price to the LTC2512-24.

The AD7684 100 Ksps 16 bit SAR in MSOP might be good for driver
feedback.  It has full differential inputs and a reference input.  We
don't need as high resolution because we can live with lower bandwidth
on the sense, and we don't have to deal with level variation over
distance, so can set the levels near full scale.  Lots of options
there, really.


One thing I'm noticing wrt. the BOM is that the number of digital drivers (and
the real estate) that I need is a lot less than I was imagining.
See [8 Aug 19].
    3 out: Per channel we have 1 output, so 3 for 3 channels
    4 in: All channels have SCKA, SYNC, SDI and MCLK in common

So that's just 2 quad chips, 1 driver and 1 receiver.  Then we need
LVDS input termination resistors, back-termination resistors on the
on-board signals.  But that's just something like 6 1206 resistor
arrays.  (Maybe we want a serial EEPROM for calibration, but that's
really getting ahead of things.)

The ADC itself is also tiny, so our real estate is almost all for the
analog, both the input path and the reference buffer.  The analog is
going to pile up, especially if I use SOIC opamps and larger
discretes.  And my fancy input protection networks.

Also, likewise, we do not need all that high a pin count in the
connector.  If we make 1/2 the pins ground (which seems a good idea
especially for LVDS), then each LVDS line needs 4 pins:
    28: LVDS 3*4 + 4*4
    8: Supplies 4*2 (paired with ground) +5, +3.3, +/-15 
    2: Reference + ground
    4: I2C

    = 42 pins

I'm tending somewhat toward a header rather than an edge connector,
because of possible costs for gold contacts in prototype PCBs.  But I
guess ENIG finish is not too exotic.


I made the 90 degree angle bracket so we can get Ry Rx rotations for
testing and calibration.  There have been problems so far, it seemed to
be sort of working for the small dataset to calibration and check.  We
calibrated, then ran the same data as a test, and got errors maybe 2x
worse than with the one-angle calibration.

But then the sensor wire got cut, and now the Matlab code is having
problems with even the small data.  We erroneously thought that the
big/small was the problem, but it's just that the wire got cut during
one of the big collections.  Maybe some of the hacks to try to get the
pose to converge (mainly state constraints) are a problem?  IDK

I made a "Z axis calibration fixture" which is a round nubbin that
fits in place of the sensor.  This is intended to let us make sure
that the stage Z axis and our rotations on the bracket have
intersecting axes.  With my first go at this the other day, trying to
manually move the pole around, I gave up because the settability was
poor and I wasn't sure how to interpret the runout.  As it happened it
was perhaps 2mm, which would definitely contribute to the worse error
we were seeing with 2 angles.

It worked much better fixing the runout by changing the axis offsets
in the Labview motion interface It seems that the top end of the stage
pole lurches around some 30 microns when the stage is in motion, but I
eventually got the runout to about 20 microns, which is really kind of
gratuitous, and likely not reproducible because of non-reproduciblity
in the stage itself.  But the Rz alignment, which I had indicated in
the other day was still pretty good, around 20 microns.

Also, the sensor post was not quite sitting back against the alignment
step on the angle bracket because of the inside corner radius.  I
relieved the corners of the post and now it sits in there nicely.
This should only be a very minor effect, 100 microns at most.


[21 Nov 19]

The FIN1047/FIN1048 LVDS chips look good. Cheap and multi-sourced,
with flow thru pinout, 3.3V.  There are not really any slow LVDS
parts; the slower ones are just older, and mostly not that much
slower.



[19 Nov 19]

Calibration DOF revisited.  See [17 Oct 19] for previous discussion.
This is an updated comparison of the known redundancies, ambiguities
and compensations by removing DOF from the state:
    +6 source fixture transform
    +6 sensor fixture transform
    +1 source/sensor moment scale ambiguity
    +1 source/sensor fixture ambiguity (with only Rz cal data???)
    
    -6 source Z position [0 0 0] source Z moment [0 0 1]
    -5 sensor Z position [0 0 0] sensor Z moment [0 0 k]
        We allow the source and sensor Z coil to define the position
        and orientation of the source and sensor.  This compensates
        for the excess DOF introduced by the fixture transform,
        forcing the overall source/sensor position and rotation into
        the fixture transform, which is what we want Forcing the Z
        source moment to 1 compensates the source/sensor scale
        ambiguity.
        
    -2 source, sensor X moment [k1 0 k2]
        Forcing the Y component of the X moment to 0 fixes the the
        rotation about Z.  This is another step in moving extra DOF
        from the calibration model into the fixture transforms.
	
    -1 sensor fixture Z = 0
        This deals with the fixture ambiguity on Z.

This leaves us balanced.  When we added fixing of the source X moment
Y this greatly speeded the optimization.  We had previously noticed
that there were still extra DOF even in the dipole model, because two
somewhat different calibrations could work equally well.

This accounting is assuming that the full magnetic model (coil
positions and moments) would be identifiable.  Then we account for the
extra DOF in the fixture transforms, and also account for any
ambiguities that we can think of or observe during the optimization.

So far as adding DOF, the fixture transforms are straightforward, and
the scale ambiguity is also something that we had already expected and
allowed for. The fixture Z ambiguity is something that we observed and
compensated. This one is a little more fuzzy.  Why does this ambiguity
exist, and only on the Z axis? I think it is because with only Rz we
have no way to pin down the origin along the Z axis. Adding rotation
using the new 90 degree sensor fixture should allow the sensor fixture
Z to be fixed, and then the already identifiable source->sensor Z
offset will in turn pin down the source fixture Z.

So, what do we expect to have to unfreeze with the 90 degree fixture?
It's only the sensor fixture Z.  Everything else that's fixed
basically has to be.

We could fix the Z coil in a different location in order to relocate
the the origin.  I had been thinking of locating the origin at the
axis intersection, which would give somewhat more magnetically
sensible numbers for a single "source sensor distance", and would
nominally zero the Z position of the XY coils.  This is separate from
the idea of having the source/sensor coordinates be precisely
mechanically defined.  Then we would need to unfreeze almost
everything, because the as-built coils would deviate from the
mechanical reference.

Um, so what's next after calibrating and testing with the fixture
rotations?

Me:
 -- What is going on with the first two points?  Is this some bug, or
    is it to do with where those points happen to be located?
 -- Do we need more precise alignment of Rz with the angle bracket?
    This could spriously increase apparent error.
 -- Check in new code.  Code review with Claudia

New calibration pattern?  Or can we just do calibration on the big
data now that it is faster?

Claudia:
 -- Figure what workspace we want to calibrate and test on.
 -- Do linearity visualization showing error along a straight-line
    path. 
 -- Documentation on how to do calibration
 -- Paper

What plots do we want in the paper?
 -- Error vectors
 -- Table of quantitative results
 -- Integral/differential error, one linear axis (X) and Rz

Use test data taken at different points than the calibration.  Maybe
just add a modest Z offset and Rz rotation to the pattern?

Can we get away with the approximate "quadrupole"?


[17 Nov 19]

Summary:
 -- Input protection, trace routing, issues with return path for CM
    currents on inputs.  Leave room for experiments here by having
    cuts in the ground plane, but then also place footprints for
    capacitors or zero ohm jumpers.
 -- Thinking ahead about main board grounding, and also the digital
    source driver.
 -- Driver power supply is an issue because the voltage/current we
    want is not easily available off-the-shelf.  But we defintely
    don't want to make our own off-line SMPS, and would like to avoid
    any custom magnetics.  One idea is to use a stock supply to prvide
    line isolation, then use boost or flyback converters on the driver
    board to generate the needed voltages.  The stock supply could
    even be an unregulated iron transformer and rectifier.


Looked at input filter components a bit.  Turns out 1% COG capacitors
are available fairly reasonably at 1 nF 100V, 0603.  At higher voltages the
price and size go up rapidly, > $1, >= 1206.  Not clear we really need
a HV capacitor.  One one hand, at 1 nF our capacitance is only 10x
bigger than the ESD model capacitance, so at eg. 10 kV we could charge
up to 1 kV.  But insulation breakdown is always time dependent.  I
expect that charging up to 1 kV for 10's of microseconds would be
tolerated many many times.

So, one question is the duration of the overvoltage.  Something I also
realized is that beads and CM inductors are going to saturate fairly
rapidly during a transient, 1 us or less.  For ESD this may be long
enough to ride through the entire thing, and in any case it will slow
the leading edge and peak current, which will reduce stress on
downstream components.  Supposing charge is dumped in the input,
bringing the capacitor up to 1 kV.  For the CM inductor, 5 mH @ 0.5A,
we would saturate at about 2.5 us.  Then, unless something else gets
it first, this charge is going to be dumped in the TVS.  But the
energy is tiny, 0.5 mJ.  TVS don't seem to be rated in Joules, but
500W for 10 us is 5 mJ.  So, I guess one observation is that energy is
lost (in resistance, etc.) when you transfer from one capacitor to
another.  In any case, it's not surprising that a TVS can tolerate an
ESD zap.  Also, clipping the fast rise of the pulse should greatly
help voltage overshoot, since dI/dt is reduced.
 ==> Not clear that ESD even would saturate the CM inductor, since 5
     mH @ 0.5 A is 1.5 mJ.  Higher energy zaps, though.

There are low-capacitance TVS diodes with capacitance around 5 pF.  At
this level, the capacitance mismatch of the TVS is not a big
contributor to imbalance in the filter.  The capacitance nonlinearity
is still less than ideal, because it will create some harmonic
distortion.

Electrosurgery discharge is not such a big deal wrt protecting input
from damage, eg. a direct hit on sensor cable, because it is HF and
high impedance. Just a 1 nF capacitor is 10 Ohms at 13.5 MHz. I think
the current is only an amp or two at most. The cap, TVS, etc, might
overheat if the discharge continues, but the voltage should be
clamped.

Main concern with Electric Fast Transient (EFT) seems to be from the
AC line, upsetting digital functions thorough transients that make it
to the supply output (mainly common mode, I would expect). I guess ESD
protection on signal lines is fairly adequate?  I guess the power
input to the main board should be treated like an external line,
filtered and clamped to chassis ground, then CM filter and send on to
the power distribution and ground plane. Maybe the ground plane just
joins the chassis ground at that point.

Build some moats into the ground plane for isolation of analog,
digital, etc, but with pads for jumpers to short across the
moat. Resistor, either zero ohm, or possibly low value, to see what
the current is.  Moats between channels, and also reducing the width
of the connection between channel and main/digital area.

Put LDOs on each input board to drop a couple volts to clean up the
analog supply, like 15 down to 13.5, or something. Also, major LC
filtering on entry to the main board to remove residual switching
noise and ripple.

I think why they say to ground only one side of the board to chassis
is so the noise currents don't circulate in the enclosure, and the
dual, chassis currents like ESD don't go through the ground plane. Why
is it not a problem to do a whole edge rather than a single point?
There shouldn't be much current at the board edge parallel to the
edge, so not so much voltage differential along the edge.  This is why
high speed circuits are placed away from the chassis ground edge.

The input board will connect to the chassis at the input connector,
but that is a chassis ground island that is largely floated from the
analog ground. Maybe some guard traces between channels running from
chassis to the plane region for that channel, but not a broad low
impedance connection. The signal is isolated by the CM choke. We do
want a ground connection to give a return path for CM currents on the
input lines. We could use a three line CM choke to isolate the ground
also, but I think that would make the ground too high inductance due
to leakage inductance in the CM choke. Maybe it would be better to
just skip the guard traces?  Thinking about the loop created by
chassis ground Main board analog ground Card common ground Input
channel ground Input lines (with or without ground) Chassis ground at
connector

The CM choke does add a lot of inductance, and common mode rejection
will help to minimize effect of voltage coupled into the loop. But it
is a big loop. It would be nice if the input lines entered the input
channel ground on the bottom, across the bridge to the common
ground. But this would also couple noise into the input, and is
awkward for layout because there is no straight input to output layout
pattern. You would have to run the input past the output.

Note also that the relatively long parallel runs of the input traces
with the guard grounds will create mutual inductance, like a common
mode choke at VHF.

Thing is, the loop is still there even if you don't have the guard
ground traces. If they are there, and not blocked by leakage
inductance, then they give a return path for currents intercepted by
the CM capacitors at the amp inputs. This should be lower impedance
than the big loop, but not so low as to encourage much current to
circulate in the big loop.

The amp only has CMR at the VLF signal frequencies. Main concern for
coupling into the loop would be HF/VHF that already got inside the
enclosure somehow, which could create rectification problem. This is
supposed to be bypassed by the CM caps at the amp inputs, but the more
current there is, the more voltage there will be at the input.

Perhaps there would be virtue in adding a small capacitor between the
input board chassis ground and say the input board main ground. This
would reduce the size of the loop at HF. Or maybe shield the input
traces on an inner layer between an extension of the input channel
ground and the board chassis ground. Neither of these extensions are
connected at both ends, but there is a fair amount of capacitance
between them. 10's of pF? Whether explicit or built-in, this connects
things at HF, while still avoiding a loop at ELF.

Not sure whether to keep the guard traces in the plane overlap
scheme. They could be connected only at the channel ground end, and
tied into the channel ground extension by vias. Then they serve mainly
as E-field shielding between channel input traces.

Don't want the capacitance of chassis/channel plane overlap to be too
much, since this is shunting the CM choke, undermining its effect. But
the choke still impedes current on the signal lines; the current that
is coupled is shunted by the quite low impedance of the channel ground
and the bridge. This current may be relatively high, but it is a far
lower impedance and less sensitive circuit than the amp input.

Shunting the CM choke is even less of an issue with a capacitor
between the input chassis ground and the board main ground because
that current does not go through the input side of the channel
ground. Any CM current goes over the bridge to the main ground, and
then has a shorter path back to chassis through the cap. Note that the
input side of the main board is not tied to chassis at the front edge,
so without the cap there is an extra long path back to the chassis
ground. Thru the connector and across the plane, to the chassis ground
we go...

I guess it wouldn't hurt to have stitching capacitors and plane
overlap between chassis and analog ground at the front of the main
board in the input section. These could be maybe 1-10 nF. We just
don't want ELF currents across the input area into chassis that might
detour into the input cards.

We could connect the guard traces at the chassis end thru a small
capacitor. The resulting loop will still have a fairly high impedance
at HF because of the high impedance of the guard traces (without the
plane overlap). Idea is that HF return currents from CM capacitors at
amp inputs will go along the guard traces because the loop is so
small, even though it open at VLF.

We could put moats in the main board input area separating each card
slot.  But there shouldn't be much current flowing in that direction
anyway, and there are no analog signals (except for the reference,
which is heavily filtered).

Maybe place SMA and series resistor footprints on one of the channels
(both lines) right at the amp footprint so that we can probe what is
making it thru to the amp. Not so much the actual signal, which would
be loaded down too much (if we terminate the cable), but the HF and
ESD. Likewise, we want to probe the actual ADC inputs for that
channel. All channels should have test points at the ADC, but
connectors on one channel would be nice. Reference too, I guess.

We can place caps to connect input main to input chassis and guard
traces to input chassis, then experiment with/without. My thought is
to not do the plane overlap, but shield the input traces on both sides
with extensions of the channel ground. As well as an electrostatic
shield, this adds capacitance in parallel with the CM capacitors at
the amp input, and will help to shunt VHF.

Stitch the two the sides of the shield with vias, but maybe not the
guard traces? Thing is, we are leaving open the option of connecting
the guard traces to chassis ground thru cap, but we don't want such a
connection to be very low impedance except wrt the currents on the
tightly coupled input traces.

The need for a return path for CM input currents is really only a
thing for VHF and up. At HF the ground bridge is not much of a
barrier, while the moats are useful for isolation.  160 pF is 10 Ohms
at 100 MHz. Likely I would not see any effect from most of these
grounding variants except under carefully contrived circumstances. But
if we had a loop then that could create pickup at ELF, where the
enclosure is also an ineffective shield. So my suspicion is that
having the guard traces connect to chassis is not a win, but a 1-10 nF
cap between input chassis and input main ground may be a help at
HF. Most of the VHF CM at the input should already be shunted to
chassis ground by the CM caps there. Eg. 1 nF is 1.6 Ohms at 100 MHz,
while the bead is 100 Ohms. And for internal VHF pickup, having a
connection is probably only hurting.  The signal current is
differential so does not need a return path (and is ELF anyway).


Can set up a board slot for digital expansion, the idea that
application logic such add Micron can run on the zedboard. Also need
connection to the digital source driver. This interface can also be
used for the interim DAC outputs.

We want the input board slots to be on a clean corner of the analog
ground plane, where the only digital activity is the data transfer
synched with the ADC idle time. So the expansion slot needs to be over
on the other side of the main board. I guess the analog ground around
the input slots does *not* connect to the chassis at the front
edge. This keeps enclosure currents out of the analog ground. Recall
also that the board is mounted on a chassis which is itself only
connected to the enclosure at the front panel. So currents on the
enclosure have relatively little reason to go thru the chassis.


When I do the driver board, we could consider putting the feedback
ADCs on the driver board, and using lower resolution cheaper ADCs. 16
bits should be plenty, maybe even fewer. It seems like overkill using
an input board just for feedback. $$$, not only the ADCs themselves,
but the input sections, reference buffers, etc.

Note that we also need voltage sense, if only to make sure there is no
DC offset. Without the internal feedback of the analog driver there is
otherwise no closed loop control of the DC offset. The current
transformer has no DC response, but will saturate if there is much DC
at all (a problem for DC even way below the signal level).

Power supply for driver is a nuisance. +/- 80V 0.5 A supplies are not
an off-the-shelf thing. I don't want to make an off-line SMPS, but
Chinese import supplies for audio amps are not also that great from a
safety perspective, and are much higher power than I need (big). Also,
regulation would be nice. The post regulators shouldn't have to deal
with AC line variation. One possibility is to use 24 or 48 V supply
for line conversion, then have a forward converter on the driver board
which generates the HV split supplies. This does increase the
complexity of the driver design, but evades the safety issues.

The off-line converter could be unregulated. One possibility would be
to use an iron transformer on the chassis, and then route the AC
direct to the driver, with the rectifier and filter caps on the driver
board. If you have DC you could run that in too. The supply could also
be on a separate board.

Hmmm, if you had a center-tapped transformer, then you could get split
rails and use two boost converters, one for each rail. Can likely use
stock inductors for that, avoiding custom magnetics. FWIW, each rail
is then separately regulated. Or use a flyback converter to generate
the negative rail from a single unregulated input. Flyback can do both
boost and polarity flip; it needs a transformer, but the turns ratio
isn't critical, could be 1:1 pulse transformer.

This is nice because the supply issue is all on the driver board, so
people could choose from a wide range of supplies, and not have to
source obscure supply boards. The converter transformer would surely
be a custom design, though. But really, I think hardly anyone is going
to build these boards themselves. But you could sell just the boards,
and leave the packaging, etc., up to the user.


[14 Nov 19]

Need to consider effect of input bias resistors. In particular,
current noise is nearly 1 pA/rtHz. For high impedance sensors we would
want to reduce this. Otherwise not much point in an amp with lower
current noise.

Also, not clear what degree of CMR to expect for voltage noise from
these. Any mismatch will contribute to input imbalance, and total
imbalance of all input components affects CMR. Increasing bias
resistor will increase CM noise, putting bigger demand of CMR. Would
think we could go up quite a bit, though.


[13 Nov 19]

Summary:
 -- Input filter network simulation and design.
 -- EMC discussion, filters, grounding
 -- First stage opamp selection, comparison table
 -- Stage fixturing for XY rotations
 -- Reference distribution
 -- No dynamic gain switching, but settable by jumper blocks.
 -- Digital driver and feedback discussion.


FWIW, the current input filter is:
         ferrite bead x2
	 220 pF x2 common mode
	 290 uH CM choke
	 100 pF x2 common mode
	 3.3 nF + 1211 Ohm diff mode snubber

It is preferred that there be a differential-mode capacitor which is
much larger (10x) than the CM capacitors, to reduce the CMR reduction
due to mismatch in the diff mode caps.  But this gets awkward with the
high inductance of the sensor coil.  We sort of have this, except that
the shunt impedance has a considerable resistive component in the
passband, since the zero is at 22 kHz.  The zero needs to be this low
to damp the sensor resonance with the input capacitors (though we can
entertain the thought of "what if the resonance is not damped").

Messing with an input filter simulation:
    6dof/magnetic/cad/in_filter.sxsch

At least so far as the input filter is concerned, 5% imbalance in the
CM capacitors is not being a huge problem.  When sensor is 20 mH, the
resonance is starting to get pretty low, though.

A good setup seems to be:
         ferrite bead x2 (L1,L2)
	 1 nF x2 common mode (C1,C2)
	 5 mH CM choke (TX1)
	 100 Ohm x2 series resistor (R8,R9)
	 1 nF x2 common mode (C3,C4)
	 3.3 nF + 100 Ohm diff mode snubber (C5,R6)

This works well with a 3 mH sensor, but resonance starts to get out of
hand with 20 mH.  According to simulation R8,R9 have little effect on
attenuation, but improve damping of the low resonance somewhat.  The
low attenuation is because the TX1 impedance is high, 800 Ohms || 20
pF.  This may be so, but it will still help with ESD and internally
coupled noise.

This model does not include the TVS.  I am modelling CM injection as a
capacitance which doubles as the cable capacitance. The filter
capacitance loads down the CM signal at the input, but that seems
realistic.  Each line has 50 pF in series with 100 Ohms.  The idea is
that the resistance models antenna impedance at VHF.  The capacitive
divider makes the source strength scaling somewhat arbitrary.  With
50+50 pF coupling I am getting a filter balance of -88 dB at 13.5 kHz.

We could do somewhat better (10 dB?) on filter balance in the signal
band with a capacitor trim on eg. C3, but this is assuming ideal CMR
in the amp.  In-band CMR is likely to be limited to ~60 dB by the amp
resistor matching.  The trim effect is biggest at HF, where the amp
has no CMR.

For HF filtering we are only concerned about the signal at the amp
input which may get rectified, since any HF signal is not going to
make it through the amp and antialias filter.  So what we look at is
the "Unbalanced" signal at the amp input.  This is -64 dB at 13.5
MHz.  (Signal coming through the antenna itself is especially
attenuated, though hard to say how much "a lot" is without a HF model
of the antenna impedance.)

The next step would be to pick actual parts for the bead and CM choke
and try to model them more accurately.  Probably can get a bead spice
model, IDK.  Also the TVS.  Capacitance mismatch in TVS could be an
issue.

The 100 pF CM coupling capacitance is awfully high, but is probably a
low value for the cable CM and diff mode capacitance.  Really these
should be split in the cable/CM model.  Could also refine the sensor
model by adding a shunt R and C.

I'll skip the rolloff capacitors we now have in the second stage amp
because they are a matching concern, and the second stage will now be
followed by a 3 pole antialias filter (with a much lower corner).


See [24 June 19] re. amp selection discussion.  The OPA2210 (super
beta bipolar) is an interesting possibility for the first stage.  En
is 2.2 nV/rtHz, but In is only 0.4 pA/rtHz.  Noise resistance 5.5
KOhm.  Compare LT1028 at 0.85 nV/rtHz, 1 pA/rtHz or 850 Ohms.  One
interesting, but probably not important, thing is that the OPA2210 has
a much lower current noise 1/f corner.  I say not important because
the sensor reactance is low at the low carrier.  But current noise
will contribute to feedback resistor noise also.  It is a dual, which
is a plus.

But with even the 20 mH sensor, reactance is 1.25K at 10 kHz.  The
noise analysis is a bit tricky because the common mode impedance is
much higher, so we rely on CMR to ignore that.  This does seem to work
with the current LT1028 amp.  Noise is way lower with the inputs
shorted together vs. floating.  But the two amp uncorrelated noise
currents will vector sum to sqrt(2) larger.

For LT1028 the noise current is spec'd with matched source resistors,
which we do not have. For one thing, even if the magnitudes of the
sensor reactance and feedback resistance may be similar, they are in
quadrature.  Since the noise at high source impedance is roughly the
same with one or two resistors, it seems that the current noise is
rougly 1/2 correlated.  So you could perhaps double the current noise?
For OPA2210 it is not specified how current noise is measured.

The bias and offset current of the OPA2210 is *way* less, 0.3 pA
vs. 40 pA typ.  But OPA2210 still seems to have bias current
cancellation, because the bias is centered on 0.

If we say the LT1028 current noise is 1 pA/rtHz * sqrt(2) and Zin =
1.25K, then that's only 1.8 nV/rtHz.  But based on guesswork for
unmeasured correlated current noise, it could be up to 2x this, or 3.5
nV/rtHz.  This completely swamps the LT1028 voltage noise, and is also
bigger than the OPA2210 voltage noise.

While for OPA2210 it would be 0.7 to 1.4 nV/rtHz, which is in this
case less than the voltage noise of 2.2 nV/rtHz.

At 180 Ohms the Johnson noise of the dipole sensor is 1.7 nV/rtHz.
Including ~50 Ohm feedback impedance, this is 1.9 nV/rtHz.  The LT1028
current noise at high carrier with sensor reactance is probably about
1.7 nV/rtHz.  
 ==> It could be that the current noise with the dipole coil is not a
     big deal because the sensitivity is higher, so that amp noise is
     swamped by EMI, and less gain is needed, reducing amp noise
     contribution.  

Opamps:
 -- OPA2210 (dual), super beta, En=2.2 nV/rtHz
 -- OPA2209 similar to OPA2210 (super beta), seems maybe an older version.
 -- LT1037 (single) similar to OPA2210, En a bit higher, but only in a single.
 -- AD8559 (dual), low distortion, En=1 nV/rtHz, but current noise is
    about double LT1028. Less than 1/2 price of LT1028 per-amp.
 -- OPA1656 (dual), CMOS, En=2.9 nV/rtHz @ 10 kHz, 1/f corner ~2 kHz,
    OPA2156 similar.

I am not sure how to analyze noise for an instrumentation amp.  AD
MT-065 says that each input current flows in 1/2 the differential mode
source impedance.  I'm not sure I believe that for my case where the
CM impedance is high.  But regarded as a diff amp block, the IN-amp
noise analysis seems pretty straightforward.  Not super-certain about
the feedback resistor noise, though.

Would need to do more analysis, simulation and actual testing to pin
down the amp selection.  My feelz are that, especially with smaller
sensor inductance, we are probably doing better with eg. the LT1028,
but a suitable dual amp with lower bias current (super beta) might
work about as good, and be smaller/cheaper.
 ==> Main decision is single or dual, and what package.  Most are
     available in SOIC, and may have smaller packages.  With small
     packages, the advantage of dual may not be what it once was.
     Seems like duals are still cheaper, but highest performance parts
     are often single only.
 ==> I've found only two vendors of low noise (< 3 nV/rtHz) high
     voltage (> +/- 12) opamps, AD/LTC and TI/BB/NS.  Going to +/- 5
     opens options in the high speed category.  And above 3 nV/rtHz
     there are more (cheap!) audio options, like the NJM2114, NE5534A.
     3 nV/rtHz would be only a bit more than a 50% noise increase for
     the dipole sensor when compared to "best" solutions.
 ==> En < 1 nV/rtHz is probably gratuitous here, since noise of 100
     Ohms is 1.26 nV/rtHz

The fully-differential audio amp LME49724 or OPA1632 might be good for
the balancer.

FWIW, the old RFID coil, on XY, is 4.7 mH and 78 Ohms (working back
from Q spec).  Z is 7.2 mH.  The dipole sensor is certainly at the
high end of what we would want to support, both in impedance and
physical size.  78 Ohms is 1.12 nV/rtHz.  If we add say 20x2 Ohms for
input resistors + 42 Ohms for -in (45 || 500, for 10x gain), that's 78
+ 80, or 1.6 nV/rtHz.  At 13.5 kHz on Z, reactance = 611 Ohms, but
with LT1028 current noise, that's 0.86 nV/rtHz.

So, what noise should we be getting currently?  From [25 Aug 19], I am
getting 2.5 nV/rtHz RTI for the amp, with dual 50 Ohm terminators.
Looking at 1/2 amp, feedback resistance is 23 Ohms, Av=20.  Uh, let's
say that is matched resistors, so we can use just the 50 Ohm noise,
which is about the same as the En.  So 1.26 nV/rtHz per amp, or 1.8
nV/rtHz for both together.  For the second stage, En is swamped by the
noise of the 1K combined input impedance, would be about 4 nV/rtHz.
But the noise gain is 2x, so 8 nV/rtHz RTO, or 0.4 nV/rtHz RTI, which
contributes negligibly.  If we use the max En value and series 23+50,
I get 2.2 nV/rtHz.  That is getting close, but assuming max noise is
probably unreasonable.  So something is unaccounted for here.  If the
discrepancy with "typical" was due to a noise bandwidth error, it
would have to be 2x, which seems unlikely, though I never tested the
corner of the 7A22 input filters.  Compared to the "max" value, the
bandwidth error would be +30%.  But the agreement is "fair", and lower
noise than predicted would be surprising.


From phone

You want the CM filter first because that minimizes the amount of CM
signal that is seen be downstream input impedance mismatch and
converted to diff mode. This is only an issue for in-band CM.

There are reasonable size SMT CM filters up into the mH range, usually
used for power presumably.  Hmm. Eg. 399-20954-1-ND is designed for
signal, has very wide frequency range, bead-like, but also 800 Ohm
impedance. Found by low current rating. Just an error it came up,
though, since it has no inductance spec. To deal with eg. Electro
surgery, probably want excellent attenuation up to 50 MHz, but a SRF
that high corresponds to uH inductance. So if we want much LF CM
attenuation, then the HF rejection needs to be supported by other
filter stages. But impedance of 100 pF is 133 Ohms at 13 MHz, so not
much shunt there.

Need to consider bigger input cap. Ott suggests 1nF is typical for RFI
filter. As well as impedance mismatch due to cap mismatch, also need
to consider resonance with sensor coil. Could be damped by diff mode
RC snubber as now, but then even more C load. Reasonable value of
series R (from noise perspective) is not going to provide any damping
at the sensor resonance. With 20 mH dipole approximating sensor and
500 pF (two 1 nF in series) we get 35 kHz, which is ok. Need damping,
tho. Z0 = 4.5 kOhm.

Even when there is a well defined resonance, operating past the
resonance is not terrible, attenuation is flat, capacitive divider, up
until the next resonance, which may be 100 MHz. Tho SRF varies with
inductance, the parasitic capacitance will not vary much with turns
for single layer coil.

Diff mode impedance is mostly only a decade lower than CM. So for
signal filter, there is only so much benefit for CM choke vs. diff
mode. That is, the diff mode bandwidth will not be much higher than
CM. CM for signal use makes most sense when the diff mode BW needs to
be higher than CM and impedance low. But we have good CMR at signal
frequencies, and intrinsic inductance of sensor is so high that modest
diff mode inductors will not increase it much.  But if filter is a
significant fraction of sensor L then it will make current noise
contribution. Bigger issue at lower sensor L. Getting a 10x improving
is not nothing.

CM choke (or other largish input inductance) would give significant
current limiting for CM surges, ESD, etc. if it comes before
TVS. Small diff mode caps at input, perhaps 3 terminal, would reduce
any VHF emission. But eg. 100 pF only goes below 5 Ohm at 300 MHz. At
VHF the CM impedance of the cable is only ~50 Ohm.  Could do ferrite
bead, small cap, then CM choke, larger cap. Input cap in front of CM
choke must withstand KV surge. Stuff behind should be protected by
TVS. This would also improve rejection of fast ESD
transient. Everything up thru TVS is on the chassis ground. With all
this stuff in front we may not need a very big TVS.

If there were a polymer fuse it would be in front of the TVS. Not
clear what the threat model is, but for telecom and networking there
seem to be devices that would tolerate faults to AC power line,
eg. TS250-130F-RC-B-0.5-2CT-ND. These parts are not small. Smaller
devices seem more aimed at short circuit protection for supplies, have
limited interrupt voltage. But given that the inputs aren't supposed
to be connected to anything but the sensor, high power faults seem a
bit farfetched. Also, these devices would seem to have a bunch of
parallel capacitance, so non-fault resistance does not help much with
RFI.

We could also put a gas discharge tube (GDT) at the front end, which
would help to protect against severe insults, like perhaps a direct
hit on the sensor with electrosurgery discharge.  This is seeming a
bit gratuitous, though.  We don't expect lightning hits.  Thing is,
the input filter components and PCB might get burned by overvoltage.
I looked for it, and it does seem that people build deliberate spark
gaps into the PCB, with a breakover of perhaps 2 kV at 8 mil
space. This seems like a good idea.  Relieve the solder mask and apply
solder.  The wearout concern with seems to be no discharge (?) rather
than leakage.  Maybe what they really mean is the metal burns off.  I
like the idea of a gap with several triangular points for discharge
(teeth).  Points intensify the field, but also give less metal.

Then we should also have RC at opamp inputs, on analog ground. I guess
up to 50 Ohms will contribute negligibly to voltage noise if our
feedback R is 500.  50 ohms and 1 nF gives a corner at 3 MHz. But
actual source impedance will be higher, trace Z0 at least, and CM
filter inductance for external CM signals at HF. This RC at the chip
also helps limit ESD current in the chip, important because the TVS is
relatively remote, back at the connector on chassis ground. Don't
expect KV at this point, but 10's of volts could happen. RC also helps
reject digital junk coupled into input traces from inside the
enclosure.

Not quite sure what the grounding issues are wrt how the digital side
noise, on supplies and Ethernet, finds its way to chassis. We don't
want that to be through the analog section. But seems we do want a
chassis connection at the inputs. Maybe just not a fat ground plane
connection. Just guard traces? We want a good return path for CM
currents that make it through the input filter, but inductance does
not need to be that low. Guards connect to analog ground at the amp
(and RC) for that channel. Will be running over analog ground plane
for much of the way. Narrow traces are probably good. Guards also
minimize coupling between channels.

Note that although the ADC is not converting when the digital
interface is active, the analog input sections are always doing their
analog thing, and can easily "remember" a disturbance until the next
conversion. So we want to make extra sure that there is not any
nonlinear behavior in the input caused by digital activity. Linear
effects should be filtered out, at least to some degree. But we don't
want RF rectification.

Probably want a single point ground in the single ended portion of the
signal path, between the input second stage, antialias, and -in of
balancer. This is tied to the ground plane at a single point to avoid
any influence of other ground currents. Other than there the input is
differential, so should reject a variety of insults that might happen
inside the box. We don't relieve the ground plane, and bypass caps
still go to the plane. It's just that the signal ground of the
antialias filter goes to ground at a single tie point together with
the second stage output sense and the -in of the balancer.

Because of the relatively low gain, low impedance, and low bandwidth
after the first amp stage, and because the enclosure should be fairly
clean inside, it seems doubtful there would be much benefit to a
shield can on the board. If there is much of a problem we would need
to can the ADC as well as the amp. The input lines themselves are
moderately high impedance though, so be careful with the
routing. Admittedly these lines eventually go into the harsh outside
world, but internal crap will be synchronous with sampling, so may be
relatively worse, mV for mV. It might make more sense to can the
microzed.

Hmmn, inductive reactance of 20 mH dipole sensor is 1.2 K at 10 kHz. I
thought we were ~100 Ohms?

LVDS is gratuitously fast, except for the full rate output (testing
only). This may create unnecessary EMC problems. Have to see if there
are slower devices. Back termination resistor on receiver output will
reduce some crap. Could add series resistors to driver output also,
but that seems a bit weird, and would form voltage divider with end
termination. Also, I guess the driver has high Zout anyway.  Lines
from ADC to driver can also have series resistor, reduces switching
currents at ADC.


Calibration fixturing: make spacer so we can flip source on it's side,
rotating 90 degrees about X. Then also use a hex or pins to allow
fixturing at -45, 0, +45 about Z. Can't rotate source by 90 or more
except about X, since will take us out of our hemisphere. Or, really,
it is ok for calibration, but harder for testing.

Sensor we could rotate any which way, and this may be kinematically
easier to understand. But it seems a bit harder mechanically? IDK. I
guess we could make right angle plates we can move the sensor to, and
could also add Rz indexing to base, eg. by hex, or even just the
square. Also, we don't have to rotate about the Z coil as long as we
know what we are doing, or can absorb it into the sensor fixture pose
or calibration.

I'm a little fuzzy on what our stage kinematics are at the moment, and
how the re-fixturing might be represented in the stage
pose. Basically, when we run the calibrator collection we would give a
pose offset to apply to the reported stage pose, which would be seen
in the currently zero Rx Ry axes.  As long as the fixturing maintains
a gimbal-like pivot point for each rotational axis we should be able
to absorb the translation offset between the rotation axis and the
sensor origin (Z coil).

We might replace the current sensor platform with the positioning
fixture. Precise rotational indexing is much more important than for
position because of the large position change implied by a small angle
change.


Reference distribution: at each ADC, have a single pole RC with C
located near converter, and the 2 opamp composite buffer. The input RC
decouples the converters and AC references the signal to the ADC
ground.

Principle with the reference distribution is that we really want to
avoid adding noise after the path splits, especially 1/f noise. Need
to quantify the noise levels of concern for ADC and compare to specs
for reference and distribution amps. We are doing ok now with single
pole reference filter, but low carrier is probably being degraded.

I am not so worried about DC and LF offsets in the input card ground
plane, but when we look at the main board, it's bigger, and the ground
plane is going to have maybe 100's of mA of supply currents flowing in
it, connection resistance, chassis currents. So we might want a
differential reference buffer at the card level. This would also want
the chopper opamp to avoid introducing 1/f noise. Problem is a
difference amp with low value resistors will create significant load
on the reference and ground sense lines, at least a few mA. If nothing
else, this will create voltage drop in traces and across connector. So
might want buffered input. But should quantify the problem before
doing this, because more stages will only increase noise, as well as
cost. Without the diff amp, the individual RC and buffer combo create
minimal load, and only have one chopper amp worth of noise added.

When we do the digital driver the differential reference seems like an
extra good idea, since the driver is hanging off cables and has big
currents.

Note that a lot of the analog supply current will be mostly flowing in
the supplies, rather than thru ground, eg. the output feeding into
virtual ground. Only when there is a single supply or the load is to
ground do we get supply current running through ground plane. This is
probably mostly the ADC supplies and the LVDS drivers.

Should rough out how much supply current I expect the input card to
draw. Just 10 mA between 15V rails is 300 mW. We could easily end up
with 2W per channel.

Have a serial eprom for calibration. We should gain calibrate at the
card level. Likewise for the source and sensor. Maybe 1-wire?

For input protection, maybe a pair of integrated three terminal line
filters followed by per-line limiting device, polymer fuse if that is
ok for noise, or just resistor otherwise. Max resistance of say 10
Ohms, for low noise. This is a sacrificial component in the worst
case. We can experiment what to populate there. Then a bidirectional
TVS, 5-10 V or so, with capacitance of maybe 100 pF. Nonlinear
capacitance might create harmonic distortion, but we do not care too
much. We also want a common mode inductor and diff mode filter
somewhere.  The common mode coil might go various places. Before the
TVS would be best because it will limit surge currents also. The
single ended line filters should be first, though, to keep out the RF.

huh, the CM coil needs to be looking into CM capacitors. I don't think
I have that currently, it's just going into the inputs. Diff mode
capacitor is there (with damping resistor).
 ==> Nooo, I do.  See [13 Nov 19]
     

I think I'm ready to give up on dynamic input gain switching. The
benefit seems fairly low with the LTC2500, and it would be a big
nuisance to maintain smooth output with a gain step. It seems a lot
better to smoothly vary the source drive and then compensate this
using the current sense. We can estimate the current at high rate
using a KF with a rate control input. The current sense bandwidth can
be a lot lower, maybe 20 Hz. That would avoid introducing noise via
the current sense. This would also compensate any 1/f noise introduced
by the DAC or driver. (Noise on a common reference will not even cause
a current sense change.)

But I think for the instrumentation amp input I'll place maybe two
shorting blocks with gain set resistors (and one always in), so that
we can strap first stage gains of say 2-10-20-30, depending on the
jumper combination. Then GIC antialias (2 opamps), unity gain buffer,
then gain 1/2 fully diff amp for level shift and balancing.  This
gives overall gains of 1-5-10-15. OFC we can populate this with
whatever resistor values. Running the input with 20V PP then
attenuating to 10V PP in the balancer reduces the noise contribution
from the instrumentation amp difference stage and the
antialias. (Currently we are multiplying the antialias noise by 2
rather than dividing by 2.)

If we are running with those big swings then we can't use too low
feedback resistors or the output current will be excessive. Eg. 500
ohms gives 20 mA at 10V. The swing in the first stage is only +/-5 on
each differential output. Currently I have 500 Ohms for first stage
and 1K for second, giving a more conservative 10 mA. Impedance at the
inverting input is 1/2 this or less, tho. For first stage, I'd think
it is 1/2 the gain set resistor, which is perhaps 50 Ohms.  Maybe run
at 15 or 20 mA in the first stage, then 10 later? Our sensor inductive
reactance is in the low 100's of Ohms (and non-lossy, so noise is from
amp current noise).  RFID sensor may have resistance in this range
also, IDK.

Possibly we want a higher noise resistance amp for the second stage
because z=500 Ohms. LT1028 datasheet suggests all the way up to 4K,
tho. Input impedance is matched +/-, which rejects correlated current
noise. Adding matched input resistor does not really increase
noise. But we can likely reduce cost and space by switching to a dual
amp after the first stage.

We don't care about DC, so bias cancellation is only increasing
current noise. Look at audio opamps.

I'm thinking we can bring out the B port non-decimated output to the
card edge, but only have this connect to the FPGA on maybe one slot,
to save IOs. Maybe we don't need LVDS for everything, like SYNC, which
is not edge sensitive or jitter critical. But I think we should still
buffer all outputs from the FPGA to clean them up.

With digital driver, the supply must be driven from the reference. We
*want* the reference 1/f noise scaled up to the supply voltage (but
minimal extra noise). See old notes about power regulation, level
adjust, etc. Distribute reference to per channel regulators at high
voltage near rail.

I think part of the reason I used a current transformer is for sense
with full bridge drive. Full bridge is probably a good idea to reduce
the supply complexity. We could also use a reference coil on the
source, but this makes source coil more complex/less standard, and
requires more wires and pins. But this allows the output capacitor to
be in the source or cable, which is nice because it can be tuned to
match the source type, and driver will work with multiple source types
with no changes. You would want a shield between the source coil and
sense also, because of big CM signal with full bridge drive, adds to
complexity. Can also get this with current transformer in driver if we
add an extra pin per channel which has one end of the cap so we can
measure the capacitor current too.
 ==> But see [4 Sep 19].  Good point there that having full bridge is
     going to increase output filter complexity; need more inductors
     ($$$).  The overhead for split supply is spread across three
     channels (but post-regulators are still doubled).  IDK


With digital sigma delta, do we get less noise at low frequency?  If
we are integrating the error, then we have a long time to fix it. That
would be nice because the low carrier is at a much lower
level. Possibly some special noise shaping would help focus more
effort on the LF.

Do we need some noise or digital dither? If we repeat strictly then
all the quantization noise is concentrated in the bin centers,
ie. DC. This may prevent averaging resolution increase. It may not be
terrible having some analog broadband noise in the output. But if we
add digitally then we can make sure it is out-of-band. Maybe at say
3-5 kHz?

Thing is, the quantization noise is completely under our control. Can
we just cancel it somehow? Given our known periodic signal. Sigma
delta is just one way to generate a bit sequence. Can we make sure
that each high rate block has the same average in-band noise? Could we
concentrate the noise outside the measurement channels? Generating the
bit sequence is basically an optimization problem, and aiming for flat
noise is a generic assumption.


[7 Nov 19]

I figured out a bit more about what was doing on with the labview 1DOF
calibration.  Turns out there was various partially implemented code
related to a concept for multi-carrier calibration which was making
things more confusing.  Also, I had forgotten that I was doing some
scaling of the optimize state, which was wrong after I changed to
calibration in meters rather than mm.  (Maybe it was wrong even before
that, IDK).  This calibration is not useful for anything more than
giving approximate distance units anyway (though the fit is quite
good for 1DOF motion).

After fixing the current sense processing distance_processing.vi to
work again, I notice that the 1/f noise in the low carrier is
significantly improved (with UR44).  There is still something a little
screwy with the calibration, it doesn't "work" sometimes?  I can't
calibrate with the YZ carriers off using UR44.

 ==> OK, I spent a half day on this, and I think I finally have the
     calibration and distance processing sorted.  I realized that what
     I was doing with that loop in state_to_calibration was solving
     for the low carrer Ks.  I kept this (without the loop and discarded high
     carrier re-solution), ripped out the 2D Ks array in the
     calibration.  Also, when I added a test for the calibration being
     "good", I botched it so that no calibration was "taking".  This
     explained some funny results I was seeing with trying to
     calibrate with X-X rather that vector-vector.  And the low
     carrier path in distance processing was not respecting the
     source/sensor selector, so that was screwy when the mode was not
     vector-vector.

FWIW, today, after the various cleanups to calibration and fix in
distance processing, I am getting rather lower noise on UR44.  With
sense on, X-X mode, I get [325n, 446n] high/low.  Sense off it's more
like [317n, 715n].  So minimal effect of sense at high, significant at
low.  However noise at 240mm UR44 is about the same as yesterday.


[6 Nov 19]

Summary:
 -- RMS noise comparison of demodulated signal with UR44 and LTC2500,
    with and without distortion filter, at three distances.


I re-tweaked the driver gains and standard setup.  What I had chosen
before, -4 dB @ 130mm, required a rather too high drive level, 1.41 V
low carrier at the coil, so I had backed off the low carrier to 0.03,
but then it was a lot noiser than high.  I changed to 120mm, with,
1.15V, which still gives -4 dB.  This is all pretty arbitrary and
specific to the evaluation setup, so the details don't matter all that
much, but I wanted to see what the low carrier noise was going to be
with the higher level, without overdriving the source coil.

This is giving total coil currents:
    3.275	2.793	2.930	

As before, the pattern is rather odd, not as much current drop at
higher high carriers as one might expect from inductance.

Anyway, with the box ADC/DAC TCP connect, TCP only, 120 mm, display
options X, X, no-sense, distance, 0.2 Hz

 ==> Oops, distance calibration doesn't work in non-vector mode???
     Even if you recalibrate?

But in TCP-only, X and vector are the same, because the other channels
are zero.  Yet if you calibrate in vector vector mode, it works in
either?  But not if you calibrate in X X mode?  IDK.  But as long as
we can get it calibrated, this 1DOF calibration is just a test hack
anyway.  May be a problem with UR44 tho, since the other channels are
not zero.


condition			noise hi	noise low
120 mm, IMD filter on		278n		455n
120 mm, IMD filter off		3.88u		504n	
200 mm, filter on		554n		916n	
240 mm, filter on		818n		1.1u	

UR44
condition			noise hi	noise low
120 mm, IMD filter on		521n 		860n	
120 mm, IMD filter off		2.88u		752n	
200 mm, filter on		1.71u		3.61u	
240 mm, filter on		3.29u		5.77u	


 ==> I hadn't compared noise at longer ranges, but it makes sense that
     1/f noise does not increase with range, while broadband noise (in
     distance terms) does.  This means that the 1/f corner gets pushed
     down, and broadband noise increasingly dominates.  This explains
     the quite slow noise increase using the test box, since it is 1/f
     dominated at short range.  


[1 Nov 19]

So, how can we summarize the noise performance?  The meters
RMS-above-corner value seems like a good approach.  I also have a
bunch of numbers from the spectral noise floor, but this is redundant
with the RMS, and does not include the 1/f.  We could also do
different stuff like peak-to-peak, but this seems like pretty much
more of the same.

So, I guess go back and get average RMS noise in the several
conditions?  That is, I've been trying to get at noise from
converters, from driver, and ambient EM, by adding in parts and (up to
now) looking at the effect on the noise floor.  We can't directly get
noise in meters in the carrier-off conditions, though.  But we could
infer this from signal amplitudes. 

See [26 Sep 19] for last take on noise floor measurements.

Signal chain:
 dac -->
 driver -->
 air/EMI -->
 preamp -->
 antialias/balancer -->
 adc

But also noise contribution may vary with signal amplitude, eg. the
noise floor rise.  With UR44 the input side is a black box; we can't
separate the gain stages from the ADC other than by varying the gain
knob. 

Test conditions:
 loopback, input shorted
 loopback, no carrier
 loopback, carrier on
 over air, driver off
 over air, driver on, no carrier
 over air, all up
 

In loopback mode we get rid of the driver and EMI contributions, which
can be significant.  It would be interesting to be able to say
something about these.  I am particularly curious about how total
noise (including 1/f) compares in loopback vs. all-up, and UR44
vs. prototype.

On [26 Sep 19] I was skipping the loopback cases except for
shorted.  But that was pretty OK because the EMI was below the noise
floor.  But I notice that I wasn't really thinking about DAC noise
separate from the driver.  eg. it could be that the difference of
"driver on, carriers off" vs. "all on" is the DAC noise, rather than
noise introduced by the driver when carrier is present.


 ==> One thought is that a progress report is not the same as
     presenting the results.  The idea is where we are in process,
     have there been any major roadblocks, etc.


[31 Oct 19]

Implemented RMS output display with highpass and averaging so that we
can better quantify the 1/f contribution.  For handheld use there is
genuinely some frequency below which 1/f does not have much effect
because it gets lost in the human factors.  But also typical
electronic practice is to set a LF (highpass) corner for looking at LF
noise.  For opamps this is a peak-to-peak spec from 0.1 to 10 Hz, over
unspecified time interval (like 10s).

This works OK, but I think 1/f noise is fundamentally rather resistant
to being pinned down.  I've been using a LF corner of 0.2 Hz, and an
averaging Tc of up to 60s.  I am using 0.2 Hz rather than 0.1 because
this is giving more stable results, and is arguably still a reasonable
value for our purposes.

Set up at 130mm, TCP only, vector, vector, distance.  With these
settings, I've been getting 0.24 to 0.28 um RMS on the high carrier.
The low carrier is more all over the place, 0.6 to 0.8, with glitches
that take a long time to settle out.
So, let's say:
	low: 0.7 +/- 0.1  um
	hi: 0.26 +/- 0.02 um
	

With the 11s chart length, the peak excursion on high carrier is
around 1.5 um, which might be up to 2x that peak-to-peak.  But it
typically looks more like 2 um p-p.  This is without the highpass
filter, but removing the mean in the 11s window, so sort of implying
something like a 0.1 Hz corner.

If noise were Gaussian, we might expect something like 6x RMS, or 1.6
um p-p, which is fair agreement, especially allowing for the different
highpass mechanism.

Eyeballing the plot, I want to say that we are 1/f noise dominated.
But if I increase the corner to 20 Hz, then I am getting 0.11 um RMS
on the high carrier, which is a reduction of 2.4x.  Since 20 Hz is
about where the 1/f kicks in, I would say that is a majority, but
perhaps not "dominant".  Increasing to 50 Hz only reduces to about 0.1
um.



[30 Oct 19]

One question is how to handle D/A in the first incarnations of the
reference design.  I am unhappy with the PCM1794A 1/f noise and
absence of an external reference, so I don't want to actually design a
card for it, but we could continue to use the eval boards that I have
until we move to direct digital output.

Another possibility would be to use a high speed 16 bit DAC like the
LTC1668.  This has AC specs like SFDR, with output rates of up 50
MS/s, and we could implement oversampling to get more bits.
Unfortunately it has a parallel interface, which would be a bit of a
nuisance.  At 1000x oversampling, we get sqrt(1000) more bits?  Or 31
+ 16?  That sounds kind of implausible, but 1000x oversampling would
likely be overkill (and would somewhat computationally demanding even
with the FPGA).

It probably makes more sense to stick with the PCM1794A or some other
off-the-shelf board until I can develop a custom digital input driver.
A digital driver makes sense for the reference design for cost and
simplicity even if we do not at first reach ideal performance.  We
want a custom driver anyway because the audio amps are not optimized
for low-loss inductive loads.  But this means we have to live with the
reference 1/f noise until then.  I guess this is not a big deal if I
can finish both the input section and driver in the next year.



[24 Oct 19]

When we optimize the quadrupole parameters with the dipole parameters
held at nominal default values, then when we unfreeze the dipole we
find that the optimizer does not want to move the dipole moments from
the default, eg. diagonal ones for source or 0.2 for sensor.  The
residue is very low, something like 7e-5.

It seems either there is a bug or we do not have enough data to
identify all of the parameters of the dipole+quadrupole model.  WRT
not-enough-data, the usual suspect is the lack of RxRy rotation other
than that incidental in the translation.

The +X-Y+Z edge of the workspace seems to have worst error, especially
the Y component.  The asymmetry is a bit puzzling.  This could be a
physical imperfection in the hardware or setup, but might also be a
consequence of the asymmetric test pattern.

 ==> Some points have bad residue during the pose solution.  Are these
     the same points with high error?

Todo:
 -- Try to reproduce dipole-only calibration with current code.
     - Then freeze dipole parameters while optimizing only quadrupole
     - Then optimize all
 -- Report XYZ and RxRyRz vector error as max of magnitudes and RMS
    across points.
 -- Error vector exaggeration in plot.
 -- Quantify the effect of calibrating on one dataset and testing on
    another.
 -- Explore effect of reduced workspace on quantitative error.
     - Calibrate and test on a reduced workspace.
     - Figure out what parts of the workspace we should drop to reduce
       the error. Points with high residue during calibration or pose
       solution. Points with high pose error.
 -- Collect some new data.  What happens if we refixture source or
    sensor?

If we solve for fixture transform as in ASAP, eg. using
pseudo-inverse, then we can test accuracy of arbitrary patterns with
unknown fixture transform.  So simply seeing how much accuracy
degrades would be straightforward.

It would be more involved using data from multiple fixturings in the
calibration optimization.  One possiblity would be to optimize each
fixturing separately, then combine the results, such as by freezing at
the individually solved fixture poses.  Or you would get similar
results from just using the pose solver and the pseudo-inverse fixture
transform to register the calibration sets.  We would hope that the
identified source/sensor parameters would not vary much as we
refixture.  If we do see a lot of variation, then that is interesting
in itself.


[21 Oct 19]

One way I could make the input card design process more complicated is
to consider what would happen using other ADCs.  This ADC is kind of
weird, and also expensive.  I don't plan to *actually* use a different
ADC, but it would be nice if the main board could support it.  I am
not designing the main board right now, but I am designing the input
card interface which will define much of the main board.

I guess the bottom line is that we have some inputs, some outputs, and
a master clock.  Really, the LTC2500 has more IOs than a typical audio
ADC would.  You have your bit clock, your word clock, your system
clock, and the output bit.  3 in, 1 out.  The 2500 has 2 out, 5 in.
Clock generation might be different.  You might find you want a
different clock frequency to get the right sample rate.  Have not yet
decided (and do not need to decide) what the clock rate should be.  A
programmable clock (rather than straight crystal) would maximize
flexibility, but jitter is then a potential problem.

See [8 Aug 19] for discussion of IO counts and packaging.

Part of what I can do in the input board evaluation is see whether
jitter is a problem with various clock sources.  Right now, just using
the FPGA, we can't really tell.  My hunch is that with shared clock we
will not get much effect in the 1/f region currently dominating noise,
and (I think) we are already meeting the specs of the LTC2500 for
noise (at least for broadband), despite the crappy clock.


[17 Oct 19]

I started working on the input channel design in KICAD.  I looked a
bit at the two-opamp instrumentation amp design, but turns out that it
has HF CMR issues because of phase mismatch in the two input paths.
There is not as much CMR benefit from the three amp configuration for
us because we are running at a low gain, maybe 5-10x, but it does
still give good HF CMR as long as you have good resistor matching, and
the matched resistor arrays seem to be an effective solution there.

See [25 Aug 19] for preamp noise testing and gain discussion and
[24 Jun 19] for sensor design.

 ==> Worth some investigation, simulation, analysis, etc., of the preamp
     noise, because I seem to be getting something like 2.5 nV/rtHz RTI
     with 50 Ohm dummy Zin.  We can definitely get into the thermal
     noise limited regime at closer ranges.
This is more than I had predicted, way back when.  But I think I may
not have accounted for the resistor noise.  Then with the sensor there
is also the current noise across the sensor impedance.  I don't


Still unsure about gain switching.  Adding a couple attenuation steps
right before the driver would be easy and would get us some
flexibility.  With +/- 15V supplies we can get >20 VPP output, whereas
the DAC only takes 10 VPP.  My current setup is:
    20 gain in preamp
    1/4 attenuation
    antialias unity gain
    2 gain in driver

This is giving us 10x overall, -4 dB FS @ 130mm. We had been running
at 70x in the 200mm setup.


[24 Jun 19] also has some discussion of higher order magnetic model
DOF, but I am having some trouble following the reasoning.  I notice
that there is no reason why the quadrupole parameters need to be
defined with respect to the dipole.

Let me recap the state of my understanding wrt. the calibration
model.  3DOF position + 3DOF moment is the physics.  It does not have
extra DOF.  But there are other problems that we run into:
 -- We add extra DOF for the source and sensor fixture transforms.
    Some of these are redundant with the magnetic parameters.  In
    particular: redundant with the sensor fixture Rz component is the
    Y component of the X sensor moment (which allows the moment to
    rotate in the sensor XY plane).  [For the source also, ???]
 -- Since we do not have any precise mechanical calibration of the
    source and sensor, or of the fixure transforms, we allow the Z
    coil pose to define the source/sensor coordinates (that is, we
    force the position to 0 and the XY components of the moment to 0).
    This can be seen as a way to avoid redundancy between the fixture
    transforms and the coil positions.
 -- Since we do not have any direct measurement of the source or
    sensor moment magnitude, we need to fix one or the other.  IIRC,
    we are currently fixing the source Z moment to [0 0 1].

Hmmn, interesting exercise to count the DOF we are freezing and see
if it matches the extra or ambiguous DOF.
    +6 source fixture transform
    +6 sensor fixture transform
    +1 source/sensor moment scale ambiguity
    -6 source Z position [0 0 0] source Z moment [0 0 1]
    -5 sensor Z position [0 0 0] sensor Z moment [0 0 k]
    -1 sensor X moment [k1 0 k2]

That leaves us with an extra DOF, unless the source X moment Y is also
forced. One way to see the fixture transform issue is that the
relative pose of the source and sensor coils is observable, but the
absolute pose wrt. the fixture is not.  We want to absorb the fixture
pose so that we can have source and sensor coordinates which are
defined relative to the source and sensor, and not including the
fixture effects.
 ==> Just looked at some new calibration results, and it seems we are
     forcing the sensor fixture position Z to zero, so we are good
     from a DOF perspective.  But maybe this is another ambiguity?  We
     were getting runaway between the sensor Z and the sensor fixture
     Z.  Is there an issue that the relative source/sensor fixture is
     defined, but the absolute values of each are not?  I recall I had
     some argument that if we had the missing rotations then we would
     be able to identify the sensor fixture Z.  IDK.

With these constraints we are getting convergence to sensible models
that work to some degree.  The question is, when we add the
quadrupoles to the model, are there any constraints of this sort which
are needed to avoid redundant DOF?  My intuition is no.
 1] The quadrupoles (like the dipoles) do not in themselves have extra
    DOF.
 2] We have not introduced any new fixture transform DOF.
 3] There is not a new scale ambiguity because the scale has already
    been set by the dipoles.  We need to allow the quadrupole to vary
    to any strength wrt. that axis's dipole.
 4] The quadrupoles are not redundant wrt. the dipoles because they
    have different effect over distance, and if they have different
    position or orientation this results in distance dependent
    asymmetry.  I expect that the dipole solution will be very similar
    to what we get without the quadrupoles, because this is forced by
    the behavior at distance.

Now, there will be problems with optimizing everything at once
without a good initial solution.  And, as I keep mumbling, it might
help to identify the parameters if we had more angular motion.

What should we drop out of the optimization initially?
 -- The sensor quadrupole moments
 -- We are most unsure of the quadrupole moments.  We could try
    freezing the source quadrupole position at the dipole position to
    get an initial solution.  This is analogous to what we did
    initially for the dipole to find the source and sensor dipole
    moments.
 -- We could try bounding the motion of the dipole and/or quadrupole
    position to somewhere near the dipole solution.  Likewise we can
    bound the change in the fixture poses.  These will prevent the
    optimizer from going too far in the wrong direction, but it will
    ultimately need to recover to a more central position.


 ==> FWIW, the quadrupole moment has 180 degree symmetry, flipping it
     end-for-end


Looking at some quadrupole calibrations from Claudia, and the results
look rather sensible, though the optimization still tends to run away,
especially with the source Y moment.  The quadrupole and dipole
moments are getting large, and the dipole sign is wrong.

One thing that does make sense is that the quadrupole and dipole
locations are pretty similar, except in some cases where the
quadrupole is getting pretty small.  The sensor quadrupole moments are
also relatively small compared to the dipole.  Interestingly, the
source Z quadrupole is pretty small also.  The source XY are tangled
up to various degrees with the Y axis problem, interactions with the
source fixture, etc.

I do kind of wonder if there is a bug in the forward kinematics, since
the Y dipole and quadrupole moments should not be able to cancel each
other, since the quadrupole basically cancels itself.


[11 Oct 19]

Of course what's happened now that the IMD is under control, and the
broadband noise is reduced by working closer with lower gain, is that
the 1/f noise is now quite prominent.  [I guess the higher SNR is at
least a partial explanation for how the IMD became so prominent.  Not
because I increased the IMD, but because I reduced the broadband
noise.]

I am not sure about how much effort to put in now on investigating the
1/f noise.  It seems the main defense should be getting the input and
output sides on the same reference.  I had been thinking of this
mainly as reducing drift, but the same applies to 1/f noise.  I don't
think there is any other "fix", but it would be nice to quantify the
1/f better and to compare what I am seeing over the air with
loopback.  While 1/f is prominent in loopback, the broadband noise is
a good bit lower there.

The plan to put in a more heavily filtered ADC reference does not seem
related to the 1/f, which is not going to respond to filtering.  The
reference improvement might help the low carrier SNR.  Possibly clock
jitter could be contributing to the 1/f.  We would expect to see
little reduction of the 1/f in loopback if it is being contributed by
jitter (or by mismatch between ADC and DAC references).
 ==> We do already have a common clock, which will tend to reduce
     jitter effects, especially at low frequency.

Oh, I had been thinking of whether we could force a voltage reference
into the DAC at its reference bypass cap, but an alternative idea
would be to derive the ADC reference from the DAC.  This would be just
a hack to test the reference mismatch theory.

It seems that audio converters are in general not set up for external
references (and the DAC doesn't even have a voltage output).  The
ultimate solution would be get rid of the DAC and implement a digital
input driver.  This could presumably reduce noise in the driver itself
(1/f or broadband).

Aside from keeping the calibration effort going, I am now thinking it
is time to design and prototype the ADC board.  It would be good to
have this well under way by the time of the progress report.  I am now
thinking gain switching is less useful, which will help to simplify
things.


Thinking a bit about the driver feedback.  Aside from the isolation
and compensation for coil resistance change, another advantage of the
current sense is that the low carrier current is about the same as the
high carrier, greatly improving the SNR of the low carrier
measurement.

I haven't proven that the current sense is adding a lot of IMD, but it
seems likely.  I was not worrying about that at all when I designed
the CT setup.  I am driving out a big voltage, probably 1 V RMS.  Using a
transimpedance amp for the input, or just using a higher turns ratio
(lower output voltage) could help quite a bit.  And selection of the
current transformer for this purpose.  Higher current rating, higher
inductance (and higher turns ratio).  We don't want to reduce the
output so much that we get into thermal noise limits in the input,
though.  But I think the gain from higher turns is quadratic or even
cubic, because the inductance goes up (as turns squared) and the
voltage goes down (linear with turns).

To be super-optimized, perhaps for a research paper investigation, we
could look at custom components with a core type (metal?) optimized
for linearity in our frequency range.  But for the reference design it
would be nice to use off-the-shelf components.

Going off of the transformer frequency rating we do not have a huge
number of choices at digikey.
 	582-1161-ND  CR Magnetics CR8348-1000

This part says that it has a nanocrystalline amorphous core.  I guess
metal, maybe tape wound?  The frequency range is given as 50 Hz - 50
kHz.  It is a littly pricy at $10, but we would only need 3.  This
part is stocked, and is not mechanically that big, 20mm on a side.
The current rating 20A.  They also have larger (higher current) units
which are not stocked by DigiKey.

We are talking at least 1000:1 in these units.  This may have mostly
to do with the market (do you want more than 20 mA @ 20A?), but
probably also has to do with the very turns ratio effect I was talking
about, of pushing down the low frequency corner.


[9 Oct 19]

I got the hum and distortion filters working.  It seems that
split_averaging_filter_nu was broken, so I retained that breakage when
I made the new split_averaging_filter_cdb.  See [14 Jun 17], it seems
when I switched to time domain averaging that I broke things by
decaying the noise model like an ordinary first order LPF, rather than
doing the integrator action.

I also added detrend feature and made the FFT filter conditional
(turned off).  The detrend (or at least mean removal) allows us to
function in the presence of a DC component.  I also reduced the order
of the distortion filter to the actual repeat rate rather than using
the low rate FFT size.  And I reduced the distortion filter Tc from 3s
to 1s.  These changes let it adapt a lot faster, which helps with
initialization (and presumably motion).

The hum filter is not currently doing a whole lot in my basement
setup.  The hum seems to be about -120 dB FS.  In comparison the
distortion is -86 dB RMS (the peak spur is -91 dB in the model).

I really don't think the IMD was this high before.  It's quite
prominent now even with the UR44.  Had I been accidentally
setting a very low pass output?  IDK.  I made a few tweaks to the
driver, but nothing that should have much effect, certainly not on all
channels.  I did increase the drive a bit on the high carriers, though
I backed off the low carriers in partial compensation.  I did not
examine the levels issue too closely, other than seeing that I wasn't
into overt clipping.

I am now seeing about 0.5 um RMS on the "chart high" with distance
enabled on Y channel with UR44, using the FFT demodulator with hum and
distortion filters.  

A bunch of spurs at what look like the distortion filter order appear
when the I have the detrend turned on, and go away if I change to
"Zero mean".  Didn't notice those before, does this have to do with
reducing the distortion filter order?  Or was that always there?  Not
entirely surprised that detrend causes problems.  Hmmn, looks like
there is still a trend in the filter?
 ==> Yes, that does seem to go away when I change back to the higher
     order distortion filter.


[4 Oct 19]

I have modified ilemt_ui.vi to permit turning off the KF demodulator and
instead using plain FFT demodulation followed by a KF, and then hum and
distortion filters (operating at the Fs high rate, rather than the input rate,
as previously).  I expect that the hum and distortion filters will work in
this position, but the code is not debugged yet.

Possibly in this setup the sigma (variance) outputs from the filters might be
useful, especially for the hum filter.  The idea is feeding the variance into
the KF input noise estimate.  So we will adapt a bit slower at times where we
have greater hum uncertainty.  This will cause any actual motion to
intermodulate with the hum, but unlike the situation at the input rate, the
demodulated signal is nominally static, so we do not get intermodulation with
the full amplitude.

Especially with the sigma from the filter it would likely make sense to put
the hum and distortion filters in front of the KF, rather than after (as I
currently have it).  I guess it doesn't make much difference because it is all
linear, but the dataflow would be clearer.
 ==> Also this allows the filters to work on a signal without DC component.
     The split_averaging_filter did not have to deal with DC before, and I
     have tried to work around this by using the LF cutoff, but our model
     still ends up with a DC component.
     

I had the idea that we could use the guard channels for the ANC reference
input, but I'm pretty sure that will not work.  The adaptive filter is linear,
and will not introduce new frequency components.  The guard channel works for
broadband interference because that is splattered across all channels, but
what frequency a narrowband interferer appears as after demodulation is
completely arbitrary and channel specific.


[26 Sep 19]

Summary:
    I reduced the input gains by 12 dB (using attenuator between preamp and
    box, and UR44 knobs).  The setup is now -4 dB at 129 mm.  I rejiggered the
    driver levels so that the gain from code to voltage is approximately equal
    across channels.  The currents are unequal across channels (expected), but
    the pattern is peculiar, dropoff at higher frequences less than expected.
    I reduced the low carrier from 0.05 to 0.03 to reduce source heating.
    
    These changes reduce the EMI contribution to signal, bringing up the SNR
    at the sensor.  The hum EMI is now below the UR44 noise floor (Driver
    noise was already below UR44 floor).  X floor is largely driver noise
    limited.  At 129 mm there is only about 4 dB advantage of X vs YZ, but
    this increases at longer distance up to about 10 dB.

    IMD is now dominating the demodulated signal.  This is pretty clearly
    driver or source related because the SFDR re. carrier does not change as
    the range varies, and is not present in loopback.  SFDR is 10-20 dB worse
    in the current sense than over the air.  This may be IMD in the current
    transformer (unless fixed this would likely make current sense ineffective
    for pre-distortion).

I set levels at 129mm as described above (1.43V at coil, input gains equal.
Theory is that this should give roughly equal sensor outputs (modulo magnetic
configuration and sensor assymmetry), with higher current at the lower
carriers.

I checked that the current sense inputs are equal by swapping sense cables at
the driver box.  With both carriers I am getting currents:
    3.981	3.378	3.511	

Only high carriers:
    2.626	2.060	1.935	

Only low carriers:
    2.997	2.684	2.941

So the high current does drop across carriers, though not in line with a pure
inductive model (with equal source inductance). The source coil
inductance/resistance is not matched, but the Y and Z coils are progressively
bigger, which should exaggerate the drop at higher frequency rather than
diminish it.  In the low carriers there is no current trend. We would expect
less trend because of lower relative frequency difference, but this doesn't
explain Y being low.  Possibly I just ended up with Y set low?

Also, FWIW, with only high or only low, there is no audible whine from source.


As to outputs in this setup, the XX coupling is +4 dB WRT YY and ZZ, rather
than the theoretical magnetic +6.  IIRC the situation was similar before at
200mm.  Maybe this is because the X moment is lower due to smaller coil?

129mm standard orientation:
High coupling:
    -4.115	-47.575	-41.359	
    -37.582	-8.503	-47.343	
    -34.610	-43.112	-8.603

Low coupling:
    -32.641	-70.778	-71.104	
    -68.437	-36.392	-75.783	
    -60.424	-74.364	-36.449	


129mm, with source X aligned with sensor Z, we get:

High coupling:
    -40.435	-62.269	-4.948	
    -34.110	-8.190	-38.256	
    -6.791	-34.070	-38.583	

Low coupling:
    -71.393	-79.131	-32.644	
    -62.746	-36.080	-68.724	
    -35.822	-62.976	-62.159	

The 4 dB pattern still holds fairly neatly at low carrier, but the high rate
is a bit messier.  It's very roughly what you would expect, but not at all
precise. (Within say +/- 1 dB.)


Source is running pretty hot, so I'm backing off the low carrier amplitudes to
0.03. (After the above measurements.)

Noise floor with these new settings in TCP only mode (carrier on, 129mm):
    -139.7	-Inf	-Inf	
    -135.5	-Inf	-Inf	

Including UR44:
    -138.2	-135.3	-135.5	
    -133.4	-129.7	-132.9	

Carriers off:
    -139.0	-136.0	-136.8	
    -135.9	-135.6	-134.8	

Driver off:
    -141.2	-136.8	-137.3	
    -143.7	-135.4	-136.7	

Hmmn, I was expecting a stronger driver noise contribution.

Loopback cable shorted:
    -141.9	-137.1	-137.8	
    -143.3	-136.8	-136.3	

This is no different than with sensor, so EMI (and any sensor impedance
current noise effect) is now below the input noise floor.

With box input shorted:
    -151.6	-137.1	-137.8	
    -153.7	-136.9	-137.4	

So preamp noise is still 10 dB above the driver/ADC?  Hmmn, but with preamp
inputs shorted at the preamp (and routed into box), I get:
    -151.1	-136.9	-137.8	
    -154.5	-135.8	-137.1	

So, basically no difference.  The preamp noise is now below the driver/ADC!
 ==> Eh, I was getting crud coupled into the setup from laptop power cord.

Now with loopback cable shorted I am getting:
    -151.2	-137.0	-137.8	
    -154.8	-137.0	-136.7	
(Same as shorted at box input)

Now, over air, driver off:
    -149.7	-136.7	-137.4	
    -154.3	-137.8	-135.8	

Driver on, carriers off:
    -143.2	-136.2	-136.9	
    -138.0	-135.3	-134.5	

All on:
    -140.0	-135.3	-135.5	
    -135.6	-131.8	-132.8

EMI/sensor noise: 1.5 X, nil YZ
Driver on: 6.5 dB X, 0.5 dB YZ
Carriers on: 3.2 dB X, 1.2 dB YZ
Total swing, shorted vs. all on: 11.2 X, 2 YZ.

X performance is largely driver limited.  YZ are highly input/ADC noise
limited throughout.  As it stands, X is at least better (which it wasn't before
the antialias filter), but the difference of 4.5 dB is meh.  If driver noise
could be improved then the fancy ADC would be more valuable.
 ==> Haven't ruled out possibility that out-of-band driver noise is getting
     into the X ADC.

On the plus side, with lower noise ADC the SNR does not degrade nearly as fast
with increasing distance (at constant gain).  Looking at high carrier dB
re. noise, at 129mm:
    124.702	58.893	62.851	
    69.894	115.903	76.759	
    71.845	80.726	115.705	

And at 180mm:
    122.323	42.884	51.922	
    57.509	108.288	69.985	
    63.914	73.637	108.383	

So X drops 2.5 dB, while YZ around 8 dB.


However, in any case, what we actually care about is the demodulated output,
which is dominated by IMD, and then by hum.  For whatever reason, XX IMD seems
especially bad.  In the output I am getting RMS:
    XX		YY	ZZ
    90.7u	26.4u	34u

Carriers off:
    XX		YY	ZZ
    1.6u	3.3u	2.9u

High carriers only:
    30u		7u	10u

High carriers only, TCP only:
    6.5u

High and low, TCP only:
    82u


At 200mm, all on:
    XX		YY	ZZ
    11u		7u	11u

Hmmn, is the IMD happening in the sensor?

In the current sense, I see the 2*low spurs at dBc:
    -65		-70	-73

In raw data at 200 mm:
    -83		-78	-83

Raw data at 129 mm:
    -81		-79	-84

The discrepancy between SFDR in current vs. over the air is rather dramatic
(8-18 dB).  This mismatch suggests that pre-distortion according to the
current sense will not work well.  Distortion might be due to intermodulation
in the current transformer.  This could perhaps be helped by running current
sense into a transimpedance amp virtual short, and/or by a (perhaps custom)
current transformer with more turns (higher inductance and higher ratio).  And
a bigger and/or higher mu core, or one with a more stable mu vs flux.

It also seems possible to do predistortion using over-the-air signal, but it
is not entirely clear how this is better than simply using a distortion filter
on the receive side.  The main advantage is that if we can concentrate on
cancelling true source distortion then the compensation will work even during
gross movement, whereas the distortion filter has to adapt to the new spur
amplitudes.  We may reasonably hope that eg. hum will be relatively constant
over translation because the source is in the far field, while the source
spurs will vary proportional to the carriers.

The existing distortion filter works at the low rate.  I suppose it could be
generalized to work in the KF demodulator by adding the spurs to the carrier
model?  Hmmn, that could be a way to make hum and distortion filters actually
work again.


So, no, the SFDR does not vary with sensor distance (signal strength).  This
strongly supports it being predominantly a driver/source problem.  However
these readings also do not entirely explain the variation in demodulated signal
spurious amplitude, thought the trend is right.  This is at least partly due
to more than one spur contributing.  Spurs in demodulated signals:
    XX:    -108 @ 269 Hz	-113 @ 539 Hz
    YY:    -120 @ 656 Hz
    ZZ:    -117 @ 363 Hz	-123 @ 762 Hz

I guess why the demodulated spur ratios are larger than the SFDR is that both
sidebands contribute to the demodulated output.  Perhaps a 6 dB difference?
Then on XY comparison we have -5 dB SFDR, whereas 10 dB from output RMS ratio,
while 5+6 = 11.  So that would check out.

Oh, also, with the lower input gains I do not anymore see big hum harmonic
peaks in the YZ demodulator output (with driver off).  There is a broad noise
hump near 275 Hz for some reason.  On X the hum peaks can still be seen above
the lower noise floor.  Driver off demodulator out high, RMS:
    0.84u	4.2u	3.8u


[25 Sep 19]

I got the ADC/DAC hookup working over the air.  The X drive is on the
low side because of DAC output limit.  I set distance at 165mm to get -4 dB.

Carrier on:
    -133.9	-Inf	-Inf	
    -132.1	-Inf	-Inf	

Carrier off (DAC off):
    -134.7	-Inf	-Inf	
    -133.5	-Inf	-Inf	

Driver off:
    -139.0	-Inf	-Inf	
    -148.9	-Inf	-Inf

Box input shorted (antialias, driver w/ 2x gain):
    -151.6	-Inf	-Inf	
    -154.8	-Inf	-Inf	

ADC input shorted:
    -157.9	-Inf	-Inf	
    -159.3	-Inf	-Inf	

(We are currently losing 6 dB in the antialias.)


From [27 Aug 19], this is very much the same as the UR44 (where the
noise floor happens to be close to EMI).  Compare to about -150 dB in
loopback mode.  There is very little effect of carrier presence on
noise floor.  At this range we seem to be seeing some driver noise
(based on decrease with driver off).  Driver noise is larger at low
carrier, which is what we would expect for a voltage noise in the
driver output.  The EMI floor is rather higher at high carrier than at
low.

With driver and DAC off, the X-X demodulated signal (chart high, ~10s)
is 5.6u RMS, almost entirely hum harmonics.  eg. the noise floor is
about -255 dB using default window, with hum peaks at -225 to -240 dB.
In this mode the 1/f noise is not prominent (no clear corner, < 1 Hz).

With carrier on the 1/f noise is pronounced, at about 10-20 Hz.  The
noise floor (in demod signal) has not really moved, but amplitude has
increased to 22u RMS.  This is dominated by apparent IMD peaks at low
carrier and low carrer *2.  The IMD peaks are around -214 dB, ie. ~20
dB above hum.  The IMD peaks have around 40 Hz width at the base
(ie. are only moderately repetitive).

I do not recall the output as being so dominated by IMD (or by hum
harmonics) in the previous results with UR44, but I don't think I
spent much time looking at the demodulated spectra, especially at HF.
I did not have good tools for doing this. The IMD is present in
loopback only at very low levels.  I have no explanation how the
driver/source IMD would have increased.

Looking at "Raw data" spectrum, the 7.5 kHz +/- 2*269.531 spurs are at
-86 dB, or -82 dBC.  The +/- low fundamental spurs are at -103 dB,
which is a rather bigger the difference (~20 dB) that what is in the
demodulated signal -208 vs. -220.

In thinking about it, I realized that LF reference noise will show as
LF noise in the carrier amplitude, so also in the demodulated signal.
The ADC reference noise may be a major source of the 1/f noise in the
output.  (And also the DAC reference noise.)  Not sure about the
spectrum of sample clock jitter, but seems likely it is also close in
to the carrier because the noise is modulated by the carrier, which is
entirely equivalent to the carrier being modulated by noise (as with
reference noise).  So we should not expect the carrier presence to
increase the broadband noise floor (and this is indeed what I see).

Given that we need a stronger signal (closer to sensor, less gain) in
order to use full ADC dynamic range, it seems there may not be much
benefit to ADC gain switching.  It seems that EMI is dominating amp
noise, so gain is only going to increase noise from EMI.  Or if we
were amp dominated, will only increase amp noise.  It depends on where
we set the noise (from amp and EMI) at the highest SNR (closest)
position, but unless the non-ADC noise sources are forced way below
the ADC noise (eg. by very close position) we will not get much SNR
improvement by increasing gain as the signal drops.

This is actually good because gain switching adds complexity, and can
only harm the signal integrity.

It would make more sense to decrease drive at short ranges to avoid
input overdrive.  Degredation due to eg. fixed driver noise will be at
least partly offset by the 'r' geometric accuracy improvement at short
ranges.  But we would have to drop the signal as 1/r^3 which is still
going to use up the driver dynamic range quite quickly unless there is
something fancy like variable supplies.

So, how much would it make sense to reduce the gain?  If EMI is about
-140, then we could decrease by 10-20 dB to bring EMI down at or below
the input electronic noise floor.  Likely we are going to hit the
driver noise floor not too much below the current EMI level, though.
(We are not using full driver dynamic range due to under-drive).

I add 12 dB attenuation to the amp output, move to 105 mm (!) to get
-4 dB and I get:
    -137.7	-Inf	-Inf	
    -132.4	-Inf	-Inf	

So we gain 4 dB at high carrier, nil at low.

I crank the driver gain trim up to max, and move out to 152mm to get
-4 dB (a more reasonable distance).  The current is 4.2A.  I have the
other drivers off.  Increasing the LV carrier level to 0.9 results in
bad distortion (clipping presumably), so don't do that.  In this setup
I get:
    -142.4	-132.0	-132.2	
    -139.2	-131.8	-131.7	

This is around 10 dB better than what I was getting in the old UR44
setup.  But if I reduce UR44 gains to match, it will improve also.


From phone:

Thoughts on how to implement driver pre-distortion from current sense:

Average current.
[May want to detect phase jumps and reinitialize]

Interpolate VI complex frequency response from carriers and
current. Take conjugate to get IV response.

Zero carriers in current FFT, negate, multiply by IV response. Roll
off compensation outside of low/high carrier bands. (?) Theory is we
don't care about outside those bands, and trying to cancel may have
cost.

Add a fraction of the transformed current error to an outgoing carrier
response. IFFT to get DAC output. (Might want smooth block
transitions, eg. by interpolation.)



[24 Sep 19]

Thinking about adding quadrupole model to Matlab model.  We don't yet
have good handle on where things are breaking down, but we are getting
some pretty big errors >10mm with the dipole model.

We shouldn't forget other possibilites:
 1] There is an error somewhere in the code, or
 2] that maybe our calibration data is not diverse enough to fully
    identify.

I'm kind of skeptical of [2] because the test data we are using is
equally kinematically impovrished.  Where I would expect to see [2] is
when we look at configurations that were not in the calibration data.

[1] is of course possible, but I don't greatly favor that because the
calibration solutions we are getting seem physically plausible (in
terms of coil locations, orientations, etc).


[23 Sep 19]

I hacked up a 14.5 kHz 4'th order 1 dB Chebyshev filter using part of an old
ASAP board.  The filter noise is higher than I was hoping, perhaps 6 dB above
the ADC noise floor, but it should definitely squash any moderate-frequency
noise that is coming through the signal path. In particular, with it in we get
OK performance with the DAC in external single-ended loopback.


[19 Sep 19]

There was a *lot* of HF crap on the DAC output, something like 40 mV PP.
These seemed to be modulation artifacts rather that some sort of digital
hash. I put a 10 nF cap in the attenuator, which should give approx 30 kHz
bandwidth.  This gives a noise floor almost as low as with the DAC off:
    -152.1	-Inf	-Inf	
    -149.7	-Inf	-Inf

At one point I was seeing spurs at 10.5 kHz and 13.5 kHz (the other carrier
frequencies).  But these seem pretty much gone now.  IDK  This was kind of odd
because nobody was doing anything with those frequencies.  At the moment I
have the UR44 USB disconnected.

What's notable now looking at the demodulated output (chart high) is that 1/f
noise has become quite dominant.  Looking at the spectrum plot the knee seems
around 10-30 Hz, but this is somewhat confounded by windowing issues with
the spectrum of a wandering signal.  Most of the action seems to be in 1-5 sec
time scale.

Not clear where the 1/f noise is coming from, but it certainly could be LF
reference noise in DAC or ADC.  At such low frequencies use of the current
sense should be pretty effective in compensating for noise in either
reference.

Possibly clock jitter could also cause this?  I was thinking that use of
common clock for DAC and ADC would have some effect of cancelling jitter
noise, especially in the LF region.

I don't currently have a good way to quantify this 1/f noise.

Of course loopback is the limit of potential dynamic range.  Once I go back to
using the actual magnetic stuff then other noise sources will likely dominate,
either driver or EMI.

Hooking DAC differential output through attenuator/filter with attenuator set
full-scale, with bandwidth 1 Hz - 10 kHz, I get 0.22 V RMS on 323 at scope
output, with 7A22 50 uV/div, x2 for output, so 22 uV RMS.  Using Keithley 179
in this setup I get 3.37 VRMS.

With labview output set to FS, I get 0.33 * 10V = 3.3 V RMS, 9.6 VPP visually
on scope.  I increased the cutoff to 30 kHz for this test because there is
some attenuation of the 7.5 kHz carrier at 10 kHz BW.

From [19 Aug 19] it seems that these boards are 2 VRMS output, so for the full
differential output I should get 4 VRMS, but I have some attenuation.  The DAC
has onboard 100 ohm series resistor, and I have added another 200 on each arm,
with 5K pot across.  That should give 3.57 V.  So we are not too far off from
where we should be.  We have definitely not lost a bit anyway.

But 3.4 V / 22 uV is only 104 dB?  According to datasheet, with 2 VRMS output
(stereo) we should get 127 dB.  In mono mode that should be 3 dB better or 130
dB.

Seeing a periodicity in the noise, amplitude modulation perhaps, at about 100
kHz, on 50 us/div?

Getting a big disagreement between 323 and 179 on the noise measurement.  At
50 uV div, 179 says only 0.073 V * 100 uV = 7.3 uV.  This is a 3x difference,
which would get us to 113 dB.  The difference is presumably due to there being
a lot of noise that is still getting through the 7A22 first order LPF.
Looking at the signal at 2 us/div I can see there is still a lot of periodic
stuff at around 250 kHz, even with the 7A22 cutoff set at 10 kHz.

Back in loopback mode, with LV output at 1.0 and attenuator at max, I get -1
dB, or 0.633 RMS, 0.893 peak.  It seems it is not overdriven, not seeing any
increase in second harmonic 15 kHz (which is at -122 dB FS).  With carrier off
I am getting 2.3u RMS, or 109 dB dynamic range (ignoring any possible
carrier-on effects).

At 16x oversample SINC2, the LTC2500 is only supposed to get 117 dB dynamic
range, 5.2 uV RMS noise.  With DAC off, the noise drops to about 1.2u RMS, or
-115 dB.  That's fairly close to 117 dB I guess.  With inputs shorted we gain
about +1 dB, or 116 dB.  About 1.1u RMS, or 5.5 uV RTI.

So the ADC noise is right where it should be, but in loopback we are still
seeing a noise increase (though much smaller than before).  The DAC setup is
under-performing, and this is presumably because there is still too much HF
crap in the output.  Ideally I guess I should add some sort of active higher
order antialias filter to the DAC output, and also add more filter caps on the
ADC board (since we were also having problems with HF EMI using the sensor and
preamp).

There was a bunch of line-synched crap being coupled into the DAC outputs
which I was seeing when the DAC was off.  This was greatly improved by
connecting laptop directly to ILEMT box rather than hooking both to net
switch.  There was also stuff coming from the 5V power adapter for the
microzed.  However I think this is buried when the DAC is operating and you
have a good differential hookup.  May still be an issue with single-ended
hookups to driver, etc.

I think I changed the ADC filter response from sinc2 to flat, which should
give much rejection of noise aliases, but could not see any difference.  Is it
really in "flat" mode?  Why is there so much noise rolloff starting at 15 kHz?
At 1 msps we are supposed to be flat to 22 kHz.  But we are running slower,
48/64, or 16.5 kHz.  That is about what we are seeing.

I did find there is considerable effect the demodulated signal from mechanical
stress on the DAC (in loopback).  Direct pressure on the IC creates an
excursion >10x bigger than the typical LF noise.  This is consistent with the
idea that what we are seeing is 1/f noise in the DAC reference.  I did not see
any noticeable effect from poking at the ADC board.  Of course this modulation
by LF reference noise is not a big deal in the intended audio application, but
is a nuisance for us.

When I turn on the low carrier and put into distance mode (to get comparable
gains), the low carrier demodulated signal is tracking the 1/f noise in the
high carrier quite closely.  This is also consistent with gain modulation.

When I do single-ended loopback using jumper outside the box, going through
balancer, then I get the crappy noise floor gain.  I guess my
differential-mode filter is not working for a single-ended output.  Shunting
the output by an external capacitor gets us most of the way back



[18 Sep 19]

I have the DAC working and have tweaked the labview so that I can turn off the
UR44 and synch to the microzed system.  This does indeed give nice synch.

However, the DAC noise floor seems rather high?  I added an adjustable
attenuator at the DAC output and patched it directly to the ADC board.  With
the carrier on and adjusted to -4 dB I am getting noise floor:
    -127.1	-Inf	-Inf	
    -125.4	-Inf	-Inf	

At first I thought this was the noise floor rise in presence of carrier, but
the floor is exactly the same when I set the carrier amplitude to zero in
Labview.

But it drops to:
    -157.0	-Inf	-Inf	
    -153.7	-Inf	-Inf	

if I stop sending to the DAC.  If I do send to DAC, but set attenuator at
zero, then the change relative to DAC off is:
    -2.3	NaN	NaN	
    -3.4	NaN	NaN	

(Basically the same with DAC off and attenuator zero.)

It seems the DAC mutes its output when there is no data, and that the noise
being added by the I/V and buffer stages is not much.  Since the attenuator
can zero the noise (and the signal) the noise floor is clearly coming through
the nominal signal path, and not by a sneak path such as digital interference
from DAC into ADC.

Is the DAC counterfit or something? WTF is going on?
 -- Check output level to make sure we are using the DAC full scale (not
    losing any bits digitally).
 -- Since noise is present without signal, measure it directly and see if it
    makes any sense WRT DAC specs.


[11 Sep 19]

I reconfigured ADC to run at x512 clock and it simulated with no difficulty.

At least initially I am going to send DAC data without trying to synch with
ADC data transfer.  This is easier and more likely to work.


[4 Sep 19]

Looking into digital input driver. Sigma delta modulator, etc. We
could for example use 8bit pwm output with 16x oversampling, 62.5 ksps
* 8 * 16 = 256 MHz clock. Don't know what noise performance to expect,
but Cellier15 got 100 dB with 5 bits/8x oversampling. So I would guess
around 4 more bits or 124 dB. Sounds good (too good).
https://hal.archives-ouvertes.fr/hal-01103684

It would be possible to get 2-4 more bits of time resolution by using
the FPGA PLLs to synthesize several phase shifted clocks. The low bits
would select which clock defines the edge. This is limited by the
number of PLL outputs, clock jitter, phase accuracy, routing
delays. My guess is that this trick is unnecessary.

Above paper has a scheme to give local analog feedback based on
comparing output voltage error, using digital input and bridge output
(before filter). Interesting because this works in error (less analog
dynamic range needed, maybe).

Usually some sort of analog feedback is used, but we may be able to
make do with predistortion.  Aside from quantization noise, the main
noise sources would be clock jitter and supplies. We can deal with any
repetitive supply effects by predistortion, but actual supply noise
would get through. This should be controllable by an analog
post-regulator.

Each channel should have its own reservoir capacitors and decoupling
inductors (per rail). If the high carrier ripple voltage is low enough
we do not need much inductance to maintain continuous current through
the inductor (eg. no sourcing into the rail). Then the regulator
should be able to hold the rail pretty stiff. We can fix minor cross
coupling by predistortion, but don't want to push our luck.

I thought about using full bridge output. While it reduces supply
complexity, that is shared across three channels, and the number of
output filter inductors is doubled. Also there is DC on the output
then, which may be a shock concern.

We want a readback of the DC output voltage to see if there is any
offset, since the current transformers will not see this. This does
not need to be high resolution.  Having this serve as AC voltage
readback could also be useful for monitoring load impedance.


[28 Aug 19]

Summary of work on test setup, especially loopback for 23-27 Aug:
 -- My ADC X channel is more sensitive to various interference.  This
    causes the noise floor to rise.  At higher levels the noise floor
    rises on YZ as well.  This creates uncertainty about whether the
    ADC performance is realized.  Also, understanding how interference
    (such as from being near a power brick) affects performance and
    can be controlled will be relevant to the final design, although
    the situation with the separate preamp box is peculiar to the test
    setup.
 -- There is an apparent noise floor rise near the carrier which is
    proportional to level, and is worse than the UR44 noise floor.
    This is what we expect to see from reference noise and clock
    jitter, but could possibly also have to do with clock synch.
 -- There is currently too much gain (70x) for the ADC performance to
    be realized.  Once EMI and carrier-related noise floor can be
    resolved we will want to reduce gain and increase signal, such as
    by operating closer.
 -- The noise of the UR44 inputs is quite good, pretty similar to my
    preamp.
 -- With this source and sensor, at 200mm, driver noise seems to be
    below the noise floor.  There is minimal difference between
    loopback and all-up (over the air).

Things I need to do to credibly evaluate DAC performance:
 -- Implement external clock and DAC interface.  This is the only way
    to get synchronous sampling and control the jitter.  A lower noise
    DAC will also help with the evaluation.
 -- Implement low-noise ADC reference
 -- Make sure EMI issues are under control. Getting DAC inside the box
    should help a lot with EMI in loopback tests.

 
DAC interface, how?
 -- Interface should (ideally) be active only during ADC acquisition windows
 -- Synchronized to ADC 20 MHz bit clock
 -- This means it has to be integrated into the ADC controller
 -- I'm assuming irregular bit clock is OK, it's the word clock that
    has to be regular.
 -- We need a system clock
 -- Reset input

System clock has to be certain multiples of sample rate (LRCK), needs
to be synchronized, but does not have to have any particular phase
relationship.  But in datasheet, when talking about clock synchronization,
the bit clock BCK is also mentioned.  So it would be lower risk to
implement regular clocking.  But it seems you can also have extra bit
clocks, only the first or last N count (depending on format setting).
This is used so a single data line can be multiplexed across multiple
DACs?  With I2S and the similar left-justified format, the bit length
is determined by the input, whereas for "standard" you need to specify
16 or 24 using pins Also gives a way to send 16 bits rather than 24.
I guess I would use I2S because that is more standard?

So it's the rising and trailing edges of LRCK that indicate data
should be latched.  Data is clocked in on the rising edge of BCK.

If we send data at the same bit rate as the ADC, but without pausing
for the convert periods, then we have more than enough cycles to clock
out the data, even though it seems we need to send stereo data for
mono.  Eg. we have 10 * 16 = 160 clocks at 20 MHz/2, whereas we only
need 48 or so.  If we were going to transfer only when ADC is idle we
would need a higher clock rate to transfer both DAC channels.

I guess the DAC interface can be a separate module, with one or two
wires to synchronize as needed.

Maybe I want to use mono mode?  This seems to route the negative of
one channel to the other, giving a differential output range of 2x,
improving the SNR 3 dB (not 6 dB because noise from the two DACs
adds together).  I guess I might as well, and then I can use just one
output, or both for differential.  It could drive the ADC board
directly (with an attenuator).

It seems the DAC system clock is going to have to be a weird derived
frequency? Currently we have 20 * 16 = 320 capture_clks per sample.  I
have this at 15.36 MHz for 48 ksps input.  That is exact.  The
supported DAC clock SCK/Fs ratios are:
    128 192 256 384 512 768

It should be pretty simple to change the ADC interface to work at 512
clocks/sample instead, and then we will be in synch.  We change the
cycles from 20 to 32.  So then capture_clk is 32 MHz nominal for 1
Msample/sec.


[27 Aug 19]

Noise floor with amp inputs shorted:
	-141.7	-134.3	-134.5	
	-139.8	-134.7	-134.9
[X 7 dB better than YZ]

Set up for loopback, but with attenuator input shorted (ref):
	-139.0	-134.2	-134.5	
	-137.6	-134.7	-134.8	
[~3 dB increase on X wrt amp shorted, no effect on others. X about
5 dB better than YZ.]

Levels with attenuator connected to X DAC, muted:
	-126.6	-131.4	-131.4	
	-124.6	-129.9	-129.6

Delta re. attenuator input shorted:
	13.4	3.1	3.2	
	13.8	4.0	4.6	

[X swings from 5 dB better to 5 dB worse wrt YZ.  Difference X wrt YZ
is only 10 dB because YZ got 3 dB worse also.]

Loopback with carriers:
	-124.4	-131.2	-131.2	
	-124.0	-129.9	-129.8	
Delta wrt attenuator shorted:
	15.1	3.1	3.3	
	13.6	5.4	5.5	

[X got about 2 dB worse at HF, YZ nil effect.]

With sensor, source off:
	-138.5	-133.0	-132.9	
	-139.7	-134.8	-133.8
Delta re. loopback attenuator shorted:
	0.8	1.4	1.9	
	-0.7	0.8	1.5	

Source on:
	-127.9	-132.3	-132.1	
	-130.1	-131.5	-131.9
Delta re. loopback attenuator shorted:
	11.7	2.0	2.7	
	7.2	2.5	2.7


Delta loopback with carriers re. all-up with sensor:
	4.5	0.8	0.0	
	7.9	3.0	3.0

Levels in loopback with input XLR cables draped over the power
adapters for laptop and UR44 (like they were yesterday):
	-105.6	-112.9	-112.9	
	-104.7	-127.2	-127.7

 ==> OOPS!

Note that the effect on X axis (whose signal that doesn't even go
through those cables) is much more.  [Since all the channel inputs
are shorted together in loopback this makes more sense than it might
seem; something that couples into one channel couples into all.]
There are various signs that X is more sensitive to EMI, presumably HF
EMI which is only visible in the output as an increased noise floor.
Even once the power adapters are removed I got a 3 dB improvement in
the noise floor on X by fanfolding the excess XLR cable for the YZ
axes (cable run from RJ45 breakout box to UR44).  Putting 4 turns on a
toroid on the USB cable (to UR44) also improved X noise almost 2 dB
(still in loopback).

Just realized the HP 350B is not a balanced attenuator.  Well, that
explains why there is no separate ground.  I switched to using the
HP353A patch panel which has a balanced coupling transformer.  Now the
ADC-connected noise floor is:
	-138.3	-133.7	-133.9	
	-135.3	-132.6	-133.6
Delta from sensor inputs shorted:
	2.7	1.0	1.0	
	-0.4	0.3	0.1	

This is about 2 dB better on YZ and 12 dB better on X than with the HP
350B. (And maybe other changes of wire layout, IDK.)  With X carriers
looped back:
	-127.9	-133.2	-133.4	
	-140.3	-132.0	-132.1
Delta from sensor inputs shorted:
	12.9	1.6	1.7	
	-0.6	2.1	1.9	

So now X swings from about 4 dB better than YZ to 5 dB worse.  And the
DAC-connected effect has largely disappeared.  Perhaps because noise
has reduced on X there is a pronounced humping around the carrier,
taking us from -140 to -125 (visually from spectrum plot, fair
agreement with the noise floor stats).  The humping does reduce
noticeably as the signal is scaled down.  Eg. at -10 dBFS (instead of
-4) we get:
	-134.2	-134.4	-134.6	
	-140.7	-133.3	-133.7	
Basically the hump drops down near the noise floor.

There is some fairly pronounced second and third harmonics, about -84
dBc.  These show up on all channels.  I'm guessing this from the
coupling transformer in the 353B.

I don't get reasonable results with the HP353B when I disconnect the
attenuator from the DAC and short the attenuator input.  In this setup
the ground for the attenuator comes from the DAC cable.


Picking up a bunch of ~100 MHz in the output cable from the amp
probably 1 mV P-P.  Happens whether the amp is on or off, inputs
connected or not.  Maybe shunt the preamp output with a small
capacitor?  

So it seems I've been spending quite a bit of time tweaking my test
setup and trying to understand the performance, rather than proceeding
with the ADC evaluation issues such as synch, jitter and reference
noise.  I'm not sure how much this can be helped.

It does seem there is a problem with interference pickup which is
affecting my evaluation setup much more than the UR44.  I think it is
worth gaining some experience with this issue rather than just fixing
it with setup issues or ignoring it.  That is, I've reduced the noise
floor rise due to the setup by changing the attenuator, but it is a
concern that this rise appears especially in my input setup.

I'm not sure whether to fix the noise pickup in the preamp or
integrate a new preamp into the evaluation box.  The unbalanced
connection from the preamp to the box does seem to be a problem, at
least judging from what I see with the preamp output plugged into the
scope.

I've been rather surprised the degree to which broadband interference
such as hash from the power supplies shows up as a general rise in the
noise floor.  I guess it does make sense that anything which is not
repetitive shows up as noise, and stuff that is well outside the
passband may end up all over the place.

Even without carriers there is a general noise hump in the kHz range,
centered around 5 kHz.  There is some weird crap between 18 kHz and 23
kHz on X that I can see in loopback with carriers off.  There are also
a few other weird spurs that only show up on X.  All this disappears
when I short the input at the box (and the noise floor also drops way
down because of getting rid of the preamp).



[26 Aug 19]

Fixed a weird 21 MHz instability interaction between preamp and
balancer.  At low gains it would oscillate, but only under certain
conditions of load at the output, and when hooked to the preamp.  It
was extra confusing because there was oscillation being coupled back
into the input.  I think the problem was some combination of C load at
the balancer positive output, maybe stray C at the non-inverting
input, and a highish impedance source with perhaps inductive
character.  It helped to isolate the opamp output from the (possibly
capacitive) load using a ferrite bead, and also to put a ferrite bead
and capacitor on the input.  This decouples both the input and output
at MHz.

Because 21 MHz is well outside the response of the 7A22 it was
creating confusing behavior like offset voltages.  Possibly there was
also instability when driving the ADC board?  It seems the ADC offset
is now lower.  I was puzzled why it would be a few mV, when the inputs
are AC coupled.

I also enabled and tested the ground-loop-breaking quasi-differential
output that I had designed into the preamp.  I had originally shorted
this because I was nervous about possible weird behavior when the
output connector "ground" is actually an input.  Lifting the ground
did basically eliminate the few hundred uV of hum that I was seeing in
the preamp output whenever the preamp ground was connected to the
UR44/driver gestalt (even when the preamp was powered off).  But with
the output ground reference only through the output there was a
dramatic increase in HF noise.  I had been planning to use a
capacitor, but any value of capacitance that I tried (0.1 to 1 uF)
seemed to cause instability.  I found that even 15 Ohms was enough to
basically squash the ground loop.  I found that the capacitor was
tolerated when shunted by a resistor.  I ended up using 100 Ohms and
0.1 uF.

The hum is a lot lower on X now.  There was a lot more noise on Y
especially, but YZ improved to roughly match X once I moved the XLR
input cables away from the power adapters for laptop and UR44.

After making a banana shorting block, the pickup in the input lead is
quite minimal when shorted.  These are the shorted-input noise
floor values now:
	-138.6	-135.5	-135.7	
	-138.6	-135.2	-135.3

This is with the sensor, driver off:
	-135.2	-134.8	-133.9	
	-137.1	-135.0	-134.6

The small advantage for X has pretty much disappeared.  Driver on,
muted 200 mm delta vs off:
	0.5	1.0	0.1	
	3.8	3.6	1.6

The levels:
	-134.1	-133.3	-133.7	
	-134.3	-131.0	-133.4	

Unmuted:
	-126.9	-133.8	-133.0	
	-128.8	-132.6	-132.2


So, loopback mode, also at -4 dB level, from X channel output:
	-114.9	-117.2	-117.5	
	-115.2	-128.4	-128.2
And delta from shorted:
	24.4	18.5	18.3	
	23.1	6.5	5.7

Wow, that's weird.  The noise is a lot worse in loopback?  I tried
turning off the low carrier (which is disproportionally big in the
drive signal) but no real difference.  It seems odd that going through
the source driver, the air, and the sensor, would make noise better!
The only thing I could think of that might make this a real effect
would be the resonant output filters in the driver.  But this is a
pretty big difference, like 15 dB.  Possibly the noise is more because
I'm using the HP350B attenuator, which is 600 ohms?  This is somewhat
higher impedance at the input than with the sensor, but would not be
an effect anywhere near that big.  IDK

So, anyway, there is still the noise floor lift issue with the test
ADC.  All of these issues need to be addressed for this to mean
anything:
 -- synch
 -- reference noise
 -- clock jitter

Also, we should be using demodulator output to evaluate rather than
the "noise floor".


[25 Aug 19]

I've changed the spectrum display to use my "stft demodulate".  So now
the display is always by-God dB FS.  As well as the scaling fix there
are some other changes I am seeing.  For one, we can use higher order
window to get narrower channel bandwidth so that we can eg. separate
the low carriers well.  Averaging also works differently.  There are
two outputs now, a phase-coherent averaged complex array array used
for the levels display and an incoherent averaged dB display used in
the plot and for the noise floor measure.

Note that since the levels display is coherent averaged the synch
error with X channel causes level reduction.

The default parameters are now "7 Term B-Harris" window, order 8,
averaging 2.  Averaging is maybe not quite right yet?  But anyway, the
absolute levels are different from betore.  Increasing the averaging
has no effect on noise floor level (or carriers), just variation.  For
noise floor measurements I've increased averaging to 4.

I made an RJ45 cable to hook all of the input channels together and
then equalized the levels.

Inputs shorted (ref):
	-136.2	-134.2	-134.1	
	-137.6	-134.3	-135.0	

Sensor connected (driver off):
	1.6	1.1	1.1	
	2.2	-0.3	0.0	

Driver on, muted, 200mm:
	4.0	1.5	1.5	
	7.2	1.1	1.6	

All on, 200mm:
	14.1	2.1	2.9	
	19.0	2.4	2.7

As noted, the noise floor measure is not the best way to evaluate, but
this is still quite useful.
 -- With gains matched the noise is quite similar, my setup is only
    maybe 3 dB lower than the UR44.  But if I drop out my preamp then
    the noise goes *way* down, like 20 dB.  With the UR44 zeroing the
    gain knob has little effect.  So in my setup, preamp noise >>
    ADC.  I also get more variation in the noise floor as I wave the
    "shorted" cable around.  I'm guessing my preamp is more sensitive to
    HF EMI than the UR44.  There is about 20 dB more hum pickup also.
 -- There is surprisingly little effect of adding sensor, either
    with source off or muted.  Effect is bigger on X.  This may be an EMI
    effect, but might also have to do with unsynch interacting with
    eg. hum.

Effect of -90 degree sensor Rz with driver on muted, 200mm:
	-0.1	0.4	-0.1	
	-2.0	2.5	1.1

So we do see more LF driver noise on the axis facing the source, but
this is well less than the expected 6 dB, and basically none at HF.

Let's look at my preamp noise again.  So, using 7A22 with 1kHz-30kHz.
Let's use 35 kHz for noise bandwidth, allowing for single pole upper
rolloff.

Test conditions:
Zs = source impedance
    50: each input terminated in 50 Ohms
    jump: BNC jumper across inputs, diff input shorted, CM floating
    float: both unconnected
    senXY: connected to sensor X or Y
    senZ: connected to sensor Z
    drivD: connected thru balancer amp, 'jump' inputs, gain adjusted
           for -3 dB FS @ 200mm.    
    
Range = 7A22 range
323 V = RMS volts at SIG OUT
RMS = V RMS at preamp output
RTO = noise density 1kHz-30kHz (35 kHz BW), Referenced To Output
RTI = noise density Referenced To Input

Zs	Range	323 V	RMS	RTO	RTI
------------------------------------------------    
50	20u	0.29	11.6u	62n	2.5n
jump	20u	0.32	13.6u	73n	2.9n
float	0.1m	0.38	76u	406n	16n
drivD	50u	0.27	27u	144n	
senXY	0.1m	0.3-0.6	60-120u
senZ	0.1m	0.5-0.9

Balancer gain:
At 1.08 V RMS balancer out, we have 0.38 V in, so Av = 2.8

This divides the ADC noise density at that input.  eg. referring the
ADC noise back to the sensor input, it is:
   33 nV/rtHz / 70 = 0.47 nV/rtHz

In other words, we could squeeze down the LNA gain a bit, though that
would run into trouble with actual source impedance.  But we also just
have way too much gain.  Suppose we could halve preamp input referred
noise to 1.5 nV/rtHz.  For the preamp to contribute negligibly we
want the ADC noise referred to the input to be 3x above the preamp, or
4.5 nV/rtHz.  That would be a gain of 7, 10x lower than what we have.
We would need to go down to 93 mm distance to get a corresponding 10x
signal boost.

A more sensible scenario would be with the ADC noise equal to the
preamp, when we would want gain of 22 with the hypothetical 1.5
nV/rtHz preamp.  Then we would reduce distance to 136 mm to keep same
signal level.  That would be sort of plausible.  Insofar as EMI
permits, a better direction would be to increase the sensor
sensitivity.  We don't have much room for increasing the field
strength, other than by increasing source size.


With the sensor what we are seeing in time domain is mainly power line
crap, which is highly variable.  Difference on Z may represent actual
EM vector.

See [19 Sep 15]. Oh, preamp gain is 25.  Since then I changed the
damping resistor on input differential cap to 1211 ohms and added a
110 kHz rolloff in the output stage.  Also, I'm not sure how I got
that measurement?  And it seems 2x too low?  If I redo using 100 Hz -
1 kHz there is so much hum it is hard to read, maybe it's 5 uV, maybe
10 uV.  I am super puzzled why I said there was not much difference
floating vs. shorted.  This is extra obvious in the LF range,
completely impossible to miss, about 500 uV p-p of mainly LF noise.
The change in the diff mode RC would not have any effect at 1 kHz.
Maybe I was looking at HF spectrum display?  But it's a big effect
even then (tho the shunt RC would have more effect).



[24 Aug 19]


 ==> Oops, if we set input gains all to the same nominal level in
     standard orientation, then we actually have different sensor
     channel gains because the source coils all have the same drive.
     This may have affected the other ADC evaluation.  We are running
     at a higher gain so the noise looks worse.  When we compare input
     chains we should compare using the exact same source, same
     loopback channel, or same sensor channel.

I put an OPA1612A into the amp/balancer.  This is supposed to be 1.1
nV/rtHz typical voltage noise.

With the 7A22/Ballantine 323 setup, on 20 uV/div, 30 kHz BW.  The
noise floor is 24 mV * 40e-6 or 0.96 uV RMS, which is negligible.
With the balancer input shorted and gain set full scale (gain 9 * 2 =
18) I get 0.33V RMS * 40e-6 = 13.2 uV RMS.  Correcting for single pole
@ 30 kHz, noise bandwidth is 36 kHz.  So noise density is 70 nV/rtHz
RTO (at max gain) and 3.9 nV/rtHz RTI.  At min gain (2x) noise would
be 7.3 nV/rtHz.  This contributes negligbly to ADC noise 33 nV/rtHz up
to gain of 3, while at max gain preamp noise will dominate.

The 3.9 nV/rtHz figure is only a bit more than we would expect just
from the Johnson noise of the feedback resistors, inverting impedance
is 400-500 ohms for each stage.

I don't seem to have written it down, but with the LT1215 I was
getting something like 30 nV/rtHz at min gain (2).  This is in line
with the LT1215 noise spec of 12.5 nV/rtHz.

0.34 with amp input shorted.

84 uV RMS with notch filter thru balancer, set at max gain.  So this
is 4.7 uV referenced to notch filter in/out, or 25 nV/rtHz.  With the
OPA1625A, which is 4.5 nV/rtHz FET input, I get 62 uV RMS (18
nV/rtHz).  Reducing Q to unity, I get 42 uV RMS.  Not sure why this so
high, would expect more like 1/2 of that.  Well, I guess it's the
resistors again.  Not sure what the impedance is looking into the
twin-T, but the DC resistance is something like 15K ohm, and 18
nV/rtHz is the noise of 20K Ohm.  Also, there is noise gain when Q > 1
because of the positive feedback away from the notch.

The working from the DAC spec of -129 dB dynamic range at 4.5V RMS
output, this is 16 nV/rtHz.  So the notch filter noise is just about
the same, not less.  But we would be able to see if there is a
significant rise in the noise floor in the presence of signal.

Note that in un-notched loopback mode we would need some attenuation.


[23 Aug 19]

So, what did we learn?
 -- FFT window has significant >6 dB effects on the noise floor, and
    some window is necessary due to imperfect synch.
 -- We can look at driver noise by moving the sensor very close, with
    carriers muted.
 -- Confusion about the X gain, overload happens at -6 dB?

The differences with noise driver on/off @ 80mm, from yesterday:
    14.8	10.9	11.1
    25		17	17.2

I was rather off in eyeballing the differences.  The X channel looked
more lower because it went below 10.  But these values are not so far
from the 24 dB I expected.  And, anyway why did I expect 24 dB?  That
would be the difference of "on" at 80 and 200 mm, assuming driver
noise dominated.  If everything else was way low noise, then we could
get an arbitrarily large drop in the off state.

The window also has a pretty big effect on LF noise floor (driver off)
because windows with weak frequency resolution blur together the
hum peaks.  There is not such a big effect on the noise floor measure
with drivers on, though the Blackman Harris window does give a
close-in hump.

For now, I'm going to standardize on the 4-term B-H window, which
seems like a reasonable compromise between leakage and resolution for
the on/vs off measurements.

At 200mm, increase in noise floor measure, driver off vs on, is much
more pronounced on X channel:
    9.8		1.3	1.1	
    13.5	3.1	7.7	

This is the change from 200mm to 80mm, driver on, muted:
    18.8	10.1	10.5	
    23.0	16.6	15.3	

This is the change off vs on (muted) at 80 mm:
    21.2	10.3	11.1	
    32.6	16.9	16.7	

[Looking at just the dB shifts partly avoids our uncertainty about the
X gain, though FS is also important.]


OK, testing in loopback to focus on the ADC evaluation itself, as
opposed to system issues of what is noise limiting.  I dropped the low
carrer, so we have only 7.5 kHz (there were noticeable IMD sidebands).
I set the differential input to ADC at 8V p-p by using external HP350B
attenuator and the internal gain pot.

That should be -1.9 dBFS peak.  According to "carrier levels", this is
-3.9 dB.  On the trace display I get various values according to the
window, none=-4.6, B-H=-6.9, 4-term B-H=-7.6, etc.

In the time domain I am showing 0.641 peak, -9.9 dB.  -9.9 dB is
0.640, so that is a peak measure.  But I should be at 0.8.

SIG OUT on scope is 0.5 V/div.  So eg. with calibrator at 4V PP, input
at 2 V/div, we get 2 div P-P on scope.

Units V/div cancel (or something) so we get gain 0.25, dimensionless,
or V/V if you prefer.  The gain is in general 0.5/(attenuator
setting), or the voltage attenuation is attenuation x2.

[Why do they call it V/div anyway?  Am I confused?  Oh, the units are
inverse gain.  As a gain it would be divs/V, but it is the inverse,
attenuation in V/div.  This is convenient because it matches the
eg. x10 notation for attenuation.  This also always seemed backwards
to me, the probe is *dividing* 10.  The interpretation is that is what
you multiply by to get the actual value.  There is a multiplication,
but it happens between eyeball and paper, not in the hardware.]

Anyway, at 4V P-P input, with gain 1/4 (4x attenuation) the output is
1 V P-P, or 0.5 V RMS.  The Ballantine 323 and Keithly 179 both agree
on this.  So I have re-understood how we can measure RMS through the
7A22 (a little more clearly than before).
 ==> 10V P-P sine is 0.884V on the RMS meter @ 2V/div (4x
     attenuation).

So, I am getting 8V P-P 7.5 kHz at the ADC input (2 V/div on 7A22).
This is -3.2 dBV at the meter, or 0.695V (Keithley 179 says 0.706).
So, 2.78V at the input.  After 2*sqrt(2) RMS to P-P conversion for
sine, that is 7.86V.  That is decent agreement with the scope display
(2% error).

re. full scale, 8V P-P seems to be the saturation point.  Above that
the KF demodulator unlocks and the P-P amplitude starts decreasing
(without ever looking like clipping).  I guess the reason it doesn't
clip is that we don't have enough bandwidth?  eg. the first square
wave harmonic is third, which would be 22.5 kHz.  Well, we actually do
see 22.5 with the LPF off.  More specifically, I guess that the sinc2
rolloff prevents us from seeing any square corners, even without LPF.

[I checked and the raw data display is now truly the raw data (after
LPF). At one point it was being converted into frequency domain and
back, IIRC.]

There is rather odd -67 dB fundamental in the residue.  My guess is
that the fundamental leakage is because of synch error.  The notch
around the carrier residue is also missing.

The window also has a significant effect on the peak shown in spectrum
plot.  A window can of course in general have any DC gain, but this
effect is likely increased by synch error, IDK.  I don't recall using
different windows in labview that much.

OK!  At 750 Hz from HP 209A I get well defined clipping.  The time
domain display says 0.996 FS peak, 0.704 RMS.  This is 3.54V RMS at
the ADC input, which should be 10.02V P-P, agreeing with scope and
what the ADC input range should be.
 ==> So there is too much HF rolloff somewhere, preventing full scale
     output ant 7.5 kHz.  Analog, digital or both.  This seems perhaps
     more digital because bad things do happen at around that level?
     Oh, and also, speaking of DC gains not one, the DC gain of the
     IIR prefilter is not 1, it's -1 dB.
 ==> At 7.5 kHz the signal at the ADC input is -2.4 dB wrt the input
     jacks. We are -1.2 dB at the onboard buffer outputs. CL output
     impedance is 10 ohms?  That is way high.  Datasheet says less
     than 0.1 ohm at 100 kHz unity gain.  Maybe phase shift?  Uh I
     think we are in current limit.  OK, I ripped the C72 out and
     things are much much happier.  As well as loss of amplitude there
     was terrible distortion due to slew limiting.  Now at 7.5 kHz the
     time domain display shows 0.922 peak, which is -0.7 dB.  This may
     be the sinc2 filter rolloff.



-3 dB FS is what the RMS value should be at FS sine input (as shown in
time domain stats).  This is about what I get in the spectrum display
with the B-H window (which I had tweaked for).

Not sure why I am showing the time domain 'dB' value at -6.1 dB.  At
one point this was tweaked to match up with something, but that may
have had to do with spectral normalization confusions.  I have it
documented in tip strip as "RMS dB re. FS sine".  I would expect that
with a FS sine this should be 0 dB, or maybe -3 dB (matching the RMS
output).


Spectral normalization issues:

Any scaling factor applied to the dB spectrum is just a zero offset.
So for relative measurements it has no effect.  Power vs "voltage" is
x^2, but dB is supposed to be power always, so in a dB display there
is no difference.

The 'Trace display' levels indicator tracks the amplitudes in the
trace display spectrum.

OK, what do we *want* in the spectrum display?
 -- For looking at the carrier signals it is most useful to have 0 dB
    be a full-scale sine wave (ie. dB FS).  This will require window
    normalization according to the DC gain.  This should then agree
    with the time domain stats.
 -- Spectrum display should also match (or be settable to match) the
    carrier amplitudes measured at low rate.  This would require
    adding window normalization to the low rate FFT.
 -- It may also be useful to support normalization by window noise
    bandwidth, and perhaps also by frequency resolution, to give a
    noise density.  This would have the property that the noise floor
    should not change as we change the window or the FFT size.  As we
    change the FFT size the floor woudn't change, but the carrier
    peaks would!  That seems weird, but having a noise that could be
    inter-converted with the input-referred noise density would be
    very interesting.

I currently use the 'RMS' averaging a bunch.  If I switch to using my
windowed FFT then the window order is a form of averaging, but it is
going to be like 'vector' averaging.  Vector averaging gives a lower
noise floor because the noise is not phase-coherent.  But this is
indeed what actually happens in the low rate FFT.  It does seem that
post-averaging of the spectra is useful for display purposes.


At that setting we still get various peak values in the spectrum plot,
depending on window:
    flat top:	-6.5 dB
    none: 	-0.8 dB
    Blackman Harris: -3.1 dB
    B-H 4 term:	-3.8 dB

I'm thinking that window gain issues may explain why I had to add a
fudge factor to the result to get it to match up with eg. the time
domain statistics.  How are the windows normalized?  I think it is to
peak of 1.  Clearly this is not going to give unity DC gain, but might
have other intesting properties, like maybe conserving total power.

See:
https://dsp.stackexchange.com/questions/18164/is-it-customary-to-correct-for-the-gain-of-a-window
The DC gain is the mean of the window coefficients.

Um, OK.  I dug down into the spectrum display and it's a mess.  Need
to think about what I want here.  Really it should be the same as the
demodulated output, or at least settable to be the same.  But
currently it is the PSD, which is square of the amplitude (or 2x dB),
and is also already being normalized for noise.  And as I have often
noted, the window order is 1 rather than whatever the low rate window
order is.


There is about 15s buffering lag on the TCP connection??  Now I am
seeing much less, only a second or two.  Is this because I changed to
running "nc -w 1"?
 ==> TCP output rate is higher than we consume, this builds up in a
     buffer somewhere.  You can clear this by stop/restart of ilemt_ui.


Seeing some low driver-off levels again:
    -131.1	-124.5	-125.7	
    -134.4	-127.0	-126.6	

Earlier this morning was fairly similar to what I saw yesterday.
Back at 200mm, everything on:
    -116.8	-120.2	-123.8	
    -119.0	-123.6	-119.3

Driver off:
    -120.1	-120.7	-124.6	
    -121.2	-126.0	-123.2	



[22 Aug 19]

Hah! I found the problem with the driver Y channel which was causing
the output offset and the bus pumping.  See [3 July 19].  This cropped
up again, and my bus clamp blew out somehow.  I checked all the 22 uF
electrolytics in-circuit and didn't find anything too alarming.  ESR
was mostly 0.6-0.7 Ohm.  The capacitance was rather low at 16 kHZ,
more like 14 uF than 22, but losing capacitance at HF is typical for
aluminum electrolytics.  The one cap nearest the heatsink seemed
somewhat worse, more like 1 Ohm 10 uF.  FWIW, the cap spec'd in the
app note is a completely standard 85 degree C "general purpose
consumer" type, doesn't even have an ESR spec.  This board seems to
have substituted a physically smaller generic part, which is not going
to help with lifetime.

I was seeing an output offset of up to 2V this time.  This would
correspond to a current error at the inverting node of only 15 uA.

I started messing with heat/cold and found a big effect on the output
offset, heat improved it, cold made worse.  It seemed worst around the
input coupling cap, so I replaced that with a 3.3 uF mylar (had been
47 uF electrolytic with my modification).  This did *not* help.  I had
already noticed flux buildup on the bottom and cleaned that, to no
effect.  But now I noticed some on the top of the board near the input
coupling.  I cleaned this and problem solved (cross fingers).

R114 is noticeably discolored on some channels, and is getting up to
170 F or so.  But voltage drop across these resistors seems to be
around 22V max, which is 1/2 W.  We *should* be pretty OK.  Oddly, the
current draw seems to flip between different levels or something?  Now
I am seeing 8V on the Z channel.  Maybe because drive is present?  But
Y is still at ~20.

I went through and compared all of the power-variant component values
to the reference design.  This seems to be mostly the 200W variant.
I'm not sure what R114 is doing in the first place, but these are
spec'd as being 2.2K in the 200W variant, rather than the 1K actually
used (spec'd for 150W).  There is also one stinker component, R12, the
low side current limit resistors, which is just weird, not any of the
variants (would be an extra high current).  The value 8.2K happens to
be the same as other current limit resistor.  Probably this is fairly
harmless, although the limit seems much higher than what we need.

R114 is in the collector of a series pass regulator TIP31A, so is not
obviously doing anything.  I had been thinking it was to take on part
of the power dissipation, but this sort of current is well within the
DC SOA for the TIP31A.  Or stability?  Anyway, increasing the value is
only going to increase the power dissipation in the resistor since it
is being driven by a current.  Another thought is that perhaps this is
to limit peak currents during startup, nearby nuclear explosions, etc.

I was looking at fig 10 in the IRAUDAMP7 ref design, which is a
spectrum plot of 1 kHz tone, and was thinking "Oh, so the dynamic
range is only ~95 dB".  But then I realized that the position of the
noise floor is arbitrary.  I think this is similar to the DAC
datasheet; it seems preferred for audio components to have spectral
plots that show no harmonic spurs, even at the cost of making the
dynamic range look low.  Since you can raise or lower the noise floor
using FFT size and averaging, and can lower the spurs by reducing the
signal, it is one way or another possible to submerge the spurs in
noise.  Oh, and also that plot is dBV with a 1V output, not (as I was
thinking dBFS).  So they *have* reduced the signal, and this explains
appearance of reduced dynamic range.  The resolution bandwidth seems
fairly narrow, a few Hz.


OK, so I have the source back together enough to use it. (Holding off
on deciding whether to fix what isn't broken.)  And now it is clear
there is a bad synch problem.  The frequency is surely a little off,
but I think it is made a lot worse maybe because we are losing samples
when there is a timeout.  As is, the X carrier is showing up as 8.2
kHz rather than 7.5.  The actual output rate according to the
frequency counter is 44.0208 kHz.  I had designed for a 44 kHz rate.
But oops, the audio interface is set for 48 ksps!  I changed the FPGA
for 48 ksps, and am now getting 47.9983 ksps, which is working much
better.  However the rate error is still big enough the spectrum is a
complete mess with no window.

However there is a noticeable noise hump near the carrier, and even
away from it the noise floor is exactly the same.  With outputs muted
the X channel noise is reported as -126 dB on X rather than -121 dB on
the other channels.  But there is the noise hump from "Not As We Know
It".  When I turn that off, I am getting a noise floor of -132 dB vs
-121 dB.  So that is significantly better.
 ==> Oh ho!  The hump near the carrier basically disappears many of
     the higer order windows.  I wonder if this was an issue with the
     BB ADC evaluation?  The synch was also not perfect there.

I expect that changes to reduce clock jitter or reference noise will
not have much effect on the noise floor, but a 10 dB improvement is
still solid.  However with the driver on (w/o interference) we are at
about -129 vs -122

Hmmn, now with driver back on, I am not seeing any noise improvement
on X.  Before it was also noticeably better at LF.  There is a
considerable noise hump roughly symmetrical about the 7.5 kHz carrier.

Oh, and when I rotate sensor back to normal position the noise
floor changes ~6 dB, on the order of the difference I was seeing with
driver on.  But now I can't see whether the driver is on or off, and
any X channel advantage.  It seems the ambient noise has increased???

Note: the "Noise floor" on the "Stats" page is based on the trace
display.  Also, this can be somewhat affected by whether the input LPF
is on or off.

Uh, and now I am seeing the on/off effect again?  So, to standarize
condition, select raw data, set averages to 15, 7 term B-Harris
window.  So, now with the source off (outputs not muted) I am getting
-131 dB vs -121 dB, so where I was before.  Has EM noise gone away, or
am I doing something different?  And with driver on (carriers on) I am
getting:
    -119.7	-118.8	-122.1	
    -121.3	-125.4	-119.3	


With X input shorted at the unbalanced input I am getting -146 dB.  So
we are clearly not being limited by either ADC or the
unbalanced/balanced converter.
    -146.6	-119.9	-120.9	
    -144.8	-124.9	-119.2

 ==> It seems that I am getting "unlock" when the X carrier goes above
     -6 dB.  Is there still some bit shift problem?  With the audio
     interface I can go up to -1.2 dB.

With both preamp inputs shorted (but drivers still on) I get:
    -139.0	-119.0	-121.7	
    -136.8	-124.5	-122.8	

 ==> Eh, something weird with coupling into preamp/adc setup with
     inputs shorted?  Not sure what is going on.  The levels vary as I
     move the preamp around and rotate it, but not in any simple way
     corresponding to distance from the source.  I think I am way too
     far from the source to have much pickup with a single turn.  I'm
     thinking more of a ground loop, or HF EMI coming out of driver.
     I don't have the box all the way closed up, back is entirely off,
     and only a few of the screws are in.
 ==> There does seem to be some effect from ground loop, connecting
     box to ground.  But there is also distance/orientation effect on
     the preamp.
 ==> re. distance for effect, I was recalling metallic interference
     effects, which is r^-6, but this is only r^-3.  We have >100 dB
     dynamic range so even a small effect is visible.

Anyway, FWIW, with preamp inputs unconnected the noise floor rises up
to match the other channels.  This must be a cooincidence.  But
clearly source impedance is contributing to the noise floor.  With
inputs floating there is only a 20K pull-down.

I moved the source/sensor as far away as the wires would reach.

OK, now with driver off:
    -124.9	-125.3	-123.3	
    -125.0	-125.8	-119.9	

And back on:
    -124.3	-126.0	-124.4	
    -124.6	-126.6	-119.3	

Hmmn, at some point I tweaked the gain of the ADC driver to put it at
-6 dB.  Maybe this was after those early results where I was seeing a
better noise floor?  IIRC, I was not seeing any improvement on noise
with driver off vs. on with the UR44, but I had inferred that the
driver noise was perhaps right below what we could detect.

With the sensor at 80mm, driver on, but outputs muted:
    -109.4	-115.3	-114.4	
    -102.0	-110.1	-109.4	

And driver off:
    -124.2	-126.2	-125.5	
    -127.0	-127.1	-126.6	

Given r^3 field dropoff, at 80mm we expect 15.6x higher signal vs
200mm, or +24 dB.  This more like what we are seeing on X.  Maybe this
is our 1-bit error that we were seeing in with input clipping?  What I
see looking at output on scope, 1 Hz -- 30 kHz BW, with RMS meter on
scope output, is about a 17 dB shift in noise with source on vs. off.

In time domain, a pretty prominent aspect of the noise is bursts which
are fairly periodic about every 500 ms.  The bursting is at about 30
kHz, which probably just reflects with LC input filter in preamp.  As
well as being pretty peroidic, the bursts also seem to be line
synchronous.  For some reason the 120 Hz switching hash is better
suppressed with the preamp than running the sensor directly into the
7A22.  I guess that is because of the input capacitor in the preamp.



[19 Aug 19]

Looking at the PCM1794A DAC datasheet.  Turns out that it is a current
output DAC (with an internal reference) so that will not be possible
to drive it from an external voltage reference.  There is an Iref pin
which is supposed to be grounded through a resistor, but it is not
documented what this does.

So, using this DAC it would not be possible to do full ratiometric
operation, but we can still correct any gain drifts through the
current sense channels.  With the current output some offboard
circuitry is required for the IV conversion and (optional) diff to
single-ended conversion.  However, on the plus side, the full scale
can set to whatever you want.  The boards I have seem to be set up for
2V RMS (5.6 Vpp).  The specs are at 4.5 VRMS output.

It's interesting that the reference noise seems to be relatively
noncritical in this design.  The SNR spec holds up all the way to full
scale.  Tradeoffs for general purpose use are that there is no DC
accuracy to speak of, and the gain variation is fairly bad also. Both
these specs are down in the 1% FS range.  These are not problems for
us though.

They also say (without elaboration) that the design has reduced jitter
sensitivity.  The input digital code is upsampled 8x then digitally
filtered, implementing most of the antialias requirement.  The output
circuit has a 2 pole rolloff with poles at 96 kHz and ~200 kHz.

Performance is better at the lower output rates.  This does not seem
to be a matter of increased oversampling (as is common with
sigma-delta ADCs).  Notably, THD+N increases 4x from 44 ksps vs. 192
ksps.  The channel separation decreases 3 dB.

So, dynamic range and SNR are specified A-weighted, which approximates
the ear's response.  This appears to be -3 dB at 10 kHz, which with
single pole approximation would be 12.2 kHz.  So at -129 dBFS with 2V
RMS output, the noise is 0.7 uV RMS, or 6.4 nV/rtHz.  Even allowing
for the lower full scale this is way less than the LTC2500 noise
density, 2/3.5 * 33 nV/rtHz = 19 nV/rtHz.  About 3x lower,
interesting.  So the DAC is definitely not going to be noise limiting.

With sinc2 filter 16x decimate the LTC2500 noise is -116 dB FS.  These
numbers are not directly comparable because of different bandwidth,
the LTC3500 noise is measured as 3.1 dB higher.  So the DAC noise is
-10 dB re. the ADC.

FWIW, it also seems to be common to measure dynamic range according to
noise with no output.  But for THD+n it would seem you could not, IDK.
THD+N at 44 ksps is 4e-6, or -108 dB.  It is not entirely clear what
is going on here, whether there is a noise increase at high output, or
distortion, or both.  Surely there is more distortion.  I notice that
the FFT plots are at -60 dB!  (Not clear what the reference is, I
assume FS.)  At that level no spurs are visible, but the noise floor
is only 100 dB down.  That seems rather deceptive.  At least they
didn't suppress the zero on the Y axis.  From that it seems safe to
assume the SFDR is around 100 dB, compared to 120 dB for the LTC2500.
Harmonic distortion is not really a problem for us, but we can't tell
whether the noise floor rises.

Well, we will see.  Now that I have a notch filter it should be fairly
simple to figure out where any noise floor increase or distortion is
coming from.  Interesting, neither part has an IMD spec.


[14 Aug 19]

FWIW, the eval board FPGA seems to be cycling the chip enables
RDLA/RDLB to match the MCLK.  So the output is not enabled when we are
waiting for a conversion.  This seems deliberate and fairly harmless
(though the output is presumably floating then).  (FWIW, this should
also have the effect of opening the configuration window after each
read.)

Also, the RDEN output indicating new data seems to be basically a
gated version of the BUSY line.  That is, it is at the same time and
duration as the next conversion cycle (after decimate count reached).
This is relevant because the negative edge of RDEN begins the synch
period if we are going to do recurrent synch.

I'm suspicious that the data garbling I am seeing is an interaction
between synch and distributed read.  It seems wrong that I am not
synching again after doing the configuration.

re. FFT normalization, after I fixed the temporal statistics dB to be
0 dB at FS this number agrees with Carrier levels/dB.  So it is just
the spectrum display that is messed up.



[12 Aug 19]

So, thinking about references, what is our requirement?

LTC2500 is spec'd as 5.2 uV noise at decimate 16 sinc2 filter @ 64
ks/s.  This is a little fuzzy because we don't the exact noise
bandwidth, but it is presumably somewhere between the spec'd -3db
bandwidth (20 kHz) and the Nyquist BW, 32 kHz.  Let's call it 25 kHz.
Then the noise density (ignoring 1/f effects) is 5.2e-6/sqrt(25e3) or
33 nV/rtHz.  In order for the reference to contribute negligibly we
need about 1/3 this, or 11 nV/rtHz.
[FWIW, this is re. a 10V PP full scale, 3.5V RMS.  So the noise is
-116 dB FS.]

I'm not entirely sure how realistic that is with a multi-stage
reference chain (you would definitely need low noise buffers).
But anyway, to get there at 200 Hz, given eg. the 90 nV/rtHz of the
LTC6655 (used on eval board), we need 18 dB attenuation (say 20 dB).
That's actually not so bad.  You could achieve that with even a
single-pole filter set at < 20 Hz.  

BTW, the LTC2500 reference input is *not* differential, but does have
two reference pins, one of which is used for remote sense in the dev
board.  The LTC6655LN gives up the remote sense function in favor of a
bypass pin which reduces output noise down to about 18 nV/rtHz.

But interesting to compare eg. the LT3042, which is a low noise
regulator.  Given that we are fully ratiometric if we use a common
reference for all channels, we do not really care that much about DC
stability.  This part has a far lower output noise, 2 nV/rtHz @10 kHz.
The 1/f corner is higher though, but this can be overcome at 200 Hz by
a realisitic size reference bypass (22 uF).  On the LTC6655 datasheet
there is a design idea which uses the LT3042 as a low noise output
buffer, with additional filtration.  Using this part just as a buffer
seems a little weird, tho it is a buffer designed to run with high C
load, low Z out, high PSRR.

This part might be good for some of the supplies also?

I guess that with a 5V reference we need more than 5V supply rail for
the reference buffers.

FWIW, the LTC6363 is a low noise fully differential amp that could be
suited for ADC drive.  According to the eval board schematic there is
an option for differential driver, but the manual doesn't seem to
mention it.  But this part is mentioned in the LTC2500 datasheet.

"I'd suggest you to try a Bainter notch, or an high-pass SVF notch,
running at moderate Q - both require 3 (or 4) opamps, but are fare
less senstive to passive component matching than the bootstrapped
Twin-T or Wien bridge notches"


[9 Aug 19]

Got the FPGA simulations to where I'm happy with them.  And card is
booted with the new bit file, which seems to be reading all ones
(without any hardware connected).

Try using guard channel signal as reference for adaptive noise
cancelling. Unlike using a lagged output this will only cancel EMI,
not vibration.
 ==> [13 Nov 19] Sure this will not work.

But a straight hum filter at the output doesn't seem so
bad.  But there is near-synchronous EMI due to motors and so on, which
a narrow notch filter may not catch. BTW, I wonder if ANC may mess
with actual motion also? If we are just learning a filter? IDK.  I
think the prediction is based on just the reference, so if it has no
motion signal it can't cancel motion. We are learning a filter, but
it's tuned to convert the reference into the measurement.  This also
means we can rapidly track amplitude or phase changes in EMI as long
as they are shared across the signal and guard channels (the changes
in reference just need to pass thru filter delay, the filter optimizer
doesn't need to track it).

My thinking is that the KF demodulator is not such a great idea since
the guard channel KF noise estimate works so well. Also my idea that
we could use the residue to drive ANC doesn't work.



[8 Aug 19]

Have been mostly working on FPGA code for interfacing the LTC2600-32
this week.  Working my way up the learning curve.  I am debugging it
in simulation now.

I thought some about the reference design, especially packaging.  I'm
thinking that for the basic structure it would be nice to attach all
of the working bits to a chassis/front-panel which slides into a
rack-mount chassis.  So all connectors will be on the front panel.
This is supposed to be somewhat better for EMI than having connectors
on multiple sides of the box.

I'm going to put each 3 inputs (XYZ) on a plugin card.  The input
cards will be kind of expensive, $100-$200 for BOM alone, so a field
input expansion upgrade would be handy.  This will also make it easier
iterate the critical analog section design.  At first I can populate
just one ADC on one card.  It would be possible to use the existing
MicroZed breakout initially, rather than design and fab on the entire
system in one go.  The driver current sense can use the same input
card (possibly with different part stuffing, since it needs a fixed
low gain).

[It is likely that an optimized commercial design would use a single
board in a pizza box to get single board assembly.]

We could use a standard edge connector like PCIE.  The converter cards
have 3D printed aluminum parts that attach the plugin card to the
front panel with machine screws, rather similar to the metal bracket
on a PC plugin card.  Ideally the connector should be mechanically
tied to the bracket, which then fits in a rectangular cutout in the
front panel.

Though this will be a bit confusing with the actual ethernet
connector, the card can be populated both with the RJ45 connector and
a high reliability connector, so that we can use our existing sensors,
and can keep using RJ45 for convenience.

Maybe something like 2U or 3U half rack box.  The box would not need
to be very deep, maybe 8-10".  Drivers could eventually go in the
other half width.  But we need room for at least 3 ADC cards and one
DAC, and having room for 4 ADC (3 sensors) would be good.  (It would
not be hard to make width variants of the motherboard.)

We will be using the existing driver for some time, so we want
balanced XLR cabling running to/from the electronics.  The mini snake
cable may make sense eg. for the current feedback, and also for drive
if we can retrofit balanced input to drivers.  It's probably on the
big side for sensor cable, but we can continue to use ethernet cables
until we decide to get a fancy cable and connector setup.

Probably we would use an external 12V supply with converters for other
voltages. +5 for MicroZed, +/- 5 analog for ADC, maybe 2.5 for
digital.  The analog input section may need +/- 15.  Supplies would be
on the motherboard, behind the Zed.  I'm thinking the Zed goes on the
left (looking from the front), and the IO cards to the right, first
output, then the inputs.  When they are ganged together the driver box
would be to the left of the analog IO box to keep them farther away
from the inputs (and closer to digital signals if we have digital
drivers).

The motherboard also has the clocking and reference circuits.  If the
analog sections are at the back of the motherboard then they will be
relatively uninvolved in digital ground currents.  It is nice that the
high speed digital paths are on the MicroZed, so we do not need to run
any signals off that aren't synchronized with the data acqusition
clock and sequencing.  When we are converting there should be no
digital activity on the motherboard at all except for the clocks.

Some crud from processor activity will presumably leak out into the
ground and also come out as common mode on the signal pairs.  But all
the analog and digital being differential should help with this.  We
could also put ferrite CM filter chips on the lines.
 ==> With the fanout buffers offboard from the MicroZed this will help
     with CM noise on the outputs, since they get rebuffered with
     cleaner supplies. Leakage out of inputs to Zed should be much
     less of a problem because they are looking into high Z CM
     impedance on the Zed and low driver impedance on the input card.

We could integrate the DACs on the motherboard, but we have to get the
DAC outputs offboard somehow (with a clean ground), and the signals
would have to go through (or at least to the outside side) of the
digital area, so it might be good to use a similar plug-in card for
the DACs.  Using the vertical card also saves space, given that we
have already commited to a taller form factor.

We need:
   seperate inputs for each ADC output:
     adc<N>_SDOA
     adc<N>_SDOB
   2 lines * 3 ADC * 4 cards = 24 pairs, or 48 pads
   more if we want eg. DRL status

   output per card for the DAC inputs
     SCKA
     SYNC
     SDI
     SCKB
   4 lines * 4 cards = 16 lines, or 32 pads
   4 lines = 8 pads with external fanout buffer

   clock related IOs:
     MCLK input
     MCLK reset output (forces MCLK low at clock buffer)
     capture_clk input
   3 lines = 6 pads

   DAC?  Something like
   2 clock outputs (bit and word clock)
   2 data outputs (one per stereo DAC)
   4 lines = 16 pads
     
That's 78 pads with external fanout.

 ==> We also need outputs to control the gain switching, probably a
     serial digital expander on each card.  Maybe I2C with slot
     address strapped by card edge pins to motherboard?  This would
     let you test for card being present, and perhaps bring on/off
     some testing signals.  This could be driven from processor,
     though that would give up on close synchronization of gain
     switching.

Hmm, so we would be tight on IOs if we ran all these into the FPGA,
but it gets reasonable with external fanout on outputs (which is a
good idea anyway for CM noise reduction).  It's the inputs that are
the big pin suck.

We don't actually *need* SDOB.  What that gets us is full rate data
during normal operation and the overflow flag.  But we can get full
rate data from SDOA for testing.  (For some reason the datasheet says
the max SDOA output rate is 250 ksps; I suspect this is an error, and
even so this is still a much higher rate.)  The main thing that we
might see on the full rate data is some sort of board level noise
coupling, since the intended front end analog bandwidth is much lower
than the full rate sampling.

All the digital stuff gets a lot simpler if we are willing to give up
on full rate readout because the SPI clock goes way down.  We'll have
to see what happens with the board design in terms of real-estate, but
full rate output would only be for testing, so we can live with there
being bugs at full rate.  We can get a sense of the amount of crap in
the inputs by subsampling like a DSO, not initiating conversions at
full rate, but reading out each sample.

Thinking we want to use LVDS for connections between FPGA, clock and
converter cards.  This will reduce emission (including into analog)
and also minimize pickup and reflections on the critical ADC clock
nets.  We want to have the capability to clock out full rate ADC data,
which requires 100 MHz SPI clock.  This starts getting into the
"interesting" zone for single-ended unterminated signals.

Within each converter card I would think we can distribute inputs as
single ended with branches and stubs, but it would not be too big or
expensive to put a receiver at each converter, with multi-drop on the
LVDS and termination at the last converter.  Most of the input board
real estate is going to be for the LNA/gain sections, especially if we
go berserk with gain switching.

The on-card signal integrity will be greatly relaxed if we use LVDS
compatible receivers with slower output slew rates.  At 100 MHz we can
tolerate rise/fall of 1-2 ns.  Since LVDS is intended to be fast, this
might be a different class of digital receiver with compatible
voltages.  We would want the low slew even at lower clocks to avoid
reflections that might double-clock.

Going down from the top of the card, analog would be first, then
converters, then digital drivers/receivers.  Reference buffers would
be squeezed in along the lower edge of the analog, with reference
input traces at the very back of the edge connector, running around
the outside of the digital paths.

The ADC and DAC cards should have a differential input for the
reference.  May be simplest to make the per-chip reference buffer
full-differential, and parallel the inputs across the buffers.  There
are no loading or dynamics issues for the reference distribution.
Preferably the reference buffer has low offset and low 1/f noise down
to 200 Hz.  This may not be possible with full-differential amp.
Offset is less critical since we will want to calibrate gain anyway.

In clock generator, we generate MCLK as a square wave, then use it to
toggle a FF chip, like in the ADC eval board.  The FPGA MCLK reset
forces MCLK low at the end of the conversion, and also can be used to
suppress MCLK conversions when we don't want them (like during
configuration).  The capture_clk used to drive the FPGA SPI is also
generated at the same time.  We want to be able to configure the clock
generator over I2C so that we can change the clock rates, both the
convert rate and (separately) the SPI rate.  I'm not crazy about the
idea of using two different SPI clocks for the filtered vs. full rate
output.  If we want full rate we speed everything up.

We want some good test points on the motherboard. Bring out the clocks
and a couple other signals to 450 ohm resistive x10 divider SMA
connector footprint.  For the ADC signals we don't capture, bring out
test points on the input card for one of the converters.  We also want
some debug signals from the FPGA routed to connectors.  These could be
used for synch signals such as when we have acquired a word or a
high-rate block, or whatever.  Maybe a couple with SMA, with a bunch
of other unused IOs broken out to 0.1" headers.  This could be
expansion for digital output to drivers.

If later on we have digital output to the drivers then this would be
active all the time.  This would be in differential signals that comes
out of a connector on the left edge of the motherboard and goes to the
driver box, located to the left of the analog I/O box.


[5 Aug 19]

Huh, I don't know if this typical for a bidirectional SPI, but the
output transitions on the rising edge of SCK (so we sample on falling
edge), while the input is *sampled* on the rising edge of SCK (and so
presumably transitions earlier, such as on the previous SCK falling
edge).  It does seem that a two-phase clock is enough though.
 ==> But in the master both are clocked on the negative edge SCK.  In
     the slave, likewise, both are clocked on positive edge.


[2 Aug 19]

Note that with LTC2500-32 it is possible to read out full-rate data on
the A output by setting the decimation to 1.  It is not necessary to
use the B port to get the full-rate data for testing.

"distributed read" on the ADC does not require any special
configuration, you just only clock out a few bits at time (and refrain
from using the SPI during the interleaved conversions).

After getting my feet wet with the Verilog, and investigating clock
jitter a bit more, I am planning to (at least initially) generate all
of the ADC clocks in the FPGA. This is simpler in the wiring and may
be good enough.

If SNR disappoints I will want to try making the MCLK and capture_clk
be generated by external clock hardware like Si5351A (and be FPGA
inputs).  But generating a low noise Vref would be the first thing to
try.

The t_ACQ "spec" is the time available in the cycle for data transfer
(327 ns @ 1 MHz MCLK).  I guess we want something like a 10 MHz SPI
clock, since that would (almost?) allow us to transfer three bits
during t_ACQ (we need to transfer at least 2 at decimate 16).  We need
only 2x overclock to generate the SPI clock, so capture_clk would be
20 MHz.  Conversion time is 660 ns max, so we need to delay 700 ns, or
14 capture_clk cycles

SPI clock = capture_clk/2
MCLK = capture_clk/20
    MCLK is asserted for 14 capture_clk cycles (during the
    conversion), then we clear it and begin the data transfer.
    Divisor can be any larger (even) value to get more SPI clocks per
    conversion.  This would mean either a higher capture and SPI clock
    or a lower MCLK.

We are currently running ADC at 44 ksps.  To get this rate using the
onboard decimation (except for averaging) we need to use a
less-than-maximum conversion rate.  44 ksps * 16 = 704 kHz MCLK (1.42
us).  If MCLK runs at capture_clk/20, then at 704 ks/s capture_clk
would be 14.080 MHz, and SPI clock is 7.040 MHz

[Because only roughly 1/3 of the cycle is available for data transfer,
you need to use a 100 MHz SPI clock if you are going to transfer full
rate data at full converter speed (1 Ms/s). ]

Not entirely sure how to configure the ADC. It would be possible to
MUX the SPI control and use an entirely different (standard) SPI
controller. [I could also hook it to the processor SPI port for
configuration and general screwing around.]

But the conceptual overhead seems less by wedging it into the read
controller somehow.  However this is a distinct mode because we can't
start conversions during configuration (which aborts any configuration
in progress).  [Configuration can't be done during normal operation
unless SPI clock is high enough to transfer the 12 bits during the
acquisition window.]


It seems that you can get used working spectrum analyzers from around
$500 on ebay.  This would be good for evaluating clock integrity, but
they don't reach into the audio range for directly looking at our
analog signals.

The HP3585A seems fairly popular and does go down to 30 Hz, 3 Hz
resolution bandwidth, and up to 40 MHz.  This seems to be a late 70's
design.  Noise floor seems to be -100 dBFS (above 10 kHz) and dynamic
range 80 dB.  This would certainly be viable for evaluating driver
performance and investigating things like cross-coupling.  With a
scope you just can't see that stuff.  [Though a distortion analyzer
notch filter could get at this by notching out the carrier.  You'd
need two notches if there is both high and low carriers.]

Some quotes from the net re. spectrum analyzers for audio:
________________________________________________________________
"There is currently a newer generation instrument - an HP-3582A
You can go up with 3588A (20Hz to 150MHz) /3589A or 4195A/4395A (5Hz to 500MHz).
But the older models that doesn't have a digital IF has limited use
due to their lowest resolution bandwidth not being low enough."

If you are looking for old and affordable, nothing beats the HP3580A.

+1 on the hp 3580A.  This remains one of the better analog spectrum
analyzers for this frequency range.  Notable is 1 Hz RBW, adaptive
sweep, digitized display, tracking generator, balance diff input
option. It has a nice low noise floor and can be battery powered.

Why not use two different analyzers? A FFT analyzer (I use a HP 3582A,
hey, it was cheap) for up through the audio range and a 495P to go
higher."
________________________________________________________________
There are also some new USB FFT analyzers in this price range:
https://quantasylum.com/products/qa401-audio-analyzer

One issue with the heterodyne spectrum analysers at low
frequency (mainly narrow resolution, I think) is that they become
extremely slow, minutes to do a scan.  In contrast FFT analyzer can
work in real-time.  But the older FFT anlyzers do not have much bit
depth or dynamic range in comparison to a modern audio ADC.


[31 July 19]

Spent a while looking at clocking solutions again, and I guess reminded
myself why I didn't converge.

The Silicon Laboratories I2C programmable oscillators like Si549 seem
kind of nice because they are an integrated solution.  A little pricy,
but the all-in-one lends confidence that jitter performance could
actually be reached.  This is important because I have no capability
to measure ps jitter.  And the footprint is sane for DIY assembly.
(In comparison the clock generators using an external crystal have
eg. a QFN44.)  But the oscillators are not too easily available from
distributors and are definitely not on cheap breakout boards.

I looked again at the DDS boards on ebay, and these could be
reasonable as part of an evaluation setup, but you would need some
extra stuff to get a low-jitter clock from the sine output. This
really doesn't seem to make any sense as a part of the reference
design when as good or better performance can be easily acheived by a
clock synthesizer. Given that, any design effort to get this to work
is going to be basically throwaway.

The Si5351A is cheap super and popular, has lots of breakout boards
and driver code.  But this is an older part and its jitter performance
is not the best.  Since I already have this part and some software
support it probably makes sense to go with this.  The Si5351A has two
outputs so I can generate the FPGA clock and the ~1 MHz conversion
clock.  Can also test the effect of jitter by comparing integer
division with fractional.  If this makes any difference then the
jitter is having an effect.

I can also try a low jitter can oscillator of some suitable frequency
for comparison, using a hardware divider for the conversion clock.


In Xilinx UG949 page 64, <125 MHZ is "low frequency", while 125-250 is
"medium". re. fanout, low frequency design is noncritical.  At least
for immediate needs (ADC and perhaps DAC interface) we do not need any
clocks more than perhaps a small multiple (2x-4x) of the serial
interface clock.  For 32 bits at 44 ksps that is 1.4 MHz.  Since only
power-of-2 decimation is available with the fancy onboard decimation
filters, we are more or less at the DF=16 point, which is the highest
decimation that is going to give us bandwidth > 14 KHz.  This gives us
an output rate of 62.5 ksps at 1 MHz MCLK clock.  That's a 2 MHz SPI
bit clock minimum.

The LTC2500-32 is spec'd for a maximum serial clock rate of 100 MHz.
We probably want to use a much lower clock especially with the eval
board lash-up, but a clock < 32 MHz is going to require the
"distributed read" function where the serial output is spread over
multiple conversion cycles.  For the non-decimated output there is no
choice but to use >= 32 MHz (but accessing this output is going to
awkward anyway on the eval board).  The eval board has a jumper for
2.5V or 3.3V VCCIO.  The MicroZed carrier also supports both (see
below).

I am almost certainly going to use a separate digital serial input for
each ADC, routing the same clock to all and running acquisition
synchronized.  There are plenty of inputs, and we want them
synchronized.  It seems like a good idea to use a separate clock
output for each ADC to avoid branching the clock traces (for
transmission line integrity).  It should work to back-terminate the
clocks using controlled drive impedance.  With lots of ADCs (which
have to occupy a larger area) and 32 MHz clocks, synchronization might
start to get interesting.

The MicroZED I have is revision G.  See:
    http://zedboard.org/support/documentation/1519 
for much documentation.

The MicroZED has two Xlinix banks of general purpose FPGA I/O.  These
are all routed to the external connectors, routed to be usable as LVDS
pairs if necessary.  The -20 version has another bank which is
partially available.  Pretty sure we will have no shortage of pins.
See "MicroZed Carrier Design Guide".

MicroZed does not power the PL VCCIO banks. This is required by the
carrier card. This gives the carrier card the flexibility to control
the I/O bank voltages.  The breakout I have (MBCC-BK0) has header
block jumped for 1.8, 2.5 and 3.3V VCCIO.

Eight of the FPGA I/O can be connected as clock inputs (four MRCC and
four SRCC inputs). Which type of input you use does not matter if you
are driving a global clock (BUFG).

The only processor GPIO available are the 8 PMOD connections (which
are also routed to carrier board).  These are 3.3V.

Though the first step is just to acquire the one ADC any old way, it
seems like worth considering in the HDL code how to support multiple
inputs.

Driving the DAC is something that we may want even sooner, during
evaluation.  It would be quite possible to buffer an entire low-rate
repeat output block using FPGA block ram, since we only need something
like 32 KB (x3 for all three axis channels).  Not sure this is
actually a good idea though.  The write bandwidth becomes a small part
of the read bandwidth as input channels increase, and onboard
buffering will be more complex HDL and driver fu than a simple pipe
model.



Xilinx tools will infer use of DSP blocks, including multiple
precision.  It is recommended to use HDL templates to get the right
idiom.

FWIW, MicroZed is older than I thought, 2014. So there is risk of
becoming unavailable due to obsolescence.


[26 July 19]

re. high/low differences. I Looked back to [19 May 19], the last
discussion of cross-axis coupling. The cross coupling is 14 dB higher
at low carrier than at high, which is similar magnitude to the
difference in drive voltage between high and low. This could be a
cause of differences in high/low calibration.

Also, it occurred to me that thinking of the possible magnetic
coupling between source coils as mutual inductance gives an obvious
way to quantify. Measure how the inductance varies as the off-axis
coils are shorted.

See [5 Apr 19] re. effect of current sense on high vs. low and
possible mechanisms of drift.  I recall more recently having some
brain wave about drift and the effectiveness of the current sense
feedback.  (That is, current sense is much more effective at low
vs. high.)  But I can't find any notes.

Anyway, IIRC I was thinking about how I had adjusted the driver gain
trims so source coil currents are matched when the driver input volts
are the same.  But this results in unequal (apparent) source moments,
since the signal is inherently stronger in the higher channels
(actually due to inductive sensor highpass response).  So this is not
ideal from the perspective of SNR (We want to use more of our power
budget on the lower carriers.)

And that got me to thinking about what is setting the current and
setting the measured signal.  Well, there is perhaps some drift in the
DAC output amplitude or the driver gain, but to first order the coil
voltage is constant and the coil current is set by the coil impedance.

The key not-entirely-new observation, but something I keep forgetting,
is that (with lossless magnetics) the sensor voltage is proportional
to the source *voltage* not the current.  For example, wrt harmonic
content, the source current is a lot cleaner than the source/sensor
voltage.

In the case of the low carrier the coil inductive reactance is much
lower (~470 mOhm) so that the resistance becomes significant (see
[21-23 Jan 16]).  The resistance including wires is similar to this
inductive reactance.  But the observed sensor voltage is proportional
to the *inductive* component of the source drive.  The real situation
is in-between, but suppose that the (drifting) resistance entirely
dominated.  The current is then set by E/R, while the inductive
reactance is (perhaps) constant, so the source field drops as R
increases, and this is directly reflected by the measured current.

But at high carrier the effect of the resistance drift is negligible
wrt the much higher inductive reactance (14 Ohm at 10 kHz).  Ignoring
DAC/driver drift, any variation in high carrier current is going to be
due to a change in inductance.  But an increase in inductance does
*not* represent a loss of source strength; most likely if current is
held constant then the source strength will *increase*. Consider the
effect of eg. thermal expansion in loop area, or perhaps increase in
core permeability. These increase the inductance but also increase the
source coil effectiveness.

So it is clear why using the current reference at high carrier does
not work well. Possibly there is some way to save this by explicitly
measuring and cancelling the resistive component of the source
reactance.  But this would go better if we had a direct
measurement of the driver output voltage, or at least knew what the
phase was at the driver input and knew the driver frequency response.
 ==> One idea is that instead of adding another high performance ADC
     set, use passive dividers to measure the driver deviation from
     "pure gain".  This will be a much lower dynamic range signal.
     This is useful if we can assume all of the error is in the
     driver.  We could use a current differencing scheme to avoid
     the common-mode issues of an instrumentation amp. FWIW the
     present driver is inverting, so resistor currents can be
     differenced at an inverting input.


Calibrating the low rate separately worked a lot better than the "just
the gains" approach. I do kind of wonder if there might have been a
bug with implementation, but separate calibration is just fine.


I looked at the FPGA PLL resources a bit.  It seems like it can do the
basic job of generating a clock, but is not ideal. FWIW, the VCO
ranges 600-1200 MHz.  It does have settable bandwidth low medium high,
with low supposedly helping to de-jitter.  The *minimum* output is ~5
MHz, but of course it is always possible to add more dividers.

The general speed ballpark of most clock rate specs for eg. block ram
is 300 MHz at the lowest speed grade (which is presumably what we've
got).  So this is presumably the reasonable clock ballpark in general.

I'm thinking I should be looking at offboard clock generation options.
See [5 Dec 18].  I have discussion there that the clock jitter may or
may not matter depending on how you reckon the SNR bandwidth.  I guess
a big question is where spectrally does the effect of the clock jitter
noise appear?  Is it spread across the full bandwidth or concentrated
bang on the carrier?

FWIW, one weird thing about the clocking for the LTC2500-32 is that it
uses an onboard clock to drive the actual digital machinery.  The MCLK
is simply a conversion start trigger.  I'm not sure what effect this
has on the noise susceptability.


[25 July 19]

We have the low rate calibration sort of working, using just 5 gain
terms for low rate.  But it seems that if we let the other state move
then the source fixture transform ends going out of the mechanically
realistic zone.

I guess one thing to explore would be whether it would help to make the
low rate moment directions or positions independently variable.

I'm thinking that we should try doing low-rate calibration completely
seperately.  Partly just to see what happens, but also there is not
any actual reason I can think of to make the calibrations
inter-dependent.  It seemed physically reasonable that coil centers
and moment directions would be the same, and this approximation seems
to sort of work, but why force it?

Thoughts on why there might be differences other than gain between
high rate and low rate:
 -- Interactions between source coils may be frequency dependent
    (inductive cross-coupling or eddy current)
 -- Any sort of cross coupling, including in the driver, will cause
    something like a moment error (first order) and asymmetry (second
    order).
 -- Metallic interference.  In general the high and low rates are
    *expected* to be different.  If we calibrate seperately then we
    can see what poses give the biggest position mismatch.  It's
    harder to get at this during a joint calibration because the
    optimizer is compromising the conflicting constraints.
 -- Slower response or more noise at low rate?  Is one second long
    enough for pause and collect?
 

Looking at the DC2222A eval board for LTC2500-32. It seems like the
simplest setup is to use the QuikEval connector.  If you put +5 on pin
2 then the onboard FPGA bows out and you can supply MCLK conversion
clock and the SPI A port signals (and +9V).  This lets you get the
filtered data and configure the conversion.  The configuration input
data is not supplied during normal operation, you set the input low
and it will hold the configuration.

I notice that there is something interesting going on with the
clocking in the non-QuikEval mode.  The external clock BNC has a high
rate clock (40-70 MHz) which is directed to the FPGA, but there is a
flip-flop U3 which generates the MCLK pulse which is normally held low
by the FPGA but is clocked high by the (buffered) external click.  The
pulse is then presumably ended by the FPGA when appropriate.  This
must be to generate a clean clock untained by cruft from internal to
the FPGA (which is running on the high rate clock) and perhaps stuff
coupled from the DC890 USB controller.

There are notes about the importance of using a low noise external
clock, which makes sense with this setup.  But it would seem that you
could reduce the sensitivity by just doing some hardware division
rather than the latch thing.  If there is no input on "CLK IN" then
the onboard FPGA is going to be silent.  Maybe it doesn't even
configure itself without the clock? IDK

The microzed '010 that I have is not very big, the '020 version is
about 3x larger.  I was somewhat surprised that the Xillybus code
fills it about 25%.

I need to look into configuring the onboard MMCM clock generator which
has some fractional N synthesis capability.

[22 July 19]

Have Xillinux up on the micro zed.

Thinking about the ADC interface. Basic SPI function to xillybus FIFO
A should be simple (aside from getting back up to speed on Verilog).

But we need a clock. Ideally this is synched with the audio interface
(needs to be at least same nominal speed). Also the as ADC clock is
some multiple of the SPI rate to get the desired decimation. Need to
look at clock source on ADC board. At first any old clock will do. May
eventually want to integrate a low noise external clock generator.

Also we need a low noise reference before I can really evaluate
dynamic range.

I could hook to the eval board parallel port, but this may be more
trouble than it is worth. Can also cut traces, bodge wires, etc.

Easiest thing is to set up FPGA to just stream data with a fixed clock
config. Fancier is to add some control via device registers over
xillybus. I can use ncp from the xillybus device to send to labview.


[19 July 19]

Ideas for code cleanups in the calibration:
 -- Change the limits function to take an argument of a cell vector of
    strings for what parts of the state to fix at the initial values.
    Values such as 'so_moment_dir' for the off axis components of the
    source moments, 'se_pos' for the sensor coil positions.
 -- Put fixture poses into the calibration struct.  Save our
    calibration as a calibration struct rather than a state vector.
    This makes it easier to change the state vector representation.
 -- Add 5 low rate gain terms to calibration.  When enabled, we
    optimize across both high and low rate couplings simultaneously.
    

[18 July 19]

After working through it with Claudia for a while, we got the
calibration procedure to converge on sensible values with a very low
residue.  The only recent conceptual/implementation "bug" that had to
be solved was realizing that the XY sensor moment orientation in XY
plane could not be identified distinct from the sensor fixture Rz.  I
think the solution was to force the sensor fixture Rz to zero?  [There
had been coding bugs at first, OFC.]

The main thing that got us there was pursuing the procedure of pinning
the things that we knew with some confidence at the approximately
correct "by design" values until we could solve for the poorly known
source and sensor gain terms (moment magnitudes).  Then we added back
one thing at a time to the optimization.

Specifically, we froze everything but:
 -- On diagonal components of source and sensor moments.
 -- Source and sensor fixture.
Maybe we even had to freeze some fixture components at first?  IDK

Also, as a final refinement it was beneficial to add sensor fixture XY
components in addition to the sensor coil positions.  This absorbs the
XY misalignment of the sensor Z axis wrt the stage Z axis.

We had some confusion about the correct initial fixture poses. Because
we want to work in the +X source hemisphere, but mechanically the
stage is set up so that we want to work in the direction of the stage
+Y, we have the source and sensor set up with axes aligned (no nominal
rotation), but the stage is misaligned.  This requires two opposite
Rz rotations in the source and sensor fixture, -90 and +90.

It seems that the residues in pose space are in the low mm/few degree
range.  This is not terrible, but we would like to improve 3-10x.
 ==> RMS translational error 5.5mm, rotational 40 mrad.  Max errors
     something like 8/80.  That's not too bad for a first result,
     though we are hoping for a good bit better.  The results are
     basically the same on the calibration data (84 points) vs another
     collection over larger angular range and more translation
     positions (600 points).  The 600 point data was taking in the
     same fixturing, but has a superset of the points that was in the
     calibration data.

Since we are only exercising in translation and Rz it is likely that
things will not work as well with outside the calibration data and
with more general motion.


[12 July 19]

[From phone, this date or older, IDK]

Ok, letting the sensor xy moments move is pretty redundant with the
sensor fixture rotation. We are going to force source and sensor to
on-axis for now.

But back in the general case, there is not enough info to (in
particular) identify the orientation of eg. the X moment direction vs
the rotational symmetry of Z independent of the sensor fixture
Rz. Maybe just fix the sensor fixture Rz at the 90 degree value? I'm


Major problem areas (not exclusive):
 -- problem not posed right, error in forward kinematics or objective
 function. Reduced problem would *not* help. 
 -- specific optimization issue (doing too much at once, centering and
 scaling). Reduced problem would help. 
 -- not the right data to identify. Reduced problem would help.

Do we have reasonable initial state?
Fixture transforms and gains

Run a long time?

For display, normalize residue by distance factor

Go back to reduced optimization
Can we calibrate gains first?

Try big data
Think about whether we should need more varied data to calibrate

Look at the code, debug
  -- look at forward kinematics in simple cases
  -- look at objective residue in simple cases

Centering and scaling? 

Try forcing:
 -- source moments to orthogonal.
 -- sensor positions to zero
 -- sensor moments to orthogonal
 -- source positions to initial

So we are solving only for fixture poses and gains.

Currently the source positions and source fixture translation are whack.

Are the source gains the same? If we are forcing same current, then
the voltage increases with frequency. IIRC the driver gain trims are
set so equal DAC volts gives equal current. Then XY gains would be
lower than Z.

Probably makes most sense to have source coil voltages equal, not
currents, since this equalizes SNR. More of our power budget should go
to the lower high carriers. Higher current happens automatically for
high carriers because reactance is lower. This is mainly a high
carrier thing, since low carriers are closer spaced. Low carrier
strength is set by resistance, I guess strength is proportional to
inductive component? But current does not really increase, so low
carrier drive volts would ideally be increased at lower low carriers.

Hmmm, maybe that explains why the current reference is only useful at
low carrier. Low carrier amplitude change mainly caused by resistance
drift. I knew this was why uncorrected drift was bigger at low
carrier. But high carrier current change may be due to inductance
drift. Eg. expansion causes loop area to increase, increasing moment
and inductance. But current reduces so we correct in the wrong
direction. Current is in neither case directly proportional to
amplitude. Presumably you could do the right thing by looking at the
phase change of the current. We can't accurately measure the source
impedance because we don't measure the output voltage (and don't know
the absolute phase at all because of buffering). But we could model
second-order effects by direct offline measurement of eg. source
impedance, and back out the dominant drift term. (Probably due to
source itself rather than driver, etc.)


To-do
 -- sensor coil wire
 -- source cable. pins 1-3 6-7 5-2

Source coil positions
5-39 midpoint xy
z 3-37

Kinematics:
FO = Fixture sOurce
ST = stage
FE = Fixture sEnsor

Diagram: FO -> ST -> FE

Pose of sensor wrt source:
    P = FO

Map sensor translation or moment vectors  to source coordinates:
 


May want to take a layer off the source coils so that we can drive
them harder, since it is running so cool. That will create error wrt
the desired shape tho.

Thickness of stack of FRP beam and cement block is 5.5", for tie-down
threaded rods add room for nuts.

Fixture transform calibration, misalignment of both source and sensor wrt stage.
Kinematics for Claudia
Source and sensor pose origin distinct from Z coil pose.

0.6" depth is about right for source coil countersink. The z coil
ended up about 1.6 mm too deep at 0.662.

One source coil arrangement that should cancel coupling is taking the
concentric arrangement and expanding them along a *single* line thru
their centers. Cancellation of coupling is good because each coil of
each pair are pointing axes of symmetry at each other. This is also a
more compact arrangement than the cube sides.  More of a nuisance to
machine tho.  I'm guessing it would work to use any line, tho I was
thinking along one of the orthogonal axes.

Calibration model:
 -- Source and sensor gains, 5DOF. One gain is 1.
 -- 6*5 for 6 coils position and orientation. 5 because no sensitivity to Rz axial rotation. 

How do we represent 2DOF rotation? XY components of rotation from
nominal. Eg. dot product of coil axis with x and y axes of nominal
frame.

Except that we lose ??? 6DOF because of arbitrary definition of the
reference frame? Simplest to force z coil to null pose in both source
and sensor. This drops 10 or 12 DOF. Source and sensor do have a
definite relative 3DOF rotation, despite each individual coil having
Rz symmetry. This gets into the fixture transform.

Do we need more than 6DOF of fixture transform because both source and
sensor have orientation? We need at least 3DOF more because both
source and sensor will be misaligned with stage.

Do we care to define a nominal source and sensor pose distinct from
the z coil? How does this interact with calibration? We would need
somehow to calibrate the difference of z coil from nominal, and to do
this we would need to make moves that are well defined wrt the source
and sensor nominal pose (eg. by direct measurement). This could be a
separate process, or could be expressed by not having the fixture
transform fully arbitrary.

We can precisely rotationally align source and sensor fixtures with
stage axes using indicator and stage moves. Z position can be
established using level across measuring points.

We want source and sensor fixtures (or fixtures for the fixtures) to
have accessible alignment surfaces.  Note that AFAICT we can extract
the distance scale from precise stage moves, it does not depend on
precisely establishing the translation at the null stage pose.  But we
still need the precise measurement to establish the Rz coil
vs. nominal. It may be easier to do this by finding the center of
rotation. I did this with ASAP, tho did not bother to consider this a
calibration output.

I guess that it makes much sense at least initially to skip giving
source and sensor a mechanically referenced pose. I never bothered to
do that with ASAP. FWIW, it seems that commercial trackers do not have
an accessible precise mechanical reference either. They will
presumably tell you the nominal center WRT the housing, but I doubt it
is calibrated. (Maybe the source is, at least roughly, by the
calibration fixture, if they are doing per unit calibration.)

With ASAP I used alignment procedures in the stage setup to align the
stage with the ASAP coordinates. This requires 6DOF in the stage. IDK
if/how this affected the calibration state. It was useful for
evaluation and also to align the test pattern with the shape of the
workspace.
 


For plate mount stiffener project, the 8mm 18-8 hardware seems fine,
no metal effect until 10-11", and surface plate means these are ~13"
away or more.  Use 4" pieces of the 3 x 1/2" G10 epoxied across width
of 4" square tube. This transition more than doubles area WRT 2.5"
washer, and my theory is the main thing is stiffening the tube wall
locally and transferring load to the vertical sides, which give most
of the vertical stiffness.

FWIW, as in earlier experiments, the 18-8 hardware only affected the high carrier. 

53x0.18mm 1.88mm od
15 turns by 10 layers?
I'm getting 19.3 for 10 turns and 1.64mm wound height.
That would give more like 12 layers and maybe 15 turns is a little cramped.

In line with source or the sensor stepper seems ok at 10", but we need
16" or more off axis in mid range. 42 mm W x 38 mm L.  We really
should not see the midrange configuration, so don't need that
much. Measured from motor edge to source center.

Later:
 -- procedure to restart after e-stop?
 -- EMI survey
 -- align stage
 -- bolt/mortar platform together

Source mounting fixture and interface with bobbins. May still make
sense to machine. That will give precise indexing. Taper seems good
for interface to printed part. Socket in fixture with tapped hole at
bottom. Print male taper as part of bobbin, with hole in middle for
bolt and for fixture during winding.

If you change the motion server deployment you may need to change
network variable locations in motion_variables.vi and in the data
bindings of the front panel indicators on motion_ui and
motion_server. You need datasocket installed and have to pass several
ni servers thru the firewall.

Claudia
 -- get up to speed with eg. Ansys 3D magnetic simulation.
 -- source design and fab, sensor design and fab

Electronics for Claudia
D/A eval w/raspberry pi
R axes interfacing: stepper and encoder,  limit

Needed for calibration work:
Coils:
 -- sensor
 -- source

Documentation and packaging:
 -- Github organization account 
 -- Start putting stuff in project wiki

Test if there is any EMI variation. Survey using phone+preamp setup

What is the basic goal of calibration work?
 -- get accurate pose solutions across workspace. Sensor rotation a special concern.
 -- look at metal interference
 -- usable realtime implementation

Simply getting sub-mm accuracy would be a good accomplishment, even
just using matlab optimization. Not anything new, but a step in the
right direction. The other thrust would be the real-time, UKF,
etc. But IMO a mistake to optimize until we know what we are doing.

Can probably calibrate by just waving around the sensor, with no
ground truth, if we put two sensors on a known length (printed) bar
fixture and calibrate source and both sensors simultaneously. This
will make it much easier for users to calibrate for themselves, since
they don't need a stage and don't have to do manual point-by-point
calibration using a positioning fixture. The bar length is our one
known, which sets the distance scale, but rigidity adds other useful
constraints because both sensors are undergoing the same rotation.

Basic model is every coil has a location and a magnetic moment
(orientation and gain). (Except for forcing one of the gains to 1, as
is done now.) Maybe it would be necessary or numerically helpful to
constrain the model a bit, eg. allow axis misalignment, but constrain
axes to intersect (no translational freedom). IDK, we will see, maybe
we will be able to solve even configurations far different from the
default initial value.

It will be a problem if you get too close so that the dipole
approximation is violated, but we could clip that data. We can also
use greater ranges than usual since we do not need high
speed/resolution during calibration. This will allow a longer (more
accurate length) bar in approximately radial poses of the bar without
the near end getting too close.

So the unknown pose of the bar at each calibration point becomes part
of the state we are solving for, 6DOF at each point. But the sensor
readings get us at least 11DOF (2*6 - 1 for fixed bar length), so we
do gain info. We will likely want to subsample the pose quite a lot,
only significantly different poses (at least a few mm/degrees) to
reduce the problem size.

It will be a big help if we have the rough as-designed geometry
because we then have approximate pose to give feedback on the points
being taken, too close, etc.  Also we can initialize the solution for
the per-point bar pose. We can also tell by the apparent bar length if
something is really wrong, bar wrong length, initial design posed
wrong, wrong hemisphere, etc.

I guess we would mainly use the high rate for the optimization, since
the low rate is much slower (would lag unless motion is very slow),
and also the geometry should be exactly the same (tho hi/low gain
ratio may differ from ideal).

For calibration of gain switching it should be useful to make matched
gain switches on source and sensor that should cancel. Then we measure
only the error and don't blow out the dynamic range.

We want at least some switching on the Rx side. The drive and the
current sense will also lose SNR as we back off on the drive, but we
can tolerate lower bandwidth on the sense channel. Possibly we will
want something fancy on the driver to reduce output, TBD.

3D magnetic simulation, tho FEMM is better than nothing. What is the magnetics good for?
 -- optimizing coils for size/performance
 -- insight into causes for pose error,  with and without metal interference.
 -- possible tweaks to coils for pose error
 -- possibly generate correction tables
FEMM is ok for basic sizing, source loss, etc. Need 3D for the rest.

Spin off tasks: mostly related to physics, pose solution,
software. There is at least one masters there. The interference
rejection is the most researchy part, probably beyond scope of masters
project. But not clear this is a good fit for Arpita either.
 -- sub mm pose accuracy in matlab
 -- real time implementation, UKF, ANC
 -- magnetics design and simulation (Ralph?)
 -- long shot, but maybe RT Linux/FPGA?


I would feel that we have done a decent job if we just deliver a good
reference design, without any metal interference stuff beyond the dual
path high/low, and with whatever static accuracy we can get. For
micron mainly what we need is the speed/resolution.

Squeezing out the last bit of resolution is not top priority
either. For example, put off driver noise problem. Prototype of input
section is first, with ADC, preamp, gain switching.


EE-cad toolchain. The electronic design is something I will need to do.

ADC evaluation and getting up to speed on the FPGA thing. Debug cable chipscope

Packaging:
Microphone cable for sensors? Flex transition to cable.

Maybe 1U box with single flat board for input side? But drivers are
bigger, unless I go to eg. the custom digital driver. Silly to start
with a too-small box, anyway.

What do I want to do?
 -- deliver usable, customizable reference design. At much as
    possible, via continuous update of repo. 
 -- the high SNR thing, signal conditioning, board, packaging, custom digital in driver.
 -- EMI suppression, integrated with actuators


[3 July 19]

OK, I got the source working again, at least for now.  I added a
MOSFET clamp arrangement on the B+ rail, see notebook 1, page 95.  But
the problem seems to have emerged due to a drift/degradation in the YZ
driver, and if this continues the startup imbalance may become
unreasonable.  One possible fix to explore would be to shotgun replace
all of the small electrolytics on the board (see below).  It is hard
for me to imagine any other component that could have the sort of
multi-day time response I am seeing.

You can see the LED on the clamp board through the rear panel, on the
right rear side when looking through rear panel.  This is right under
the AC power entry module.  The light should not be on normally, but
may be on for some minutes after startup.  If the blue lights on
drivers are cycling on and off and the red light is *not* on then
likely the clamp has blown out.  What we are more likely to see if the
current keeps going up is that the clamp is going to start getting
super hot (though this might cause the MOSFET to blow).

Also, the TL431 is in a socket, and I forgot to mechanically anchor
this in any way, so it could vibrate out due to bumps during transport
or some such.


I explored adding an output RC snubber to the Y output filter, and
this did damp the burst of 10 kHz I was seeing on the startup burping,
but this had no effect I could see on the bus pumping.  So I did not
change the output filter.
 ==> As noted earlier, possibly this resonance could affect the driver
     output noise, but increase or decrease? IDK

The pumping current seems to be roughly proportional to the driver
clock rate on the Y channel, but there is no threshold where the
current goes away.  I reduced the no-input clock rate on the Y channel
from ~500 kHz to ~400 kHz.  Recall that the clock under load is
significantly lower, see [5 Oct 15].

FWIW, the unused channel 2 on the X driver board still has the onboard
Zobel network.  Also, the bus pumping is clearly due to the YZ board
because the X board is in shutdown until later.  When the X board
comes on this seems to help the B+ to settle down.

I suspect that the pumping is somewhat worse when we have no output
drive because there is very little real supply current being drawn.

The B+ overvoltage is *strongly* correlated with how long the driver
has been powered off, being greater on the several-days timescale than
at an hour or a day.  This can't be a thermal effect, so I am still
very much a fan of the idea that it is electrolytic capacitor
behavior, which dies down when the dilectric reforms. These boards of
course have cheap generic caps, a bunch of small ones at 10uf or 22uf,
which are especially likely to dry up.

I looked a bit at the input section supplies, but did not see any
asymmetry there, and this would not make much sense as an effect of
leakage because the Zener impedance is pretty low in comparison.  But
it could well be something like high ESR in one of the output supply
capacitors, either low side or high-side bootstrap. This could perhaps
create an asymmetry that might cause a DC offset.  In order for this
mechanism to work the output filter would have to respond in a
different way than the overall feedback.  It would seem like that
would have to involve nonlinearity, IDK.

Though I did see once that I was getting the pumping without the
output offset on the Y channel, the general correlation is
super-suspicious.


I got the tracker set up in the lab again, and collected another run
of calibration data.  I was puzzled at first about why the YZ
couplings were negative, and thought this was a sign-flip problem, but
I couldn't get it to go away.  Then the comment in the
phase_correct_coupling VI reminded me that the coupling is negative on
those two channels because the sensor is to the side of the source
(parallel) rather than in front.

Also I had unintentionally set up the the source and sensor to be
aligned with the stage XY coordinates, which meant that the sensor
offset was +Y rather than +X.  This certainly explains why the
coupling in the home pose was totally wrong, and going in and out of
the +X hemisphere would certainly have keep the optimization from
working either.  How much this helps is TBD.

This does mean that we need a -90 degree Rz rotation in the source
fixture pose and a +90 Rz in the sensor fixture (to absorb the stage
90 degree misalignment).  Also at stage null pose the sensor is maybe
30m -Z wrt the Z coil center.



[27 June 19]

 ==> Try load on unused output?
 
For unclear reasons the driver has developed a problem.  Symptom is
that it is unstable and/or has a startup problem.  This is
particularly triggered by the Y channel.  To see the problem both the
load (source) needs present and also there has to be a low input
impedance to the driver (even a short).  This makes sense as an
interaction with the output impedance and the driver gain (gain is
unity when input is open).

I had seen this behavior before, but it had always gone away when I
restarted some part of the software, so I had assumed this was a
software thing.  But it can now be reproduced with no actual input at
all. 

The actual observed pulse mode operation is mediated by an overvoltage
on the B+ rail, going up from ~75 V to 90 V.  Also, during the on
periods, there seems to be a ~10 kHz component (which is the resonant
frequency of the Y channel).  I think I had been flirting with this
issue before because I had problems with crowbar tripping on the Kepco
ATE supply which I fixed by adding a diode to prevent sourcing current
into the supply.  Possibly the need to increase the OVP limit on
[19 Nov 15] was also related, though this was well before the last
reworking of the output [21-23 Jan 16], which I think may be when this
behavior started.

There also seems to be some startup stuttering issue, even with the Y
channel load disconnected.  The B+ shoots up and we do the OVP cycle
for a while before it settles down.  Interestingly, this is visible in
the front panel power light, which is driven from the aux supply.  So
it seems that on startup the supply itself is having some current
limit issues or something.  The amp modules may be drawing a spike at
startup.  This may be exacerbated by bad current limit resistor values
noted on [19 Nov 15].  I did not say I fixed those values, definitely
didn't add any thermal coupling to the overheat thermistor.

So likely the other channels are somewhat marginal also.  I was
wondering if my decision to remove the onboard Zobel snubber had been
a bad idea, but going back to the simulation it really does not seem
to be doing much.  If there was an error, it was giving the output a
fairly high Q resonance (20-40 dB, not sure how to reckon it).

One thing that is noticeable about the Y channel is that it has a much
higher output offset voltage, something like 0.2V rather than 0.002V.

I notice that I put in the Z input coupling capacitor "backwards" (-
toward input).  This is spec'd as a polar electrolytic even in the ref
design.  Is this a common audio practice?  In theory you ought to use
a nonpolar cap or two back-to-back.  Maybe it doesn't matter that much
because the offset voltage will be small, and the other end of the
line may also terminate in a cap?  That is, if you operate at a
definite offset from ground, then this tells you which way to put the
cap, assuming input is centered on ground.  But if you operate at
ground, then the voltage across the cap is nil, and even a polar
electrolytic can tolerate 100 mv (?) of reverse voltage.

FWIW, the Y channel driver was the one that I had added in a
connection for external clock synch.  I ripped this out, though it is
unlikely that this unconnected thingie was doing anything bad.

I'm thinking about putting back a Zobel snubber, though now located
after the outboard second stage filter.  This is already present on
the Z axis, but I thought I didn't need it on XY.

Also, I'm going to check out the effect of adding a zener clamp on the
supply.  We don't want to regularly be dumping much current this way,
but it seems that this might just be a transient startup thing, that
the output is unstable as the supplies come up, and this leads to bus
pumping which cause OVP.
 ==> Does not really work.  The amount of current that needs to be
     sunk on the high side when there is this fail seems to be
     considerable.  4x 15.KE20 does not hold down the voltage, current
     spikes to 1A (or maybe more) then goes down to 0.3A.  The drop
     may be just due to Vz increasing with temperature.

The behavior of stutter-start even with Y disconnected does not seem
to be entirely reproducible.  I tried to see if the clamp helped in
that case, but was not getting the effect even without.  Also, I was
not seeing the flicker of the power LED even when it was happening.

The output offset on Y channel is definitely odd.  With the load
disconnected, I was seeing as high as 0.45V right after power up,
gradually ramping down.  Input short or open did not seem to make much
difference.  The offset spec for the chip is 15 mV max, bias current
40 nA.  However after being on for some while (with load
disconnected), it is down to 60 mV, which is more reasonable.  After
short power cycle it comes right back to there.  Measuring the offset
as DC at the output terminals is a bit dodgy because there is around
2V PP of 500 kHz ripple.

There is definitely a thing where if I leave it off for some minutes
it is more likely to go into the overvoltage thing.  Electrolytics
(maybe leakage) or thermal are the only plausible ways to get this
slow an effect.  Small electrolytics on the driver boards could easily
have gone bad.

Maybe possibly having a 175 ohm load on the B- makes things worse?
FWIW, this load on B+ is not enough to drag down the voltage, but
maybe possibly helps.  The B+/B- busses take many minutes to discharge
without this.

The B+ overvoltage does not seem to be super-strongly related to the
10 kHz burst on the Y axis.  That is, maybe possibly the snubber
reduces this thing, but we also get the overvoltage even once the
burping is stopped (the voltage settles to where the YZ board stays
on).  I am using a 300 nF 20 or 50 ohm snubber.  Spice says that 50
Ohm gives most damping.
 ==> The more I mess with switching the Y snubber in and out, the less
     I think it is making any difference in the overvoltage.  For one
     thing, the overvoltage can persist after the burping has stopped
     and the Y output settles down.  I think that maybe the
     overvoltage itself is related to the bus pumping, perhaps due to
     the asymmetrical supply voltage.

I am pretty sure the clamp is helping, but sometimes we are definitely
asking it to do too much, settling at 100 mA or more for some time
(sometimes).  I am thinking of adding a MOSFET clamp of some sort.

Also, the high offset on Y seems unrelated to the B+ bus pumping,
since I have seen the high voltage even when the offset was low.  But
since they both happen at power-on they are correlated.

Note the YZ board has a slightly higher OVP voltage.  It settles at
just above 90V, which is enough to cause the X board to shut down.  In
fact, it is *much* lower.  Maybe I didn't mod the OVP on that board?
There seems to be this thing where the overvoltage gradually drops
until the YZ come on steady, then eventually the X, at which point the
overvoltage rapidly disappears.


I notice that the board gain trimmers are 10K, whereas the input
resistor to the inverting node is only 3K.  So as we move the trim
away from full scale we are reducing the CL gain as well as getting
the voltage divider effect. eg. at mid-scale the output will be around
1/4 of full scale(?).  I suppose I probably realized this at the time
[1 Jan 16].  This does make the adjustment range techier than it needs
to be, might have made more sense to just use it as a series
resistance, which would give a non-zero lower end and linear
adjustment.



[24 June 19]

I got the labview calibration sequencer working, and have some data
for Claudia to work with. Currently this is with new source and old
sensor, because I don't have the sensor wired yet.

I guess since I have pretty much finished the stage, source and sensor
work, it's time to get back to the hardware design effort, ADC
evaluation, etc.

Toying with the idea that some sort of high-Q filters might help with
the evaluation effort, especially wrt the problem that I don't have
good ways to the get better dynamic range than that of the signal
chain I am trying to evaluate.

What, specifically would I like to know better?
 -- Where does the rise in noise floor in presence of signal come
    from? This doesn't seem to have to do with the driver because it
    is the same in loopback tests. Most likely ADC reference noise, or
    some ADC quirk. Clock jitter can also cause this. So far only seen
    with PCM4222, which is kind of out of the running.
 -- What are the characteristics of driver limits, especially noise?
    This will likely become limiting as ADC range increases.

More broadly, it would be nice to be able to individually evaluate
DAC, driver, preamp, ADC. This is probably not actually worthwhile
because it would require in effect building test instruments.

But eg. the distortion analyzer approach is not too intensive, and
could be informative.  But we would need at least two notches to get
at IMD effects.  Driver distortion itself is not a huge concern so
far, because existing tones are not problematic, ANC would mostly get
rid of them, and we can also pre-distort. But driver performance is
not yet an issue, so getting a better ADC is the top priority.

I guess what that comes down to is how to evaluate the LTC2500.  Options
options I have considered:
 1] Custom code for teensy
 2] Zedboard
 3] rPi

Really seems that the Zedboard is the way to go, because I need to
advance in that direction anyway.  Somewhat more daunting getting up
to speed, though.  Once I get the data in, I can stream it out to
Labview using basically the same TCP socket interface I used with rPi.

re. preamp design, one issue is that depending on what sensor we use
there is a pretty wide range of gains needed and of sensor impedance.

FWIW, AD8656 CMOS amp spec is 2.7 nV/rtHz typ at 10 kHz

LT6234 bipolar is 1.9 nV/0.43 pA rtHz, promising, not too expensive
and available in 1, 2, 4 packaging. SO package for duals, also.  This
part is for 10V total supply (or less). RRO

The OPA1632 fully differential is 1.3 nV/0.4 pA, so is also in the
right ballpark.  I guess I got some of those already, but see [13 Oct
18], with the low Z input it is not clear where we could use it.

I would suppose we are better off with fully differential all the way
through from the preamp to the ADC, though going to a single-ended
inter-stage would likely simplify gain switching.  We do presumably
need at least two active stages, the input (with significant, perhaps
switchable gain), an attenuator, then an ADC buffer/gain stage.

I'd say that for the first stage to swamp downstream noise, we want at
least 5x gain in the input stage. eg. the rule that a noise needs to
be at least 3x smaller to not contribute much to total (1/9 because of
squaring, ~10x). Not building more gain than necessary into the first
stage would allow us to not gain-switch (or bypass) it, as long as it
doesn't overload under the strongest signal.  Then we can follow it
with an attenuator and more gain.  For example, we could have:
    5x gain
    1 1/2 1/5 1/10 attenuator
    4x gain

This would give us gains from 2 to 20.

Building less gain into the first stage does mean we need to be
more careful about noise in the later stages.  And keep the attenuator
resistances down.

See [11 Oct 18] for gain switching.  It would be possible to put
switchable attenuation directly at the coil input as long as we can
remove the loading at full gain.  But we also don't want much coil
loading because this increases cross-couping between the sensor coils.

For some reason I find myself feeling that it is not super-important
to maintain the rejection we would get from a fully balanced signal
path.  One argument is that we are not too worried about general EMI
pickup in the preamp because the sensor itself is a huge EMI pickup
already.  But cross-coupling can be a concern, either between sensor
channels or from source circuits into the preamp.  This is a bigger
issue if the source driver is in the same box, which we want to do
eventually, but will not initially.


From phone:
 
Sensor coil with ~1600 turns is 23 mH Q=0.8 @ 1 kHz. So inductive
reactance ranges from 1-2 kOhm across high carriers. This pushes
toward an LNA with lower current noise, possibly JFET. Resistance is
~180 Ohm. JFET would pretty much eliminate current noise, at least at
high carrier. Johnson noise is ~1.8 nV/rtHz, which is still quite low
En, somewhat lower than the best JFET opamps (eg OPA828 4 nV). Noise
performance at ~300 Hz low carrier is also a concern, where the
inductive reactance 43 Ohm is negligible WRT DCR. IIRC JFETs have 1/f
current noise? But this is starting from a low level, and the
inductive effect is dropping in the LF.

From noise resistance perspective, the 2 kOhm reactance would be 5.6
nV/rtHz. So that is still pretty low noise even for a bipolar. It
would seem that we are more likely to explore the lower inductance
region, so it would probably be best to pick bipolar with noise
resistance in the ~1 kOhm region, En ~2 nv/rtHz.  This will be as good
or better than JFET with the current sensor, and will be better with
lower inductance (smaller) coils.

This coil is not optimized from a noise perspective. For the JFET,
resistance is unnecessarily low, more turns/thinner wire would be
better. I guess it's not bad for the right bipolar, but it seems odd
having most of the noise be from current noise. And SNR could still be
improved by more turns/thinner wire, tho that is a tighter box for the
bipolar.


Coil SRF should be in the 100's of kHz (with eg. 20 pF), so not a concern.

1948 turns, 27.3 mH, RDC 226 Ohm.  This is too much wire for the
bobbin.

 ==> after some frobbing of dials the inductance has dropped somewhat.
     Now seeing 18.9 mH and 24.3 mH. Not getting such a good null, maybe
     hum pickup.  

On inductive Q, RAC = 221 Ohm, Q 0.732. Other coil RAC = 182, Q =
0.636. Converting back to inductance I get 18.4 mH. The null is much
better in this mode.

These bobbins are really not the right shape.

Three coils with 1700 turns made on fixed settings on the winder.
19.89 mH Q 0.66
19.54 mH
19.48 mH

These all have some taper toward the top, visually apparent, but
measures at about 0.5 mm. You'd think that a radial error of eg. 250
um would convert directly into that distance error, but I wonder if
the nonlinearity and averaging over the coil help to wash this
out. That is, it seems that distant sources always resemble a dipole.


One obvious next step after dipole would be to include higher order
multipole moments, eg. quadrupole. In a standard polar multipole model
the higher orders drop off faster with distance, another way to say
that a distant source looks like a dipole.

I wonder tho if a big part of the attraction of multipole moments is
analytic tractability, which is not a big concern for us. What if we
just represent the coils as the sum of two or three dipoles which have
different locations (and orientation and scale). This seems like a
much more efficient way to generate the kinds of asymmetrical field
deviations that we expect.

Completely independent dipoles may be too many degrees of freedom,
eg. if the dipoles are in the same position and orientation, then
their relative scale cannot be uniquely determined. Perhaps it would
work to fix one as being orthogonal to the other.

Note that a coil or dipole has only 5 spatial DOF (6 with moment
magnitude). Once another dipole is introduced the Rz symmetry is
broken.

Two orthogonal concentric dipoles are a standard quadrupole. The
second dipole has one angular DOF in addition to its magnitude. But if
we break the concentric constraint then there is axial Z and radial r
translation. If we keep the orthogonal direction constraint then I
think that is all, we don't need a "direction of translation"
angle. (?) Bear in mind that it is only when we add the second dipole
direction that the thing has any Rz orientation at all. The two axes
do not in general intersect (only for r=0).  That doesn't seem right,
tho. The axes can intersect when r>0 if the direction of translation
is along the second dipole axis.

For optimization we want non redundant DOF. This is something the
usual multipole expansions guarantee.

Maybe it would be better if we expressed the DOF as the second dipole
angle, plus a 3-vector Euclidean translation.


I was certainly not supposing that the source coils had to be accurate
to better than eg 100 um. The source wire is 1.6 mm, so symmetry below
this scale is difficult. I am hoping we can get sub mm accuracy using
just the dipole model. Will be interesting to see how well good the
uncalibrated pose is using just nominal design geometry.

2 others made under more various conditions, but all nominally 1700 turns:
19.35 mH 188.0
19.45 mH RDC 184.6, 186.8 on GR 1608-A

Based on pretty good inductance matching I suspect the turns are
indeed the same. DCR is closely enough matched that it is a bit hard
to evaluate, because of warm-up, connections, etc. Will try Fluke 8801
again once it warms up.

184.142
182.535

The three "production" ones:
184.23
184.308
184.593

 -- 3x6 platform
 -- Rx Ry axes
 -- meet with w Vicki and see how much time she has, what to work on.
 -- set up old slew for motion control, ssd, memory, windows 10. Card,
    buy interface, drivers, cable 

For sensor coil, the width 0.25" is possible now that I tweaked
winder, but at finest spacing we only get 20 turns layer, not the
ideal 72. So each "layer" gets 3.6 passes.  Maybe go for 18 turns/pass
which gives an even 4 passes/"layer"?

Also there are position jumps in the feed slider related to switching
which are significant at this scale, eg. 0.020", or ~6 wire widths.
 ==> improved a great deal by using cable ties to add preload to cross
     feed bearing bar and adding grease/super lube. Carriage was rocking. 


[13 June 19]

Motion/calibration tasks:
 -- What project for new code?  ILEMT I guess.
    generalize/modify motion project as needed, rather than copy
 -- Locate relevant old code in:
      micron asap_calibration.vi
	Copy and hook in ILEMT acquisition
      asap calibration matlab code (read Euler pose vectors)
      anything from micron asap_stage_calibration?


[10 June 19]

I haven't quite finished the setup on the stage, but returning to
thinking about how the calibration Matlab code is going to work.

Here's the plan:

We need to represent the position, orientation, and gain (magnetic moment
magnitude) of all 6 coils (3 source and 3 sensor).  As discussed
below, some redundant degrees of freedom are forced to 0 or 1, but it
is simpler to write the forward kinematics to assume that all of these
parameters are fully arbitrary.  However the redundant values are not
represented in the optimization state vector.

 -- The position of a coil is the XYZ offset w.r.t. the source or sensor
    coordinates.  (3 elements)
 -- The orientation and gain of the coil is represented by the
    magnetic moment vector.  The vector direction is the orientation
    of the coil w.r.t. the source or sensor coordinates, and the
    magnitude is the coil gain.  (3 elements)

There are two additional transforms which get estimated as part of the
calibration:
 -- The source fixture transform: XYZ + rotation vector Rx Ry Rz (6
    elements)
 -- The sensor fixture transform, rotation only: Rx Ry Rz (3 elements)

Removal of redundant degrees of freedom:

We allow the Z source/sensor coils to define the source/sensor
coordinates.  So the transform corresponding to the Z coil pose is
null.  The source Z gain is forced to 1, which means that the source Z
position/orientation/gain does not need to be represented at all.  The
sensor Z position/orientation is forced to null, but the sensor Z gain
is not 1.  For the sensor Z we need only the Z gain.  For all other
coils we have the 6 elements above (position and magnetic moment).

So in all, we have:
  Source X: 3 position + 3 moment = 6
  Source Y: 3 position + 3 moment = 6
  Source Z: 0 (no data needed)
  Sensor X: 3 position + 3 moment = 6
  Sensor Y: 3 position + 3 moment = 6
  Sensor Z: 1 (Z gain)
  Source fixture transform: 3 position + 3 rotation = 6
  Sensor fixture transform: 3 rotation

This gives 34 elements in the optimization state.  Let's put these in
in the state in this above order.

I decided that we do need a two-part fixture transform because neither
the source nor sensor will be precisely aligned with the stage.
However only the source fixture transform has a translation part.
(Assigning the translation part to the source is an arbitrary
decision.)

Using the magnetic moment to represent the coil orientation will
require a bit more modification of the existing code (fk2.m), but it
should be pretty straightforward, and should if anything simplify the
code, because the magnetic moment is what is actually used in the
dipole model.


[31 May 19]

I got the source coils wound and wired up.  I was aiming for 7 layers
by 17 turns (119), but really only got 16 turns layer, or 112
nominal.  It seemed I was actually running 1-2 turns short of this
total.  Anyway, measured at 1 kHz, the coils are:
    #1 226 uH Q=9
    #2 231 uH Q=9
    #3 219 uH Q=8.7,  2 splices

Coil #3 (Z) has two splices.  I varnish impregnated 1-2, but am
tentatively planning to rewind #3 because the splices are big ugly
things that mess up the layering and are solid with solder, so
non-Litz.  This is probably reflected in the lower Q.  I ordered more
Litz wire.

Anyway, the inductance is right what I was aiming at.

The [17 Apr 19] entry is kind of confused, but with r=24mm I was
getting 272 uH, Q=9.8 with 7 layers of 17 turns (121 turns actual).

I got the new source coils fired up, and roughly laid out in the
linear configuration.  The cross-axis signal in the current sense does
not seem to be much worse.

They are running noticeably cooler and (I think) quieter.  Also the
drift is maybe less?  But the operating currents are a bit low (higher
inductance I guess), and there is basically no room to increase the
drive in labview.  I would have to mess with the gain pots I added to
the driver boards.  As it is, the currents are not much lower, but the
output is significantly lower.  (Not a surprise, according to
simulations).  Thermally we can clearly go to higher drive.  It is not
really warming up at all.

I glued on the stiffener plates on the stage support.  Especially
after tightening the preload/lock nuts the level is far more stable
w.r.t. pressure exerted on the plate.  I think I will go ahead with
bolting the cross-bars down to the blocks since I have the hammer
drill in the lab now.  And also glueing or mortaring the blocks
together.

The level reading does still vary across the plate, but roughly what
we would expect from non-flatness.  Each division is 0.0002" over 10",
which is a slope of 20 ppm.  According to the surface plate
certificate it is 10 um high in the center (18" depth).  This would be
a slope of about 44 ppm, or 2 divisions.  When I move from one side to
the other on the short dimension I get ~5 divisions motion.  This is
actually about right.  It is really amazing that you can make such
small measurements with a level.


[28 May 19]

Note that 3D printed bobbins are not ideal thermally because (unless
you get 100% density print) some parts will be full of approximately
foam-like air spaces.  This is mainly a concern for the cheeks, since
we do not expect any significant dissipation through the core.

ABS prints I got from CraftCloud are noticeably lower accuracy than
the ones from the CMU TechSpark hub.  Not entirely surprising I guess,
because the CMU machine is pretty high end.  Also perhaps simple a
setup error, because the worst problem seems to be that the bottom
part of the prints slumped a bit, probably too hot build plate.
Dimensions were nearly 1mm off in one spot on core at bottom.  Cheek
thickness was also quite noticeably out, more like 1.75mm rather than
2mm.  This may be the build plate slump again.


[17 May 19]

Do we need something analogous to the target-side stage exerciser
asap_stage_calibration.vi that I have for ASAP?  A big part of what it
does is compute the transform to do motion in ASAP coords.  This is
not so important with ILEMT because we don't need test patterns
aligned with the sensor (I think).  Also, a_s_c was working with full
6DOF control, which we don't have.

The other major part of a_s_c was characterizing the performance of
the calibrated system, orthogonality, gain error, etc.  This is fairly
redundant with the Matlab processing, but was a bit more convenient
for exploring issues.

As I recall, a major concern was investigating the effect of probe
error.  Hmmn, also, we figured out the light center w.r.t. stage
coordinates before running the calibration pattern.  Is this
necessary/important?  It did allow us to eg. do combined rotation and
translation during the PSD calibration.

If we do pure translation then there is not any interaction of the
sensor offsets with the pose.  But in fact what we want to do in the
calibration is estimate those source and sensor offsets.  Angular
motion will presumably help with that, tho we also get angular motion
"for free" when we do translation.

Though I am planning to make the source and sensor fixtures so that
they can be indexed by 90 degrees, it would be nice if we did not have
to do this during calibration.  OFC implementing the missing axes is
always a possibility also.  Refixturing would allow us to solve for
the position of the mechanical interface, supposing that we can solve
the coil positions without refixturing.

I had been hoping to make do with stage commands which are basically
in direct axis mode.  I'm a bit nervous about what would happen when
we use pose mode when we are missing axes.  I recally messing with
that a bit with ASAP before deciding to implement the missing third
rotation axis.  Well, there is the thing that we do Rz before x, y,
and this is in the pose code.

I think we have a somewhat better capability now to precisely align
the stage mechanically, what with the surface plate, level and
indicator.  And also the calibration is prepared to absorb various
unknowns.

I have a tendency to think of the stage origin as being somewhere down
in the bottom, where the actuators are, but this is a confusion.  The
origin is wherever the output is when the stage is in the null pose.
More or less, because the end transform will have some misalignment
with eg. the Rz axis.  This is part of what we need to solve for if we
are going to incorporate Rz motion in the calibration pattern.


I have recently been wondering to what degree there may be interaction
between the coils, especially the source coils.  Ideally with the
concentric source the coils will have no mutual coupling, whereas this
is not in general true when we move to non-concentric coils.  Some
arrangements will give lower coupling than others.  I am fuzzy on
whether this is a problem, and how much of a problem.

One way to look at it is that a voltage will be induced, and this
could modulate the field of the coupled-to coil.  Looked at this way,
the signal is being emitted from the "wrong axis".  Another way to see
it is that the coils are being driven from a low impedance, so are
effectively shorted.  Coupling will induce a current which will create
an opposing field, distorting the field of the coupled-from coil.  I
think that these views are equivalent.

This distortion will be an annoying departure from the dipole model,
and may be a reason to choose orientations that minimize the coupling.
Another idea is that we could effectively cancel the coupling by
adding counteracting signals to the drive.  This could perhaps be done
simply using the current sense feedback.  I have long talked about the
idea of using the current sense to cancel distortion, and it would
presumably also work to minimize coupling effects due to the
interaction of the mutual inductance with the driver impedance.

Having a little trouble figuring out exactly where this ended up, but
there were problems with fairly high levels of off-axis signal in the
current sense.  Part of this was due to ground loops in the driver
input, which I largely fixed.  I wondered [2 Jan 16] if some of the
residue might be due to capacitive coupling in the current
transformers, rather than actual current in the output.  The current
transformers are on the low side, so any coupling would see the
variation in the output ground, which is a combination of all the
carriers.
 ==> I don't think it occurred to me that this might be due to magnetic
     coupling between the source coils.

It was a notable pattern that the cross-coupling in the current sense
was worse at low carrier, which I think would not make sense as
magnetic coupling, but does make some sense as a ground loop issue,
since the low carrier drive signal (voltage) is much smaller.  The
high/low currents have been chosen to be about the same, but the
impedance is much lower at the low carrier (and highly lossy), so the
input signal is small, and more sensitive to ground loop issues.

If it is a ground loop in the driver, then the off-axis signals are
real, and it could make sense to cancel them.  Or likewise with
inductive coupling between coils.  I think that what would happen
there is that we would add an off-channel signal of the right voltage
to prevent any current from flowing at the off-axis carrier, and this
would result in the coil looking like an open circuit, and so no field
distortion.

I did do studies on the effect of the sensor looking into an open
circuit or short (transimpedance amplifier).  This was part of why I
decided to go for the voltage amp.  Also the issue of nulling the coil
self-resonance, as is sometimes desirable with search coil
magnetometers, did not seem like it was going to come into play, our
sensor inductance is just not that high, in large part because of the
small size.

I guess it is somewhat challenging to make a direct measurement of the
off-axis signal because of the dynamic range issue.  Any simple setup
would look a lot like what we already have, and it's hard to be
certain there aren't any sneak paths.  One possibility would be the
classic distortion analyzer approach, of notching out the on-axis
carrier and seeing what is left.  If we have the two carriers this is
more complex, but we could drive just one carrier at a time.
 ==> An interesting experiment would be to see what the cross coupling
     is when the on-axis signal is muted.
 ==> Not sure how informative that was.  The result is what you would
     expect: with no output on a channel, the cross-coupling from the
     channel goes away, while the coupling into the channel is
     unchanged.  AFAICT this is what you would expect either from
     coupling in the source or in input-side ground loops.

It remains the case that the low carrier axis cross coupling is
significantly worse, something like -48 dB vs -62 dB, or 14 dB worse.
These are in either case moderately low values.  A little surprised
mutual inductance would be that low (in this imperfect world).

I don't think it makes any sense that coupling would be worse at the
lower carrier under either the inductive coupling model or the current
transformer capacitance model.  But it does fit the ground loop.  FWIW
the low carrier output drive is 16x or 24 dB down from the high
carrier.  This is bigger than the high/low coupling mismatch, but
might be offset by an effect which is larger at the high carrier.

Capacitive coupling in CT (or anywhere else in the input chain) would
definitely be higher at high carrier.  Not sure about inductive
coupling in source.


[13 May 19]

Oops, botched the last print for source bobbins.  Should have been 2mm
shorter.


[29 Apr 19]

It's a fairly chonky coil.

Bobbin tweaks:
 -- clearance re. 10mm on ID
 -- Slight reduction in clearance on the core/flange fit
 -- longer post before the taper at the bottom.  As is there is not room
    for the wire.

The wire clearance is the biggest nuisance, since I would need to add
relief for the wire somehow in the fixture unless I rewind on a new
bobbin.  And if I change design but don't rewind the one, then they
are not the same.  I guess re-winding is not such a big deal.  But if
I am going to change, I could also go down slightly in diameter to get
more like the desired inductance.  But the quantization is a
limitation.  Eg if I went to 16 * 6 turns, that would be < 171 uH (171
from squaring, plus smaller loop area.)  At 6 layers we would be back
to R=20mm.  Perhaps as low as 119 uH?

So I guess going to 6 layers is not a good idea.  But if I reduce to
r=23 from 24, while keeping 7 layers, then I make up the slight error
in wound OD.  Also this would reduce the winding width closer to
fitting 17 turns.  As is, it's a bit too wide (17.9 turns), which is
presumably why some layers have 18 turns and the edges are a big ugly.
This would slightly reduce inductance (8%), to about 250 uH.

This seems like a good idea for purely mechanical reasons (increasing
layer uniformity by improving turns/layer fit).

Inductance per-se is not good, but we have the capability to drive
that inductance, and our air-core field strength is not so great.
(But our sensor is going to be significantly more sensitive, because
it's so big.)


[18 Apr 19]

This is a varnish resin that looks like it would be good for the
source coil impregnation. Dolph DOLPHON® CC-1105 HTC.  High thermal
conductivity and good accoustic noise suppression, suitable for vacuum
impregnation. 
    https://www.eis-inc.com/polyester-resin/p-dolcc1105htcg


[17 Apr 19]
[29 Apr 19]

Poked at the FEMM simulation again, and realized that I hadn't
shortened the coil to the magic 0.72 length and the ID was 22mm, not 20.
________________
Total current = 2.5 Amps
Voltage Drop = 0.528401+I*35.8833 Volts
Flux Linkage = 0.0005711-I*7.36784e-007 Webers
Flux/Current = 0.00022844-I*2.94713e-007 Henries
Voltage/Current = 0.21136+I*14.3533 Ohms
Real Power = 0.660502 Watts
Reactive Power = 44.8541 VAr
Apparent Power = 44.8589 VA

Point: r=4, z=199
Flux= 2.83509e-010-I*3.30797e-013 Wb
|B| = 5.64469e-006 T
Br  = 1.61452e-007-I*1.86423e-010 T
Bz  = 5.64237e-006-I*6.58353e-009 T
|H| = 4.4919 A/m
Hr  = 0.128479-I*0.000148351 A/m
Hz  = 4.49006-I*0.005239 A/m
mu_r = 1 (rel)
mu_z = 1 (rel)
J= 0 MA/m^2
________________________________________________________________

This increases the inductance and sligtly reduces the r=200mm flux.
More relevant is that it increases the fill factor to 42% which may be
too high.  I'm going to do some winding density tests with the wire I
bought and then tweak the coil size if necessary (holding wire size
constant).  The commercial Litz is tighter packed and will probably
stay more circular in cross-section when wound.  I am not sure whether
to use inter-layer paper or not.

 ==> So the coil is now 28.8mm long, 40mm OD, 20mm ID.

53x0.18mm 1.88mm od
15 turns by 10 layers?
I'm getting 19.3 for 10 turns and 1.64mm wound height.
That would give more like 12 layers and maybe 15 turns is a little cramped.

Hmmn, with 14x12 = 168 turns I'm getting 448 uH.  That is getting to
be really too much inductance (the voltage is getting too high, and
not giving any room to increase current).  So options are:
 -- Go to bigger wire (but I want to use this wire),
 -- Space out the winding more with interlayer insulation and looser
    turn space.
 -- Go to a smaller coil
 -- Just use fewer layers and let the aspect ratio go off.

What scaling do we expect?  Holding wire constant, seems it might be
4'th power?  Or more than 2'nd anyway, since winding area will
increase as r^2, and the loop area (and magnetic moment) also
increases as r^2.  I'm not sure about inductance though.  Maybe even
a higher power?  So a pretty small tweak in source size would get us
back in the right ballpark.  For 4'th power we would go from r=20 to
r=16.3 to reduce L from 448 to 200.

So: r=17, D=34, ID=17, L=24.5 L/2=12.25
12 turns by 5 layers (?)
 ==> Did I have layers wrong above?  Yes, should be 6 layers, 84
     turns.  No wonder fill factor was so high...

With this fix, the r=20mm has:
________________________________________________________________
Total current = 2.5 Amps
Voltage Drop = 0.275738+I*17.5831 Volts
Flux Linkage = 0.000279844-I*3.3485e-007 Webers
Flux/Current = 0.000111938-I*1.3394e-007 Henries
Voltage/Current = 0.110295+I*7.03325 Ohms
Real Power = 0.344673 Watts
Reactive Power = 21.9789 VAr
Apparent Power = 21.9816 VA

Point: r=1, z=199
Flux= 1.24399e-011-I*1.3462e-014 Wb
|B| = 3.96093e-006 T
Br  = 3.19568e-008-I*3.42209e-011 T
Bz  = 3.9608e-006-I*4.28623e-009 T
|H| = 3.15201 A/m
Hr  = 0.0254304-I*2.72321e-005 A/m
Hz  = 3.15191-I*0.00341087 A/m
mu_r = 1 (rel)
mu_z = 1 (rel)
J= 0 MA/m^2
________________________________________________________________
Now the inductance is rather too-low at 112 uH, and the flux is down
OFC. (should be down by 1/2)  Let's try:
  r=23mm, D=46mm, ID=23mm, L=33mm, WA=11.5x33mm, L/2=16.5

Then: 17 turns by 7 layers = 119 turns
Now we have:
________________________________________________________________
Total current = 2.5 Amps
Voltage Drop = 0.466834+I*40.6079 Volts
Flux Linkage = 0.000646294-I*8.31525e-007 Webers
Flux/Current = 0.000258518-I*3.3261e-007 Henries
Voltage/Current = 0.186734+I*16.2432 Ohms
Real Power = 0.583543 Watts
Reactive Power = 50.7598 VAr
Apparent Power = 50.7632 VA

Point: r=2, z=201
Flux= 9.07292e-011-I*1.05014e-013 Wb
|B| = 7.21096e-006 T
Br  = 1.01672e-007-I*1.16092e-010 T
Bz  = 7.21024e-006-I*8.34563e-009 T
|H| = 5.7383 A/m
Hr  = 0.080908-I*9.23835e-005 A/m
Hz  = 5.73773-I*0.00664124 A/m
mu_r = 1 (rel)
mu_z = 1 (rel)
J= 0 MA/m^2
________________________________________________________________

This brings our flux up to 7.2 uT, which is a predictable cheap
thrill.  Inductance 258 uH is a little higher than the 200 uH I was
aiming for.  So it would seem that inductance does indeed change
faster than 4'th power.  Maybe for a prototype I should try r=22mm

Uh, so if
  L1/L2 = (r1/r2)^k
  log(L1/L2) = log(r1/r2) * k
  k = log(L1/L2)/log(r1/r2)
  k = 0.3624/0.0607 = 5.97

So let's call that 6'th power.  Under this empirical obervation 22mm
should be about right.  At least until proved otherwise we want about
2mm extra radius on the cheeks, or r=24mm.
  r=24mm, D=48mm, ID=24mm, L=34.6, L/2=17.3

I wound this [with printed r=24mm bobbin], and got 138 turns, with 17
turns in the top layer for a basically full bobbin.  So 8.12 layers?
My guess is that some of those had 18 turns, since the turn spacing
was not super even.
 ==> I get a 367 uH and Q=11 @ 1 kHz.  So I need to go down to about
     107 turns to get 220 uH?

Winding OD is 50mm rather than the nominal 48, but OFC we are
quantized by layers.  Let's try taking off a full layer.
 ==> That gets us down to 272 uH @ Q 9.8.  That would be 7 layers of
     17t, or 121t.  We have two extra turns w.r.t. 7 * 17, assuming
     counting is right.

I'm inclined to stick here because much more and we would be going
significantly below the design OD, which is ~47mm as is.

The inductance change pretty closely followed the quadratic rule, we
would expect 282 uH at 121 turns by removing the layer.  I am a bit
surprised how far the inductance is from simulation (though I don't
recall how close we got in the ferrite design).  What's true is that
we have a quite thick winding, and whatever lumped model FEMM is using
may be a bit off.

 ==> Our mean layer height (with no layer paper) is 1.643mm.  That is
     identical to the single layer height, so there is not any effective
     intermeshing of layers.
So probably layer paper would not have much effect (except for the
paper thickness itself).  Two layers of paper + tape would be maybe
0.25 mm, so 6 layers would be 1.5mm.  That would be reasonable (but
not insignificant).  OTOH the layer winding would get us out of the
standard commercial winding regime (where we are really trying to
stay).

Unfortunately the coil winder does not go up to a coarse enough
spacing, I could not get lower than 19-20 turns/layer without making
the nominal winding width shorter than it should be.  And the turn
points were not quite right, so I manually tweaked a bit as the
winding progressed.  Inter-layer paper and practice would help.

It is likely that the spacing also does not go as fine as I want, but
that may work a bit better.  As long as there is even fill it should
not matter whether each layer is close-spaced.


I had earlier read up on 3D printing design rules, but didn't write
anything down.  It seems like 1mm is a typical min wall for ABS, so
2mm should be OK for the cheeks.  There does not seem to be an upper
thickness limit, but greater thickness would probably reduce warping
issues in the flat surface.  Printing the cheeks as separate parts,
face down on the bed would likely also help.

Strength on the center core would be better lying flat, but vertical
might give better surface finish, since it would be self-supporting.
It would be easier to sand the inner surfaces to clean up if there are
not any inward-facing bosses or ridges.

The core walls can be pretty thick.


[8 Apr 19]

FWIW, the ilemt_ui calibration seems to be working again, which it
hadn't when I last tried, see [26 Jan 16].  And after doing
calibration the distance from high/low carrier are matching up again.
IIRC I had stuck with the 1DOF values because calibration was not
working, which might explain the change in relative gains somehow.
Maybe I changed the low carrier frequencies?

Each carrier (high and low) is going to have a different gain, but in
Matlab we do have the source gains, which are the same as carrier
gains, and these are distinct for high and low.  In labview, though,
the distance only has any validity for the 1DOF motion that we use to
calibrate.  If we imported the Matlab gains back into Labview then we
could have a more sensible behavior.  But this would still totally
break when the source and sensor coils are not nominally concentric.

I'm tending more toward the idea of machining the dipole-approximating
source bobbins, rather than 3D print.  It's really partly a matter of
my skill set, and also the theory that we should start out with a
precise setup for characterizing, and then see what happens with lower
precision later on.

I'm thinking that we can make source and sensor on cubes, and this
will allow 90 degree indexing, which will be very useful given that we
only have Rz axis at the moment.


[5 Apr 19]

Was resurrecting the FEM analysis, and I can't seem to find the model
of the as-built core.  See [9 Dec 15], [21 Dec 15].  Seems to be 33ga
x 20 Litz, with:
  X: 40 turns, ~200 uH
  Y: 39 turns, 184 uH
  Z: 35 turns, 145 uH

See also [21-23 Jan 16] where I have impedance measurements with the
wire (which seems to add ~40 uH).

Core is a 40mm cube, so area is 1600 mm^2, corresponding to cylinder
radius 22.6 mm.

Now I'm getting:
    Total current = 2.5 Amps
    Voltage Drop = 0.50666+I*34.6751 Volts
    Flux Linkage = 0.000551871-I*1.03612e-007 Webers
    Flux/Current = 0.000220748-I*4.14449e-008 Henries
    Voltage/Current = 0.202664+I*13.87 Ohms
    Real Power = 0.633325 Watts
    Reactive Power = 43.3438 VAr
    Apparent Power = 43.3484 VA

Point: r=5, z=199
Flux= 1.15043e-009-I*6.65113e-014 Wb
|B| = 1.4635e-005 T
Br  = 5.3214e-007-I*2.90682e-011 T
Bz  = 1.46253e-005-I*8.45669e-010 T
|H| = 11.6462 A/m
Hr  = 0.423463-I*2.31318e-005 A/m
Hz  = 11.6385-I*0.000672962 A/m
mu_r = 1 (rel)
mu_z = 1 (rel)
J= 0 MA/m^2

The inductance is pretty close, but I'm surprised how low the power
is.  Also this current is maybe higher than we are running at for just
the high carrier?  I recalled the current as being about 2.5A RMS.
But, against that, IIRC the current in FEMM is a peak current, not
RMS.

The main reason for resurrecting the FEMM for the current source is
that I want to design a new air-core dipole approximating source.
Ideally the new source will:
 1] Have as good or better strength (magnetic moment) at a reasonable
    power dissipation, at a voltage and current that is reasonable with
    the existing driver.
 2] Preferably have about the same inductance (which is kind of
    implied by [1]).
 3] Be as small as we can make it.
 4] Be wound in full layers for max symmetry.
 

OK, air core, just getting rid of core and expanding winding area.

Total current = 2.5 Amps
Voltage Drop = 0.324282+I*3.35708 Volts
Flux Linkage = 5.34296e-005-I*8.38429e-009 Webers
Flux/Current = 2.13719e-005-I*3.35371e-009 Henries
Voltage/Current = 0.129713+I*1.34283 Ohms
Real Power = 0.405353 Watts
Reactive Power = 4.19635 VAr
Apparent Power = 4.21589 VA

Point: r=6, z=199
Flux= 2.21846e-010-I*3.4571e-014 Wb
|B| = 1.95859e-006 T
Br  = 8.98407e-008-I*1.38063e-011 T
Bz  = 1.95653e-006-I*3.04911e-010 T
|H| = 1.55859 A/m
Hr  = 0.071493-I*1.09867e-005 A/m
Hz  = 1.55695-I*0.00024264 A/m
mu_r = 1 (rel)
mu_z = 1 (rel)
J= 0 MA/m^2

I think it is putting all of the winding on the inside edge because the
fill factor is so low.  So the inductance and moment are also way low.

________________________________________________________________
Increase to 120 turns:
Total current = 2.5 Amps
Voltage Drop = 1.01151+I*31.7181 Volts
Flux Linkage = 0.000504809-I*2.44238e-007 Webers
Flux/Current = 0.000201924-I*9.76952e-008 Henries
Voltage/Current = 0.404603+I*12.6872 Ohms
Real Power = 1.26438 Watts
Reactive Power = 39.6476 VAr
Apparent Power = 39.6677 VA

And to 40x litz:
Total current = 2.5 Amps
Voltage Drop = 0.528766+I*31.7068 Volts
Flux Linkage = 0.00050463-I*4.88471e-007 Webers
Flux/Current = 0.000201852-I*1.95389e-007 Henries
Voltage/Current = 0.211506+I*12.6827 Ohms
Real Power = 0.660958 Watts
Reactive Power = 39.6335 VAr
Apparent Power = 39.639 VA

Point: r=6, z=199
Flux= 6.82598e-010-I*6.5459e-013 Wb
|B| = 6.02639e-006 T
Br  = 2.76431e-007-I*2.61417e-010 T
Bz  = 6.02005e-006-I*5.77339e-009 T
|H| = 4.79565 A/m
Hr  = 0.219977-I*0.000208029 A/m
Hz  = 4.7906-I*0.00459431 A/m
mu_r = 1 (rel)
mu_z = 1 (rel)
J= 0 MA/m^2

So inductance and power are about the same, but the flux is a little
less than 1/2.  6e-6 T vs. 15e-6.
________________________________________________________________


From [Fall 2014] Actual wound dimensions of 33ga x 40: 1.86mm width by
1.5mm height.  So let's say this is a rectangle, 2.79 mm^2.  Our
winding area is huge: 10x40 mm, 400 mm^2.  This would let us have 143
turns, which is not far from the 120.  But there is still the problem
that FEMM thinks there is plenty of space, so is not using the full
loop area.
   ???  I had thought previously that FEMM more or less evenly
   distributed the turns in the area.

FWIW this result is roughly consistent with what I recalled from
earlier analysis, that the core gives approx a 2x benefit in field
strength at a given size.  So you would need to increase diameter by
sqrt(2).  There would also be room to increase power dissipation,
since we have 3x or more as much surface due to having separate coils.
We might need to reduce the inductance to get more current.

Note however that a 2x loss of field strength only means a 20%
decrease in range (4th root of 2).  If we increase current by root(3),
then the field goes up to 10.4 uT.  Then range drop is only 10%.
 ??? Is 4'th root right here?  Anyway, the difference is not huge.



Stuff from phone, mostly about the stage setup:
________________________________________________________________

Problem with software limiting for pole bonk is that homing might
cause bonk if we start out in extreme pose. This is avoided when we go
home before shutdown, but don't want to rely on user discipline. How
close to plate do we need to get for eg. 170mm min range? Would be
best if we set up so that bonk is impossible until we have more
experience with operational issues. I don't want to give up the +/- 90
Rz swing.

Did some survey for metal effects in the corner of the lab where we
are doing the setup. The height we ended up with seems to be a good
compromise between ceiling and floor effects (nearly midway between by
distance). At this height the wall seems to be the biggest
effect. With source-sensor axis normal to wall we need to be about 36"
away.  In this general area the reading is nearly constant with xy
motion and modest z motion. There is about a 15 um shift with 90
degree Rz.  Possibly this is a dc field effect in the cores?

Maybe the greater warm-up drift on the low carrier is due to impending
saturation in the current transformer at low carrier frequency? The
third harmonics are higher than second. For first carrier, -48 dB.
That doesn't seem too bad, and also may be actual distortion in
driver.  Or another idea, conceivably a ferrite effect? Thermal change
in permeability?  We will see with air core coil.  I don't recall the
warm-up drift being quite this bad, 100s of microns on the low
carrier. Interesting to plot change of current during
warm-up. Probably larger for low carrier because of resistance change.

 ==> After warm-up has been going on for some time 30+ minutes, the low
     carrier amplitudes are still decreasing, but the current sense is
     helping quite a bit. Current sense is even better on the phase
     correction. Think I noticed this before, but on the high carrier
     it is unclear if the current sense is helping at all.

Z center is 13.25 from top of baseplate
Riser is 23". Blocks are ~7.5" square
Z stage bolt pattern is 5" square
Z axis is 1/2" to the left (-Y) of center. Make pole 1/2 to the right
to compensate.

NEMA 17 42 x 38 mm, Pololu #2267 looks good for Rx, except we want the
back shaft for encoder, probably.  Except US digital has a 10k line
(40k resolution) encoder now. This would be fine enough we could put
it on the output shaft. That's 157 urad, or 31um over 200mm. Better
res would be nice, but decent, and avoids the need for a limit switch,
and is not affected by error due to the belt drive.

Breadboard holes can be 1" centers
Pole is ~11" shorter than platform height above stage base

Bonk radius is about 14", +xy moves, say 18".  So we need 36" under
table? What we've got now is just 16", table is 24.  But also need to
consider that pole might hit plate. Should set limits and position
stage so this can't happen. Maybe add software limiting that is
smarter about avoiding pole bonk but still lets us get right up to the
edge of the plate.

Oh, also if pole is +Y wrt Rz, then it moves a lot with Rz, which
complicates the pole bonk near +/- 90 degrees. That may be a reason
get the pole centered, and not do the "diving board" sensor
platform. With Z slid all the way forward, mount is ~4.5" +Y. If we
use a short piece of the 2" tube as spacer we will not be too far off.

The sensor platform will be well above the plate, no matter what. Only
the pole is an issue. But with diving board we can get the sensor
closer without risking pole bonk as long as we limit the Rz. But if
pole is centered the platform can still stick out forward. (Will to
some degree, no matter what.) Thing is, we want to rotate about the
sensor, wherever it is. So if it sticks out we want to rotate around
that. Awkward to mount right over pole because poor access to bottom
of platform. So maybe having pole +Y wrt Rz is actually good.

Hmm, once we have the Ry axis we will want it to intersect Rz at null
pose.

Probably should set Rz limits at +/- 55 at least for now to reduce
pole bonk issues. Also maybe breakaway pole mount.

The z axis needs to clear the plate supports. We only need another
1.5" on the block stack (plus levelling feet).

Move plate in X direction so we have greater move away from center by
sliding. Slider Rx motor should stick out the other way.
________________________________________________________________

r.e. stage setup, for now I've dealt with pole bonk by just moving the
stage far enough away that we can't hit.  But the sensor platform can
still run into the slider.  At this distance there is no problem with
the encoder hitting the plate support.  I have a scheme to make the
pole attachment spring-loaded, which is not much more complicated than
the current just-bolt-it-down approach.


[29 Mar 19]

I've got some plans for the stage setup now, and parts made, see
notebook.  Idea is to skip the base for now, and put the stage and
risers on the floor.  I need to figure out how long the pole needs to
be, and I can fill in some numbers:


Plate stack up:
  Riser: 23"
  Plate support: 4"
  Levelling feet: 1"
  Plate: 3"
  Slider: 10" from plate

Slider top to floor = 41" 

Stage stack up:
  Feet: 1.5"
  Z stage center: 13.25 + 0.375 = 13.625
  Z motion: +/- 2"
  Pole from Z center to top end: ???
  Platform mount + platform thickness: 1.75"
  Platform top to floor = pole + 16.875"

Then pole = 41 - 16.875 = 24.125" (assuming it ends at the Z center).
Exactly how long the pole needs to be depends on details of the
attachment to the stage.  Also we may want the sensor platform
somewhat above the slider top because the sensor is smaller.  Really
we would like to be able to adjust the pole by a couple inches, as
well as ideally having some sort of breakway action and also some sort
of centering guide that helps to get the pole vertical after
repositioning or breakway.  Maybe breakaway is not really necessary
especially if we keep the stage farther away and set the Rz limits.
Combining the breakaway with pole adjustment is a bit of a nuisance.

Note riser is approximately the same height as stage top.  Feet +
stage top = 25.5", giving 16.5" from stage top to slider top,
satistifying the 14" min distance from stage even when stage is +2" in
Z.  So the riser height seems about right.

I have considered boosting up the plate supports by 1-2" so that the
stage top will pass underneath.  This is not needed or useful in
normal operation because the pole will hit anyway, but if the pole has
a break-away feature then the encoder will be the next thing to hit.

Could add a pole-bonk photo limit switch.


[8 Mar 19]

Trying to rough in height dimensions for the test platform.

Stage with current Z axis is 15" x 17" x 24" W x D x H, without any
clearance for moves.  But it is the Z axis that is most critical, and
the motor does not move.

Stack up:
  Stage: 24"
  Z motion: +/- 2"
  Riser: ??
  Plate support: 3" (no benefit to more thickness, whatever is stiff enough)
  Plate: 3"
  Slider: ?? 12" min from plate, 14" min from stage 
  
See page 86 in notes.  There is really two stacks one upward: base +
stage + pole, and one downward: sider + plate + support + riser +
base.  The slider has become way taller than I was visualizing, and
the riser has become much shorter.  IIRC, I was previously thinking
24" from stage top, but it seems 14" is plenty.  This combined with
the taller slider means that the riser could now be just 2 blocks
rather than 5-6.  That will be way way stiffer.  Then the plate is
almsot even with the top of the stage, and is at a table height
(~30").

But the slider becomes is really awkwardly tall.  To not be
ridiculously tall and skinny (and unstable) it needs to take up a lot
of the plate, not giving us all that much motion.  Well, perhaps I
exaggerate.  If it were say 12x12x12, then we would have 12" in X and
6" in Y.  This is still 2x the stage motion or more, and is far more
precisely flat.


[7 Mar 19]

I looked up about driving multiple ADCs from a single reference, and
it is indeed a thing in industrial/medical equipment.  Also found an
Linear Technology article for eg. the LTC2500 emphasizing the need for
individual reference buffers.  This was not always done in the
reference distribution schemes I found, but often they were lower
resolution higher speed, for eg. ultrasound.

Some nice TI articles suggest using a composite two-opamp slow/fast
buffer to get low output impedance at HF combined with high DC
precision.  We do want pretty decent DC performance if there are going
to be per-channel reference buffers.  The composite amp also gives
high input impedance for low filter corner, but we would want the high
impedance to be in a first stage buffer (at the reference), with the
high speed buffer located at the load (ADC or DAC).

I also had some thought that I could use an external reference buffer
and filter with eg. PCM4222 sigma-delta if this could be arranged as a
sort of capacitance multiplier.  That would not get matched DC
performance, but would allow testing whether reference noise is a
factor.  Or just try big-ass capacitor.

FWIW, the PCM1794A is also I2S, so can be fairly easily driven from
Raspberry PI.  I got two boards off of EBAY (4 channels).


[22 Feb 19]

Got the tracker back into the lab and did some magnetic compatibility
testing.
 1] Stage is less of a problem that I feared.  The upper end of the Z
    axis is pretty OK even at 12".  Biggest effect is from the largest
    aluminum bits when parallel to the source-sensor ray.  For Z axis,
    this is ok 18" (? garble).  The baseplate end of XYRz is
    noticeable at 18", but that proximity is not going to happen.
 2] *Unfortunately*, the granite bits do interfere.  Not as much as a
    similar chunk of metal, but it reaches our 200 um error budget.
    (And bear in mind these numbers are not a calibrated 6DOF pose).
    So the surface plate needs to be well below the source-sensor
    plane, and the slider needs to be taller. (see below)

So, well, getting the plate lower is kind of good.  This reduces the
structural problem with the platform.  The slider needs to be higher,
9-10", which means it also needs to be wider for stability.  At least
that wide, and perhaps 6-8" deep.  On the plus side, it might then be
possible to mount the slider rotary motor below, rather than
cantilevered out to the side.

With the newmark rotary stage the metal effect is (a bit surprisingly)
not really any lower than the Z axis motor, requires 12"+.  But if
this is used for the Ry axis (with belt) then it will be well below
that.

Max error from the granite straightedge is actually when the granite
is perhaps 50mm off axis.  With the board sitting on the straightedge
it is fairly small.  For the surface plate, it is high when on the
surface (perhaps because thicker?).  But 10" or even 8" away from the
granite seems to keep the deviation down at the noise floor.  A bit of
deviation starts farther way (feet), but this may be larger more
distant interferers in the floor.

FWIW, the idea that the low carrier is less affected is not holding up
for the room-level effects, or for the granite.  For eg. the table,
wire in the straightedge's box hinge, etc., the low is affected much
more.  This is a typical ferromagnetic effect.

With the granite, the low effect is approx 2x the high effect.  I'm
guessing the granite is somewhat ferromagnetic (a bit of iron), but
nonconductive, so the ferromagnetic effect is hitting both, but the
permeability is lower at the higher frequency (or something).

There are some moderately large position effects (10's of microns)
moving around the room, near the floor (in some spots), and especially
near the ceiling.  The ceiling effect gets into the 100's as you get
less than 12" or so away from the tile.  For ceiling and floor the
high carrier is affected significantly more. A bit of a surprise given
use of structural steel, eddy current presumably, but we are prepared
to handle that case.

The setup needs to be recalibrated for gain.  One odd thing I am
seeing is that the high and low range mismatch when there is a small
rotation in the sensor slider.  The low carrier chart also drifts
more.

Also, there is far bigger residue on sensor axis X, synchronous crud.
Distortion filter doesn't seem to help.  Clipping somewhere?  The
variable mismatch between high and low also suggests nonlinearity. Or
anisotropic frequency dependent metal effect?


[7 Feb 19]

I was thinking it might be useful to make a low-noise LC oscillator
for ADC testing.  This would permit testing of the amplitude
dependence of SNR without the DAC.

But ideally this would be phase-locked to the sample clock, which is
another can of worms.  A related thought would be to use a high-Q
bandpass filter on a DDS output.  But these are aimed at much higher
frequency and top out at 14 bits resolution.

Second thought would be to start looking at high-res DACs.  It seems
the PCM1794A has low-cost boards available on eg. AliExpress.  This
could be used to drive a resonant tank as antialias filter, which
would get us a definite phase-locked output frequency.  And this is
also relevant if we keep an analog-input driver.


[6 Feb 19]

Thinking about the new nonconcentric source design, and am now
thinking a fan in the source could well work.  I'm more concerned
about the source interfering with a brushless fan motor than I am with
getting interference from the motor.  The fans I've taken apart do not
have much metal in them, though usually a cup steel polepiece.  But
the metal bit is only doing rotary motion, and if the fan is on the
unused side of the source...  It would be possible to modify a fan,
locating electronics remote, etc., but this gets away from the idea of
simple construction using off-the-shelf components.  [And also, the
gain from running at higher power is not that big.  If you double
current, heat goes up 4x, yet range increases only 20%.  It is much
more effective to increase source size.]


[4 Feb 19]

Installed FEMM on new laptop.  I think we can use FEMM to tune the
field of source and sensor coils for dipole simulation.  We can dump
the field vector (normal and tangential components) on a circular arc
contour (really only need 90 degrees), and compare to the dipole
model.  Presumably it would also be possible to use optimization via
the Matlab interface.

This most obviously applies to the source, but I believe the source
and sensor are interchangeable in magnetics, so we can drive the
sensor and check its field.

A big concern with the sensor is how it responds to the nonuniform
field created by the source.  I suspect that with eg. a long rod core,
the response is more like what we would see at the location of the
nearest core end (the apparent source/sensor distance varies with the
sensor orientation).

A physics question is, if the vector field at a certain radius (when
used as a source) is an approximate dipole, then does the sensor (at
that radius) act like a dipole when sensing, even in a nonuniform
field?  I have a hunch that if the field is defined at a surface, then
outside that surface we cannot tell what is going on inside the
surface.  If so, then if we generate a dipole-like field then the
sensor will have a well-defined center location, independent of
orientation.

Using a 3D solver it would be possible to directly simulate these
situations.  But moving to axisymmetric source and sensor coils
(non-coaxial) makes the FEMM solution still informative.  There will
presumably be second-order effects due to interactions between the
source coils, and between the sensor coils, especially due to their
cores.  This is also something we could simulated in 3D.

I was thinking about sectioning the RFID coils to reverse engineer
their geometry for simulation.

Is the bobbin core a reasonable geometry for the source coils?  My
hunch is that the bobbin core is a less efficient field source than
more of a plain bar core, and also that the bobbin is going to have
field which is very far from dipole.  The magnetic path is more closed
due to the end plates, and this is going to increase the inductance,
requiring a higher drive voltage.  However IIRC the core loss is very
low (at 10 kHz), so it would be possible to reduce turns to get the
inductance down (get a more reasonable load impedance).

I was considering the possiblity of shaping the end of the core to
improve the dipole law conformity, but it would be preferable to use a
relatively stock core, like a rod or stack of toroids.  We can play
with variables such as making the rod OD less than the coil ID, and
making the core shorter or longer than the coil.  Also, a nonuniform
cross-section in the core can be achieved by stacking different core
parts, e.g. different size toroids.  My expectation is that there will
not be a huge difference between a hollow and solid rod core of the
same outer dimensions.

Looking at Paperno & Plotkin '04, the preferred coil design they come
up with is quite simple: a multi-layer coil with Length/Diameter (L/D)
= 0.72 and coil ID (air space) of D/2. With this design the dipole
conformity is greatly improved (33x) over the very low L/D coils
(approximating a single turn) used in air-core sources (such as some
of the larger Polhemus and Ascension sources).


FWIW they simulate 100 turns.  I would expect that the number of turns
does not matter hugely as long as there are multiple layers (but even
one layer is pretty good).  I guess I can extract the approximate wire
size d_w. The winding thickness is D/4 while its length is 0.72 D.
So:
  100 = 0.25*D/d_w * 0.72D/d_w
  100*d_w^2 = 0.18 D^2;
  d_w^2 = 0.18 D^2/100;
  d_w = 0.42 D / 10
  d_w = 0.042 D

So if D = 40mm, then d_w = 1.7mm.  So that's not crazy, FWIW.  We
would still want to use Litz for the source.

They concentrate on evaluation of dipole conformity at r = 4R (or 2D).
This is not super-close, but the coil itself extends out to R (or a
bit farther diagonally), and there will be some packaging around both
the source and sensor, so this could be a source-sensor separation of
as little as D.  Eg. with a 40mm source, the minimum source-sensor
separation would be < 60mm.  This is not terrible.  If we operated
down to 2D, and with 40mm source, and our design distance is 5D (200mm),
then we can operate over a range of 3D or 120mm.  Also we can operate
outside of 200mm, just at reduced SNR.  Over this 2D-5D range, the
signal variation would be ~16x.

I'm thinking that for our first cut we should just use air-core
coils.  This has other advantages besides not requiring design of a
dipole law core:
 -- Much less interaction between coils (only eddy current, not
    ferromagnetic).  (I am pretty sure of this though I haven't
    investigated it.)
 -- Simpler construction
 -- Better heat dissipation because ID surface area also available
    exposed.

In comparison to the RFID coils, the main disadvantages would be:
 -- Less compact for given sensitivity (due to no core and less
    aggressive packaging, wire size).
 -- Is a custom part that has to be fabricated.  But well within the
    scope of standard coil-winding practice, so it could be outsourced
    easily.

In sensor could perhaps use a stock bobbin for eg. a pot core?  But 3D printing
the bobbin and support should be straightforward too.  Just not in my
current skill set.
 ==> Pot core bobbins have too big hole, are too short.  But the RM
     core looks plausible, eg. RM6, 495-5243-ND.  With 15mm^2
     theoretically you could get 1898 turns of #40 on there.  That's a
     lot.


[23 Jan 19]
		Tce		Water abs	Tensile mod
Delrin:		1.1e-4 1/K	0.22%		3 GPa
Delrin + glass:	3.5 .. 10 e-5	0.15%		4 ... 9 GPa
G10    	 	2.3e-5		<0.1%		13-24 GPa
Concrete	0.7 .. 1.2 e-5			14-41 GPa

For a sense of magnitude, at Tce of 2.3e-5, with a 1.2 m pedestal, we
would get 276 um change over 10 degrees K.  This change is a relevant
magnitude for us, though we don't really expect a 10 K swing.  So it
is worth worrying about the thermal stability of the stage components,
especially the pedestal.

Total shrinkage of concrete during drying is ~7e-4. Reversible change
due to re-wetting (presumably saturation) is ~3e-4. Concrete is pretty
dry after 60 days (presumably depends a lot on thickness).  The
worst-case water related changes are of similar magnitude to thermal
thermal effects, so are probably not a big deal, especially for prefab
blocks, which are presumably pretty dry.

Standard concrete block is 8" x 16", available in various heights, 8"
most common.  But there are many special blocks, and thinner blocks,
like 6x8x16.  Can also have flat or even rounded ends, and also better
surface finish.  Hollow concrete block is usually between 50% and 75%
solid.  Lightweight block concrete is <105 lb/ft^3.  Standard block is
>125 lb/ft^3.  This is not a huge difference.  But lightweight block
has higher porosity, so presumably less humidity stability.

Fiberglass threaded rods on the scale of the pedestal are not insanely
expensive.  These could run vertically thru cement block, provide
tension strength and tie points for surface plate and feet.  They
could be prestressed and cemented in.  FWIW, Strongwell 3/4"
fiberglass threaded rods are specd as 4500 lb ultimate strength, so
presumably could be used at say 1000 lb.  If this was tied into
fiberglass crossbeams (eg square tube) on the top, then this should
give the setup adequate shear rigidity to avoid toppling sideways.  4'
rods may be a little skimpy, since they need to stick out of top and
bottom of column, and 4' is the height we want of plate top from stage
baseplate. But the buildup of plate + support structure could easily
add 7".

There is still kind of a problem with the baseplate.  If the optical
bench is magnetically compatible we could bolt all the way thru it

4' is six 8'' blocks high.  Let's say 5 blocks + plate and support.
Overall height from floor is going to be larger due to thickness of
baseplate and suspension.  At 35 pounds each, that is 175 pounds per
column (without any concrete infill).  That isn't quite as bad as I
thought.

Mmm, I looked at 4', and it's a lot taller than I was thinking.  This
is a possible standing height.  So ergonomics are not great.  But also
the big weight is going to be way high up on the column.
 1] The column needs to be strong enough for the shear load if
    someone leans against the table or whatever, say 100 lb.  This is not
    bad with preload on the concrete block from the fiberglass rods,
    but does make me like the 8x12" block more than 8x8".  The column
    is going to be weakest in the Y direction, which would be the long
    dimension of the block.
 2] We also need acceptable static stability under shear load, eg. not
    toppling over.  This suggests a baseline of the feet which is
    similar to the height of the plate.
 3] We have a huge angular moment of inertia for swaying Rx Ry vibration.
    A low resonance can result in a really long settling time after any
    disturbance.  Static stiffness of the cement block columns should
    be adequate, but if the platform/column attachment and the
    platform itself are not really stiff then the swaying mode
    resonance of the column/plate setup may be pretty slow and poorly
    damped.  XY vibration will also excite this swaying mode, but this
    is probably more of an issue with eg. touching the plate rather
    than ambient vibration.  At low frequencies the ambient vibration
    is weak because larger displacement is required to create the same
    acceleration (and force).
 4] In addition to the sway of column w.r.t. the base, everything
    could also way as a single unit because of compliance in the
    feet or other suspension.  It may be a bad idea to use spring feet.
    Harder feet will reduce rejection of HF vibration (eg. the 10 Hz),
    but improve the settling time after disturbance (such as moving
    the stage).  Especially with softer feet the baseline also has
    effects on suspension resonance.






Weight:
Surface plate:			185
Columns (2):			350
base plate (optical bench)	100
stage: 				150
total:				785

Columns are a lot of the total weight, and if baseplate is really that
light, then it may be limiting and/or the columns are overkill.

Spring type machine mounts would work for the suspension.



I need dimensions on the stage with its swings.

Steps in stage construction:
 -- Get surface plate and parallel or straightedge
 -- Check metallic interference between between stage and tracker.
 -- Get stage working again.
 -- Get surface plate, mount on pedestal on optical bench base, get some
    kind of vibration-absorbing support for base.
 -- Characterize performance of stage with post. 
 -- Slider
 -- Rx axis (slider refinement)  Linear encoder?
 -- Ry axis
 -- Aux axis actuator
 
 


[20 Dec 18]

Discovered that reference noise is another thing which causes a noise
floor that increases with increasing signal magnitude.

This is something that probably can't be helped much with the PCM4222,
or even really evaluated, because it has an internal reference.  I
could try adding on a much larger reference bypass cap.

But with the LTC2500-32, the eval board uses a reference design which
does not have low noise below 10 kHz (even though the irrelevant DC
noise is admirable).  It seems that to get a reference with adequate
low impedance and also low noise in the kHz range, I may need a
reference with a multi-pole filter followed by an opamp buffer (with a
large output capacitor).  See
http://www.ti.com/lit/an/slyt354/slyt354.pdf

The basic problem is that bandgap references are fairly noisy (10's of
nV/rtHz), and yet we want low noise down into the 100's of Hz.  This
requires a pretty low corner and perhaps multiple poles, and this is
not compatible with the typical approach of taking a reference and
swamping the output with capacitance.

I was also a bit surprised to discover that LTC and AD do not have
high res (> 16 bit) DACs with AC specs.  The 20 bit ADS5790 is
possible, but the chip is quite expensive, and it has no AC specs.  It
seems like the audio DACs such as the PCM1794A may be the way to go
for analog output.  Interestingly this is also not a sigma-delta
architecure, but rather "advanced segment".


[6 Dec 18]

Got to thinking about the calibration stage issues, and it's not pretty.  One
thing to bear in mind is that our desired accuracy is 200 um, so ideally we
would like a positioning error of say 20 um.  That is 0.0008", which is pretty
demanding.  OTOH, looking at my notes in
http://filter.micron.ri.cmu.edu/wiki/doku.php?id=projects:asap:calibration:stage

it seems that even before pessimizing things to deal with magnetic
compatibility we are not performing at that level.  On-axis position
repeatability seems better than 1 um, but runout and misalignment contribute
considerably.  I said that:
  Realistically it seems that for runout + axis misalignment you can expect
  better than 1 milliradian (1 micron per mm.) 

So we would only achieve the 20um accuracy over +/- 20mm motion, not 200mm.
Well, this is conservative, though we don't really know how much.

But what is really bad is the effect of putting the DUT out on a pole to get
it away from the metal stages.  As well as creating a torque load that may be
excessive, it also magnifies the runout.  Broadly, I'm thinking it is
unfavorable to have the DUT moment exceed the baseline span of the bearing.
Above this moment, an irregularity on one side bearing is magnified at the DUT
(rather than attenuated by up to 1/2 if DUT is at zero moment, midway between
the bearings).  It would seem that we would be using a DUT moment of 3x the
bearing baseline or more.  

FWIW, the flatness/straightness spec on the linear stage components is +/-
20-30 um.

One thought is to make use of a surface plate, and perhaps other granite setup
components.  Even a grade B 24x18" surface plate is flat to within +/-
0.0003", or 8um.  I assume granite is sufficiently non-magnetic and
non-conductive.  

I think this is where I got a cheap surface plate before:
https://www.shars.com/products/measuring/parallel-sets?parallel_set_category=Granite
They have some granite parallels which could be used to establish the xy
coordinates.  (most out of stock at the moment) Also granite straight-edges,
24x2x4" accuracy 0.000050 per 12.  It would be possible to get decently
straight 1DOF motion by making a sliding bed which moves on delrin sliders
with preload pushing it against the straightedge.  We could also use a linear
encoder and belt drive.  The slider should have minimal runout, but would
experience drift of cross-axis position and along-axis orientation due to wear
on the sliders.

Uh, so then all we need is the other 5 motion axes.  I guess having even just
one really good axis would be valuable.  Although not exhaustive, this would
allow testing of the large scale linearity, and by putting source and sensor
through rotations we can explore the full translational workspace at good
accuracy, assuming only that the rotational positioning "stays put" during the
linear move.  Positioning accuracy is not going to be that great with a belt
drive (though perhaps within 200um), but we would know where we are from the
linear encoder.  With the encoder we don't need the toothed belt, we could use
eg. kevlar cable wrapped around a drum, which would be much stiffer

We could also use an air bearing, which would avoid wear.  With a square beam
and vee bearing arrangement this would be self-aligning, avoiding the problems
with creating side preload.


Another possible direction is that eg. McMaster has precision ceramic 1/2"
rods (+/- 1 mil dia, up to 12" long) which could likely be used with linear
bushing bearings or just reamed holes in delrin.  It would be hard to avoid
25-50 um side play, but this could be taken up by a preload arrangement.  The
ceramic rod would be super stiff, especially in such a short length.  To get a
big baseline we would really like the rod to be longer, but this could be
achieved by having say 3 separate parallel bearings which are the anchors for
a tripod carriage.  Also, since this is all non-metallic, it can be right at
the DUT, so there is no big moment on runout.  This reduces the need for a
large baseline.


What got me started thinking about the stage is that it is a requirement for
working on the general position calibration and metallic rejection algorithms,
which are some of the easiest tasks to spin off to a student.  Currently we
have not calibrated at all, so even a fairly crude capability would allow us
to make progress.  Even just a 4DOF system with the DUT on a pole sticking up
from the Rz XYZ stage, next to a surface plate.  We do need at least a
nonmetallic stand for the surface plate, and ideally one rigidly coupled to
the stage, with the assembly decoupled from the floor.  Perhaps the existing
optical bench would work for this; it is ferromagnetic, but there is bound to
be steel in the floor also.

See [30 Dec 15] re. how much metal causes effect at what distance.  I say
effect for large objects starts at about 18", which is ~500mm, perhaps where
my idea of the stage metal-free zone comes from.  Metallic interference drops
as r^6, so the difference between 500 and 600 is perhaps 3:1.  Based on that
data, 600mm is perhaps safe, but I would like to experiment with sensitivity
to various things, including the stage components.

Height of XY stage is 3.5", Rz 2.5".  Unfortunately, the full height of the Z
axis including motor is as much as 4x this.  Perhaps that can be reworked,
but for the moment, let's supposed not.  So we say the stack height is 24" or
600mm, and we need another 600m, so the DUT would be ~1.2m from the base
plate.  That's reasonable for as a work surface height (4').

On the plus side, the Z axis is in a position and orientation where it has a
smaller effect on the measurement than it might.  Generally the dimension in
the direction of the source-sensor ray has more effect, and effect is also more
when the metal is between the source and sensor.  Here, the field which would
ordinarily be sensed is not passing through the space occupied by the z stage.

[We could reduce the stack height significantly by approximately flipping the
z-bracket upside-down, so that the z-axis is farther off to the side, and the
motor extends down below xy table plane.  Then the post-holder would need to
cantilever back to the center from the side mounted z stage.  This extra
moment on the Z axis runout would not amount to that much in addition to the
24" that we need, and the torque load shouldn't be a problem, given the low
weight of the nonmetal pole and DUT.  This might buy us 6" in stack height,
and would get the big iron mass of the motor much farther away.

Or keep the brackets/plate roughly the same, but flip the stage to the
opposite side of the plate, so its "top" is away from the stage center.  The
post mounting bracket can extend straight up a ways to clear the body of the z
stage before it comes back to the center to line up with the rz axis.
Changing the mounting plates to eg. fiberglass should also help.  The z
brackets themselves may be far enough away to not matter.

Flipping may have problems with the z motor hitting the xy motors, tho.]

It should be OK for the center of the Z workspace to put the DUT about at the
table plane.  Somewhat above to allow for half-height of source + its fixture
plate).  Remember that we are planning to use three separate non-concentric
coils, which will make for a larger source assembly. If the pole is not made
adjustable it should be set up so that the tube section can be swapped out
easily (eg. don't glue into top/bottom ends).  I guess we can have some
adjustment just by using a clamp at the bottom (tube could extend down through
central opening in table).

We can get back to 6DOF by putting a remote belt-drive Ry axis on the end of
the pole (as in the earlier scheme), and also running in a sideways Rx rotary
shaft up on the surface plate.  The Rx could be attached to the surface plate
linear slider.  The shaft would need to extend out for 18" or so to get the
relatively small motor far enough away from the workspace.  The Rx axis needs
to be far enough above the table that we can rotate the source 180 degrees
without it bonking on the table.

This would let us test over the full angular workspace and let us compensate
for fixturing misalignments, and we can use the linear slide as a ground truth
test and as a way to test at longer ranges.

Probably the typical setup would have the working hemisphere of the source
extending out from the edge of the surface plate (along +y axis).  Then the
source would need to be close to the edge so that we can reach the desired
closest working distance without the post hitting the plate.  We would
position the stage y workspace so that our -y max doesn't hit the plate so we
have the full usable y motion range.  We could also work along the +x axis so
that we can use the table slider to explore longer working ranges, up to an
additional say +/- 8".  Assuming slider is 8" long so as not to extend off of
a 24" table.  I guess the Rx motor is cantilevered out by 14"?  OK as long as
the slider is heavy enough w.r.t. motor that the motor weight doesn't topple
it over (can weight with stone).

Also, because of radial symmetry we can make the null pose of the post be
off-center on the table.  This would let the linear slider get a longer run
away from the source, instead of having its motion centered, where it is
partially redundant with the stage x motion.  This might be awkward w.r.t. the
table support, though, since the stage needs to fit partially underneath the
table. 

Note that the 6" xy range is 150mm.  This would take us from r=50mm to 300mm,
which is probably about as big a range we want to try for micron use.  But it
would be interesting to characterize accuracy at longer distances.

The lower end attachement of the table support can use metal parts to attach
it to the baseplate, to brace it, etc.  300 series angle and strip is farily
reasonably priced.  Stiffness where the table supports meet the baseplate is
key because that is where we have the largest moment.  We don't want the table
to wobble much when we touch it.  I expect that because of where the stage
sits halfway underneath the table we will need to cantilever the table out
from two support posts.  There can be nonmetal angle braces on the top. The XY
motors can potentially bonk the support structure, but this all happens well
below 1' up from the baseplate, so there is plenty of room for angle braces
for the top.

I noticed that commerical surface plate stands use three-point support
to prevent torquing the plate and rocking.  This is probably a
gratuitous refinement, but might as well.  Rocking may be the bigger
issue, because even with no detectable rocking the stiffness may still
be much lower.  Likewise, the table slider can use three-point support
vertically, perhaps using three delrin (or Torlon) balls in sockets.
These would not be expected to roll (tho they might).  Torlon
(polyamide/imide) is about 50% harder than delrin.

Some of these structures, such as the slider and Rx motor support could be
made using CNC routed fiberglass sheet using slot and tab joining (with epoxy
and glass cloth reinforcement, or maybe Bondo fillets).


[5 Dec 18]

Took a dive into clock generation.  Got the Adafruit Si5351A generator board
working from Teensy.  Something like this may be useful for generating
nonstandard sample clocks (eg. not 60 Hz multiple), and also for tuning to
phase lock 50/60 Hz interference.

This particular chip is not set up for continuous digital tuning, though
another version Si5351B has an analog input tuning interface.  This is kind of
silly, but it seems that for many years the analog VCXO has been the standard
solution to phase-locked precision clocks.  

Another approach to tunable clock generation is DDS chips such as Analog
devices AD9954. This is a better solution for wide-range tunable clock
generation because it is simple to tune and high performing.  Somewhat
nonintuitive that it is beneficial to go through a sine wave to get a square
wave clock, but the analog antialias filtration provides an interpolation and
noise reduction function.

A major performance spec here is the jitter (or phase noise). According to AD
AN-823 you an expect phase noise of 5 ps down to even 0.7 ps.  High
performance tunable oscillators offer phase noise in this range, e.g. the
SiTime SiT3907 (mems resonator) is 0.6 ps.  But this part is not field
programmable.  The Si5351 says 3.5 ps typical, but that is likely with integer
divisor, which is really not a worst case. Hmmn, there is a lot of info
in ham circles about both Si5351 and DDS, with firmwares for Si5351 that have
presumably figured out how to tune it.  See 
https://groups.io/g/BITX20/topic/si5351a_facts_and_myths/5430607

Hmmn, Silicon Labs seems to have clock generators with jitter specs down to
70fs. Si5340 for example.  Also has glitchless tuning by design.  A much newer
part presumably (QFN package, etc.)


See LTC DN1013 "Effect of clock jitter on high speed ADCs".  Hmm, at least
with sampling adcs (presumably also applies to oversampling SAR), the effect
of phase jitter is pretty intuitive, causing sample window jitter, which in
turn converts the clock noise into amplitude noise which is proportional to
the dv/dt.  Having near-full-scale signals (as ILEMT does) is kind of bad, but
the signal frequency being so low is very helpful. 

Notably, clock jitter can cause generalized rise in noise floor in presence of
high level signals.  I don't know what the situation is with sigma delta, but
the ADS4222 datasheet says "For best performance, the master clock jitter
should be maintained below 40ps peak amplitude."  Note that is peak, if noise
is Gaussian, then RMS would be ~13ps.  This is not too demanding.  The eval
board has a Pletronics SM77H oscillator which has jitter of 0.6ps RMS
broadband and 2.5ps RMS close-in (10 Hz to 1 MHz).  This would seem to be well
better than the suggested design guideline.
 ==> The idea of giving a frequency domain definition (close in) to a
     time domain spec (seconds) seems a little weird. But one way of
     thinking about this is that there is 1/f noise, so the noise
     density will vary according to how much spectral bandwidth away
     from zero that you include. Jitter is not nominally a density
     spec, but there is a frequency domain character to it which is
     concealed by time units because jitter can only be measured in a
     recurrent test. If the frequency varies then the phase shifts and
     this creates integral non-white "jitter", more of a phase random
     walk.  This random walk is what you see in the close-in jitter.

According to eq (1), effect of jitter aperture noise on SNR is:
  SNR = -20log(2*pi*f*sigma)

where sigma is phase noise RMS in seconds.  At 13.5 kHz, 2.5ps, we get
-133 dBFS.  This is in the general ballpark of where we are operating.
Just comparing numbers, this is smaller than the PCM4222 noise
degradation, but this ignores any issues with FFT scaling, measurement
bandwidth, etc.  SNR should be defined w.r.t. a measurement bandwidth,
right?  I guess for an ADC the SNR bandwidth comes from the sample
rate.  Well, in particular, the SNR is not really a frequency domain
measurement, it is the ratio of signal amplitude to noise amplitude
(though getting the noise amplitude requires notching out the carrier
or turning it off).  So this is a much broader bandwidth than the
noise floor computed from the input FFT (at 7.5 s/s?).  This means the
aperture noise contribution to the FFT noise floor would be reduced
approx sqrt(4/24k), or -38 dB, giving -172 dB.  Where 4 Hz is
the bin width and 24 kHz is the Nyquist ADC bandwidth. Right(?)

If clock noise is at all an issue, then it is quite possible that
there could be issues with eg. EMI into the ADC board in the lab test
setup.

"Analysis and Modeling of Clock-Jitter Effects in Delta-Sigma
Modulators Ramy Saad, Sebastian Hoyos, and Samuel Palermo" is
interesting.  TMI, but it seems that in 2012 the best performing ADCs
(of all types) are up against the 1 ps clock jitter wall, especially
those with bandwidth > 1 MHz (lower rate high dynamic range devices
are under-represented it seems).

Note that clock phase noise presumably contributes at a similar level to DAC
noise.

I'm wondering if I need better test instruments to develop ILEMT, like a high
speed DSO, spectrum analyzer, mixed signal scope, whatever.  But we are not
really in the RF space, frequency lower, dynamic range higher.



[13 Nov 18]

I found a picture of the FFT for the CS5381, https://www.diyaudio.com/archive/blogs/googlyone/attachments/1702d1441537932-diy-audio-analyser-using-cs4398-cs5381-limits-these-ics-adc_cs5381_improved_v2_both-channels-.jpg

Maybe partly now that I know what to look for, but this shows the same pattern
as the PCM4222 of having a broad +/- octave hump surrounding the carrier.  A
bit hard to tell in the LTC2500-32 plots in the datasheet, but it seems that
the peak broadening is only +/- one channel.


[9 Nov 18]

I replaced the OPA227 on the on the right channel and switched to that (~4x
lower gain).  I reduced the gain on YZ channels to be -7 dB at 148mm (where X
is also -7 dB on the right input).

Under these conditions (driver on, etc) noise floor is:
	X	Y	Z
high: 	-119.5	-125.4	-122.6	
low:	-126.0	-125.3	-123.4	

With driver off:
-136.2	-126.9	-124.5	
-137.7	-127.5	-125.6	

So noise reduces approx 10 dB on X, only a bit on YZ.  

Driver on, but DAC outputs muted:
-130.3	-126.5	-124.3	
-127.9	-125.4	-124.6	

Note that by default the DAW fader on the mixer panel is set at -6 dB.  With
DAW fader at -26 dB, noise floor is:
-129.3	-126.3	-124.5	
-126.9	-125.9	-124.1	

So noise on X has increased ~ 1 dB from muted (peak X carrier is -27 dB FS).
At -16 dB fader:
-126.8	-126.3	-123.5	
-126.6	-126.2	-122.9	

 ==> Note that there is still the issue that all of these noise floor values
     are +3 dB wrt. the carrier level measurements from actual low rate
     demodulator due to issue with lower spectral resolution on spectrum
     plot.  But comparison between sensor axes is valid.

This data is consistent with channel X performance being limited by DAC or
driver noise, but is also consistent with ADC noise floor rising with
increasing signal.

Let's rearrange in order of increasing signal, with just high rate:

With driver off:
-137.7	-127.5	-125.6	

So noise reduces approx 10 dB on X, only a bit on YZ.  

Driver on, but DAC outputs muted:
-127.9	-125.4	-124.6	

Note that by default the DAW fader on the mixer panel is set at -6 dB.  With
DAW fader at -26 dB, noise floor is:
-126.9	-125.9	-124.1	

So noise on X has increased ~ 1 dB from muted (peak X carrier is -27 dB FS).
At -16 dB fader:
-126.6	-126.2	-122.9	


-6 dB: 	-119.5	-125.4	-122.6	
-16 dB:	-126.8	-126.3	-123.5	
-26 dB:	-129.3	-126.3	-124.5	
muted:	-130.3	-126.5	-124.3	
off:	-136.2	-126.9	-124.5
unplug:	-154.8	-126.8	-126.2	

Looking at the spectrum in linear frequency, it seems that there is a broad
noise hump on X +/- 1 octave around whichever source channel is strong.  I can
make it move by rotating the sensor around the axes.  Since this only happens
on sensor X it seems like it has to be related to the input chain or ADC, and
adding noise proportional to input amplitude is not a typical analog behavior
(so it is presumably ADC).

Huh, now with eval board input unplugged I am getting (added above):
-154.8	-126.8	-126.2	
-151.9	-126.3	-126.2	

I don't seem to have recorded noise floor with the ADC disconnected, and there
was the 2x gain tweak also, but IIRC it was about -140 before the gain tweak
(-136 after).  This is 20 dB lower???  What is true is that this is a
different channel, with lower gain, and the correct CM voltage buffer chip.
But the gain should not matter because the amp runs at unity noise gain when
the input is floating.  Yet the really odd thing is that is that because of
the hack in decode_tcp_buf.vi I am picking *whichever* of L/R channels has the
*higher* mean absolute value.
 ==> Possible theory: ADC performance was being degraded by the missing Vcm
     buffer on the right channel.  We could have been driving the unused input
     negative, which can have bad effects on chip operation.

Uh, OK.  I added capability to force which ADC channel we are using, and we
are definitely seeing something different since the move to right channel and
replacing the Vcm buffer chip.  When either is unplugged it goes down to about
-155 dB.  And, also as table above shows, the noise jumps a lot when the
driver is on.  Let's retry part of the above, since the numbers are a bit
different now.

-6 dB:	-119.1	-125.8	-124.2	
muted:	-131.0	-126.5	-125.6	
off:	-139.9	-127.2	-126.3	
unplug:	-154.9	-127.1	-126.3	

Yes, that's pretty repeatable.  Also, the 2 Hz ripple in the high rate output
is gone.  Which is nice.

The thing is, this is mostly consistent with the X ADC having a lot lower
noise, and the noise floor being mainly from DAC+driver and EMI.  Except this
doesn't explain why the X noise floor becomes maybe 10 dB higher than YZ once
everything is going.  There is this big hump in the noise floor centered
around 7.5 kHz.  If I mute only X, then the hump goes away, and we are about
the same as all muted.  But if I rotate sensor so that sensor X is aligned
with source Y, then it's about the same as in the base all-on config.  A few
dB worse, FWIW:
    -116.0	-126.4	-124.5	

A possible extenuating factor is that it looks like a lot of broadband noise
is making it to the preamp output.  I seem to have set the output LP pole at
110 kHz (I guess to avoid CMR degredation due to impedance mismatch), and
there is a lot of crap out there even to 1 MHz (without the driver running).
Also, the lowpass pole in the ADC driver is at 590 kHz.  Now, in theory, we
are not in deep doo doo for aliasing until 256 * 48 lHz or 12.3 MHz.  At that
point the attenuation from the 590 kHz is about 20x, or -26 dB.  Possibly crap
from the driver could be being coupled in somehow, but the hump effect depends
on the overt signal path since it moves when I rotate the sensor.

I added 15 nF caps for C52,C53 which should give a driver pole at 39 kHz.
This does not seem to make any difference in the noise hump behavior.  Looking
at the ADC input signal with the 7A22, there is noticeable extra HF between 30
kHz and 1 MHz bandwidth, but the p-p amplitude does not really increase.  This
is consistent with the HF noise rolling off.


At low signal there is some SNR advantage, though the floor is still non-flat,
making it hard to summarize by a single number.  At 330 mm peak carrier is -27
dB.  Then the noise floor is:
     -135.6	-127.1	-125.9	

So there is an 8 dB advantage.  Break-even is at about 190mm, -14 dB.  

I looped the DAC output directly to the eval board input, and got this:
	-122.3	-127.2	-126.6	

When level was adjusted to -7 dB on the X carrier.  This is a few dB better,
but there is still the pronounced hump.  So what it seems is that when there
is very little signal, then the noise floor is 20 dB or more lower than on the
UR44 ADC, but when there is much signal at all then the noise floor rises
above the UR44, and the distortion is perhaps worse also.

I tried changing the decimation filter response between classic and low
latency.  You have to reset afterwards.  It did not affect the noise floor
hump.

Mmm, tricky.  I was just looking at the FFT plots in the PCM4220 datasheet,
and the ones with signal present have it at -60 dB.  At *that* level it shows
no increase in the noise floor.  The CS5381 does not have any analog
characterization plots at all (only digital filter response).


Comparing the UR44 in the same loopback arrangement, there is minimal change
in the noise floor, and the only noticeable spurs are IMD with the low
carrier, at about -98 dBc wrt. -7 dB carrier.

What there does seem to be with the loopback is a noticeable increase in LF
noise.  Looping back X output to the Z input (which is one of the line level
inputs) I get:
	-125.5	-128.4	-123.5	
	-125.5	-129.0	-118.8	

So we are looking at the last column (Z).  

I had to reduce the input gain knob to min, and then reduce the dac output
slider a bit too.  So that at the 270 Hz low carrier, the noise floor is
increased about 10 dB above the floor on the unconnected channels.  This LF
boost is most noticeable on log frequency axis.  This would appear to be from
the DAC or output buffer.  It does not change when the output is muted.

So the conclusion seems to be that the PCM4220 is not really better, though
the reducing noise floor kind of behavior would help to tolerate signal
variation.  I need some kind of writeup product for the report showing the
hump behavior.

In hindsight it would probably have been better to start out in loopback mode,
although there were still all the issues with fixing the hardware and getting
the software data pipe set up.

The low noise floor might be some help for test purposes in evaluating the
signal chain, but the way that the noise humps up under unknown conditions is
awkward.  It doesn't really have a higher dynamic range that the UR44 in terms
of peak to floor.  And for evaluating noise in the absence of peak I can just
add gain.

I still have not untangled the noise contributions from the different parts of
the signal chain.

Look at just this column (from above)
-6 dB:	-119
muted:	-131
off:	-134
unplug:	-155

It seems we can infer from this that the driver noise floor is -131.  This is
only 5 dB below the UR44 noise floor (-126 dB).  And then the preamp/EMI floor
is only 3 dB below that. So if we could solve the ADC and driver noise we
would still hit the preamp/EMI floor at only -8 dB down from the current
performance. Well, the LT1028 in the preamp is not quite optimal for this
sensor.  And I really do not know currently how much of this noise may be from
the preamp vs. ambient.  But the sensor side SNR can be arbitrarily improved at
some cost in workspace, so the "best possible" performance still depends on
the ADC dynamic range (and DAC/driver noise).  However in this test I was
already operating at the closer 148 mm source distance.


I was wondering if how I had tuned the driver output filters to be resonant at
the carrier was increasing the noise at that frequency.  That would not be
good.  But does not seem to be related to the noise hump effect.


The OPA1632 used for the ADC driver is pretty appropriate 1.3 nV/rtHz, 50
V/us, low distortion.  We are operating above the slew rate where the
distortion starts to increase, but not sure how to evaluate this.  The
degradation of THD+N on the pcm4220 datasheet above -20 dB input is perhaps
mainly due to distortion (though that is what led me to wonder if noise might
increase).  If so, at our operating point of a few V output at 10 kHz, the
OPA1632 should contribute about 0.0001% distortion (at unity gain), which
compares to the -110 dB for PCM4220, which I reckon is 0.0003%.  So the
OPA1632 should contribute negligibly to distortion.


Probably not significant, but I just noticed that the residue has added noise
in the stopband of the lowpass filter, and also notches that sort of resemble
the notches in the residue created by the demodulator passbands.  This happens
even when all the filters (except LPF) are turned off.


[8 Nov 18]

We are getting differential drive through to ADC (despite single-ended
connection from preamp), and we are hitting the wall at about 5.5 VPP input.
This agrees with the datasheet.  Overflow light does not come on visibly until
rather above that level.

FWIW, with the input LPF off there is a noise burst train at around 23 kHz on
the eval board.  This looks like the beating between a spur forest at 22.7 kHz
and another peak at 23.65 kHz, giving a ~1kHz burst train.

Before I take any more levels I am going to adjust the ADC scaling.  It looks
like the output is shifted by 1 bit, likely another driver configuration
issue.

Now the HF noise floor with eval board input unplugged is -134 dB
r.e. (actual) FS.  This is with DAC driven but driver off.

For (unequal) comparison, level on YZ with sensor unplugged is about -126 dB.

I fixed the "Trace display" mode in the levels controls, and factored out the
noise floor computation, now displaying it on the front panel.  Interestingly,
finding noise floor as median seems pretty insensitive to RMS spectral
averaging (increasing perhaps only 1-2 dB with 20 averages).  This is also
visually in good agreement with the display.

 ==> The sqrt(20) discrepancy in the spectrum display scaling may be due to
     the 2x different FFT size (?)  This is what broke the "Trace display" mode.
     Uh, yeah, and now the trace display levels are off by 3 dB.

FWIW, even on X, with the DAC outputs on but the driver off the X-X carrier
level is +10 dB w.r.t. noise.  For YZ it is more like +15.  The latter makes
more sense because the DAC and ADC are in the same box.  I guess driving the
signals out to the amp (even off) is injecting ground currents that couple
into eval board somehow (preamp is connected to audio interface ground through
the sensor adapter box).

I need to reduce the gain a bit to make X match the other channels.  Perhaps I
could reduce the gain even more, and then reduce the gain on the other
channels too.  This could allow me to work closer to the source, perhaps
getting higher SNR.  This should get us out of the EM noise floor.

It seems pretty clear that the X noise floor does rise at high signal levels.
This was one of my concerns from the datasheet.  Even so this is a
not-entirely-terrible behavior when in the real world we have to deal with
signals that are not always full scale.  But the UR44 does not do this.

When I short the eval board diff input it creates a LF noise boost below about
1 kHz.  I guess this is boosting the noise gain of the input diff amp.


OK, feeling the need to be a bit systematic here.  First of all, I want to
disambiguate:
 -- Noise from ADC
 -- Noise from preamp and sensor impedance
 -- Source noise
 -- EM noise


Also, I should evaluate the eval board w.r.t. the specs.  This is worthwhile
to help understand how specs mesh with our requirements.



[7 Nov 18]

I'm thinking that the glitch might be due to the I2S output clock (from rpi)
not being synchronized with the ADC master clock.  This is a more specific
understanding of why running the ADC in slave mode could be a problem.  This
would be consistent with the effects that I see near the glitch (pre/post
shoot and almost-duplicated sample), which would have to take place in a DSP
context (could not be caused by simple sample duplication).  If the serial
port is running a bit faster than the ADC, then it will step from one sample
to an earlier one as the exernal clock edge drifts past the onboard clock
edge.  Possibly this particular ADC makes this more of a problem.

Anyway, it seems getting rpi to be slave is the priority.

arecord -D plughw:0 -c1 -r 48000 -f S32_BE -t raw | nc 192.168.1.35 6666 &

OK, I made the ADC master, and am getting some kind of data, but the format is
messed up.  It seems certain there is some way to configure this using device
tree, but I'd rather not have to learn that.  Just looking at which bytes are
zero, it seems like we are getting both left and right channels packed into 48
consecutive bits with zeros bringing it out to 64 bits.  If I am right, then I
can pull out the right bits.  Unfortunately where the stream starts (L or R)
is also nondeterministic.

Figured out the format, it seems that it is 32 bit aligned data, but the left
and right channels are interleaved, with no particular order.

Now things are kind of working.  No glitching anyway.  But there is a pretty
big ~0.5 Hz ripple in the output which seems to have to do with the drifting
phase of the clock (and non-synchronous sampling).  Results are still terrible
with rectangular window.  The sampling drift is about 8s for 360 degrees,
vs. 2s ripple period.  Perhaps that still makes sense somehow.

With the driver off it does seem that the noise floor is about 6 dB lower on
the X channel above 10 kHz.  But then with carriers present the X noise floor
goes up about 6 dB so that the HF noise is about the same and the LF is
worse.  In what seems rather a cooincidence, the X gain is pretty much the
same as Y.

There are some kinds of connection flakies with the preamp setup.  Messing
around with shorting the two input BNC with a short cable.

I think the hum pickup is happening on the input side hookup, since it is
there even with the shorting cable, and changes a lot as you bend
it. (magnetic pickup)

With input shorted then noise floor above 10 kHz is about -140 dB, which is
about the same as with the ADC disconnected from the preamp.  With sensor
connected, X noise is about the same as on YZ above 6kHz, about -125 dB.  Not
seeing the noise increase below 10 kHz that was there before.

Ug, seems to be a scale factor problem?  Because the ADC starts overloading at
around 0.5 FS.(?) So that's even more bad news, because the noise floor is
actually 6 dB higher w.r.t. the real FS.


[6 Nov 18]

Have TCP streaming of the ADC data from rpi sort of working.

arecord -D plughw:0 -c1 -r 48000 -f S32_BE -t raw -V mono | nc 192.168.1.35 6666

There is a significant mismatch in sample rate between the RPI and the UR44.
The phase is drifting 10's of degrees/sec.  This may well be the known problem
with rpi being an i2s master, that it chooses a nearby sample rate which is a
clock multiple, rather than synthesizing a precise rate.  I guess I need to
try making the rpi I2S slave and use the oscillator on the dev board.

Even if the nominal rates match we are evenutally going to get out of to a
large degree, but I hope this is not going to be a big problem.

Also, from what I can see so far, noise and distortion are not improved.
There are these big 500 Hz spurs, and as long as the sensor is plugged in the
noise floor is perhaps even higher.
 ==> Note that the LT1028 is not optimal for the RFID sensor.  Probably a
     low-noise JFET would be better, or just a lower current
     bipolar. Inductive reactance of XY axes is 282 ohms at 10 kHZ.  But we
     are not so far off that this should make a big difference.

It is hard to really characterize the noise with this unsynch glitching going
on every few seconds.  I changed the labview to not block waiting for the
remote data, so we are no longer getting overruns from ASIO.  We *are* getting
some overruns in Linux.  I think what is happening is that the rpi sample rate
is lower (?).  The glitch every ~5s seems to involve phase shifting by
one sample, but in between phase glitches the phase is drifting negative by
about three samples.  If my calculation is right this does not correspond to a
huge frequency error.  An error of 1 s/s at 48k s/s is only 20 ppm, which is
the ballpark of crystal tolerance.

But the glitching is really what is a nuisance, not the phase drift.  The
glitch does seem to be something like a single inserted sample.  At least in
one case this is almost a duplicate.  This effect is much clearer when the
input LPF is off.  It is still possible this is some sort of sample rate
fuckery on rpi.  Another time it could still be a nearly duplicated sample,
but there are also before and after glitches on the next and following carrier
cycle.  This one is not showing up on the spectrogram display. (?)  That
worked once, IIRC, but you can temporally localize using "Level high X".

Here are the times of three sucessive glitches, captured and replayed offline.
    Block 16 + 0.00533 s (sample 256)
    Block 75 + 0.01067 s (sample 512)
    Block 134 + 0.01933 s (sample 928?)

The interval is 59 low rate blocks, plus a bit.  It doesn't seem quite
consistent, though the meta-glitch might be due to an overrun or something.
If it is 4k + 256 samples, that is 5.04 s.






I guess if the phase is lagging (measured frequency is low) then what this
means is that the sample rate (on rpi) is *high*.  Stuffing an extra sample
every 5s is not helping.

The approx 500 Hz spurs seem to have to do with the low carrier presence.
This may not be any worse on the eval board signal, it just happens to have
the highest level.  Also, using no window is failing badly on this channel.
This is probably due to non-synchronous sampling, but there is also some DC
offset.


[26 Oct 18]

Looking at updating the preamp design.  We need more gain somewhere.  And the
332 Ohm/10 nF RC at the output is loading down badly with the ~400 ohm input
of eval board.

Also the input differential filter cap is badly mistuned the RFID antenna.  x,
y are approx 4.5 mH, 50 ohm.  z is 7 mH 75 ohm.
 ==> Maybe not?  I was thinking there was some instability due to this, but
     the resonant freq works out to 41 kHz, which is about right.  Could be
     even lower.  The damping resistor RD seems way too small, though.  I
     forget my theory with the input filter and output filter.  Did I add the
     output filter just to suppress input ringing?
 ==> Response to impulse seems well damped.

I reworked the output to use the capacitance decoupling thingie (-feedback C,
series output R, return feedback resistor to load).  And got rid of the output
C.  Now it does not load down.  Have not seen the thing I thought might be
instability with messing with the inputs.  Perhaps it was just bad contact
improved by remating.

I increased the gain on the eval board left channel from what it was (~0.45)
to ~1.73 (changed input R from 560 to 156), amost a 4x gain increase.  This
gets us in about the right gain ballpark.  I can now get ADC overflow
indication if I move closer than 200 mm.

The unbalanced output from the preamp to eval board is not working very well.
It seems like there is CM pickup not being rejected which is greatly helped by
grounding the preamp to the scope.  The best solution would be a diff output
from the preamp

I think I can get away with not writing any linux code to stream readings over
TCP. Just use arecord piped to netcat (nc).


[25 Oct 18]

I2S pinouts for Raspberry Pi.

40 pin		chip pin	BCM name	evm name	evm pin
n/c						SCKO		1
12		GPIO 18		PCM_CLK		BCK		3
35		GPIO 19		PCM_FS		LRCK		5
38		GPIO 20		PCM_DIN		DATA		7
40		GPIO 21		PCM_DOUT	n/c
6,9,14,20	GND				GND		2,4,6,8,9,10
25,30,34,39	




[13 Oct 18]

Ordered eval board for TI PCM4222, investigated I2S support on RPi, and found
that it was integrated as a linux sound device.  Ordered SD card.  Need some
cabling to connect eval board, can use stray +5 +/-15 supply.

Also ordered some extra OPA1632, which are fully differential audio amps, as
on the PCM4222 eval board, and 2:1 mux parts.  For experimenting with input
stage.  A disadvantage of the fully differential topology is that it resembles
an inverting amp in having a low input impedance.  This is a problem either
when looking into the sensor itself or into a resistive voltage divider.


[11 Oct 18]

Poking at the preamp and gain switching problem a bit.  The high-speed (100's
of MHz) fully differential amps are just not relevant.  Looking at one, the
distortion starts increasing below 100 kHz.  There are a few special fully
differential amps for just this purpose, but they require matched gain set
resistor pairs.

PGAs are not so attractive either.  Very few have differential outputs, often
they have weak AC specs (aimed at LF data acquisition).

I was thinking that maybe the thing to do would be to use two diff gain stages
with an attenuator in between, based on a 2:1 analog mux selecting either full
gain or attenuated.  Using a mux feeding a high Z input makes the
on-resistance pretty unimportant.  I looked a bit at digital pots, which often
use this exact architecture, but was not very enthusiastic.  With more taps
there's a lot of capacitance from many open switches in parallel, resistances
are high and poorly controlled, tempco is pretty bad.

You could do this at the input, but then you compromise the CMR of the gain
stage.  I suppose you could do both places.  Also, when the attenuator is out
there is no CMR effect, and at high gain is presumably when CMR is most
important.


[10 Oct 18]

Beaglewire is a really small device, not even the largest in a small
family. There is no clear path to larger devices.

So far as FPGA goes, there does not seem to be much in the way of open-source
toolchain, so I pretty much need to pick a particular board/chip and target
that.  

It seems that the term "SOC" is being used for any single-chip
solution whether it has usable programmable logic or not,
eg. beaglebone itself.

One consideration is that the hardware is always a moving target. New dev
boards are introduced to promote a new chip, and then are dropped when the
next new chip comes along.  In this regard, things like Raspberry Pi and
Beaglebone are attactive in that we can expect that the same or highly
compatible hardware will be available for some time.  That said, RPi seems to
be >10x more popular (eg. from google hit estimate), and beaglebone is not
moving all that fast.  The Beaglebone black came out in '13.  There is the
BeagleBoard-X15 which has relevant capabilities (DSP), but it isn't clear that
it is going to develop enough of a community to be more than just another
development board.

It does seem that the RT Linux ecosystem is forcing a degree of uniformity on
these boards at the API and source compatibility level.  This means that you
are not going to be entirely stuck with a board or a chip vendor unless you
depend on the chip's proprietary hardware.

At the moment the Snickerdoodle board based on the Xilinz Zynq seems pretty
attractive.  But they are having problems shipping hardware using CrowdSupply.
And given that, the software support is presumably somewhat rough.  Even the
formatted boot SD is back-ordered???.

Chaining ourselves to Xilinx seems pretty OK, since they are the biggest FPGA
vendor. So we can target their proprietary features like DSP blocks and block
RAM.  

I didn't see any sign that the LTC/SocKit Linux was available in source form,
which would reduce the interest of possibly developing code on the SocKit.  

Uh, also, there is xilinux, which mostly seems to target Zync boards, but has
obsolete support for the Arrow SocKit (use by the LTC2500 eval board), which
is an Altera board. Ohh, Intel owns Altera now.  That clears up some confusion
about "Intel" SOCs, etc. I am thinking that the SocKit is already obsolescent.
Targeted boards are Z-Turn Lite, Zedboard, ZyBo and MicroZed.  Xilinux is hard
to find from the Xillybus front page, suggesting they aren't too much behind
it.

Possibly xillybus is interesting, since it is a supposedly easy-to-use high
performance interface from FPGA FIFO to linux pipe.  Xillybus core is
available for Xilinx FPGAs too.  It is also available free for evaluation and
academic use.  FWIW, GNURADIO SDR is using Xillybus for transport.  Some
people seem to be using xillinux just because it is there for eg. zedboard,
and not because they are using xillybus.

The ZynqBerry is another hacker Zynq board, this time with a RPi form factor.

The Avnet MicroZed is another Zynq board (supported by xillinux). This is a
small build-in module, looks a bit more expensive than the hacker boards
($200), but viable.  It seems that the Zynq ecosystem in general is fairly
active, since this part is out of stock, but has orders due to arrive.  It may
be that Zynq parts are in low supply.

PetaLinux is something-or-other supported by Xilinx and available for free.
It seems to be tools for generating boot images for Zynq.  Also it seems to be
based on the Yocto thingie for generating custom embedded linuxen.  This is
maybe a bit heavier-weight on the configuration side than what I would really
like (though it will presumably generate the smallest memory footprint).

See also https://github.com/bmartini/zynq-xdma, a simple thingie for
user-space DMA access on Zynq.  Sort of like xillybus.

RPi does have two SPI bus, so it would be possible to talk directly to ADC
(but not offload any DSP to FPGA).  Speed seems pretty anemic.


[9 Oct 18]

Spent some time looking at beaglebone. This seems like a plausible platform
consistent with open design. There is a beaglewire FPGA shield, but it may not
really have enough I/Os.  Probably the ultimate solution would be to use the
beaglebone as a daughter card, with the main board acting as a cape, and
ideally reusing as much as possible of the beaglewire software
infrastructure.  I would like to minimize the effort spent in custom linux
device drivers.

Integration with micron would presumably be by streaming pose data over the
ethernet, with Micron still running on a Labview PC.


[27 April 18]

Looking at analog devices converter lineup.  They have some interesting
products, like AD7767 for oversampling SAR architecture, and their evaluation
board prices seem to have become extremently reasonable.  AD7763 for
sigma-delta.

Oh ho!  Analog devices has bought LTC.  That reduces the search space, I
guess.  And LTC2500-32 is a newer oversampling SAR part which has the
interesting feature of both full-speed and decimated outputs simultaneously.
The full-speed might be useful for diagnostics and characterizing
interference.  With SINC4 filter and 16x oversample, -3dB BW is 14 kHz and
dynamic range 118 dB.  Also has flat filter which would be possible, though
group delay is a bit high at 17 samples/280 us.

As noted before, fully differential amplifiers are worth considering.  They
are broadband and low voltage noise, though intended for higher speed use (and
so power hungry).  One thing I hadn't thought about before is that some have
digital PGA functionality, which would be a big help. There are also
instrumentation amps with digital gain, but for high performance ADCs we want
the differential output.

I looked at ADCs with integrated PGAs, but wasn't too excited.  This is mostly
a feature in multi-channel multiplexed analog front ends, which is not really
want I want.

The LTC6602 is a weird application specific part aimed at UHF RFID, but that
could be relevant.  As well as programmably gain, it has matched channels and
builtin filters.  Its description is as a filter block, but I found it when
looking at PGAs.  Not sure if it would be usable, since it is designed for
center frequencies >100kHz.



[4 July 17]

What new work is proposed for interference rejection aim?
 -- Likewise for EM, we can implement ANC based on instrument noise pickup,
    and pickup from redundant low-dynamics sensor.

Any specific interference ideas conceived of but not yet done?
 -- Hum ANC doesn't actually work, know how to fix now.  
 -- Phase lock clocks to line, or software emulation in hum filter.
 -- General ANC on output, perhaps with reference inputs.

Looked at Unscented KF again.  Was a little fuzzy on how you use a nonlinear
estimator to solve an inverse problem, but basically what it comes down to is
that the "observation function" in an estimator is precisely the "forward
model".  I had sort of forgotten that the innovation is formed in the input
coordinates, not the state space, so the KF framework flow does not make any
assumptions about inverting the observation function as long as there is some
other way to compute the Kalman gain K, which projects the innovation into the
state space.  In the UKF this is done using the covariances (much as in
standard KF), but the covariances are computed by the unscented transform
trick.

I think that what you end up with is basically gradient descent, which is not
the fastest or most robust way to solve nonlinear problems, but the theory is
that all is forgiven because you already have a solution which is almost
exactly right. Experienced KF users may realize there can be a problem with
initializing, but it seems this is not typically a big problem for EMTs.


Looked at the old interfere_plot.m phase plot from the 1DOF metallic
interference tests. It isn't particularly encouraging about use of phase,
which is I guess why we aren't showing that plot.  I should redo this using an
actual sweep and the current setup because I'm suspicious that the lack of a
clear LF -> HF phase trend is due to the data being taken at different
times. There were also a lot of problems with the phase recovery (complex
magnitude) in the demodulator, especially the sign.

It does seem generally plausible that eddy current causes phase shift,
whereas ferromagnetic not so much.  So phase is a better indicator for
ECI (Eddy Current Interference).  Phase shift is lowest at frequency
where FMI (FerroMagneticn Interference) is highest.  The AL1 phase plot
makes no sense, though conceivably this is an obscure effect of
asymmetry.

My theory that the leading phase of the Sciss response is due to being a hard
ferromagnet is kind of a wild guess.  Could be true, but could possible have
to do with the shape also.  It does seem we need some explanation about why
Sciss has a leading phase response, but I expect (and IIRC, I observed) that
the phase lead/lag depends on the position of the interferer.  I definitely
saw that the position could appear nearer/farther for the same interferer,
depending on location, which is really easy to understand.


[3 July 17]

Thinking about a source design with low dipole error, inspired by Paperno
'04. They don't consider ferrite core, but since a similar aspect ratio holds
for permanent magnets I would expect that there is a similar error
minimization that can be done based on varying the aspect ratio and perhaps
ferrite ID.  This should even be doable in FEMM.  

What this means is going to a non-concentric design with three (or more)
separate coils.  I realized that this was really quite favorable for our
open-source model because it's easier to reproduce, both manually and
industrially. 

However, since we need three separate coils, to first order we expect almost a
3x increase in active source volume and a > 3x increase in largest source
dimension.  I think this can be mitigated considerably by allowing the source
coils to be arranged in an application-dependent fixture.

Best aspect ratio seems to be around 0.8 (0.87 for PM, as low as 0.72 for a
many-layer air-core).  To first order this is "square", though that we are in
the short-fat region is usually more convenient.  

We will definitely have better thermal performance due to each coil getting
its own surface area, and presumably somewhat better electrical efficiency due
to no eddy-current losses in the off-axis coils. Perhaps we could take our
current 50x50x55mm core (137e3 mm^3) and turn it into three 45x35mm DxH
cylinders (56e3 mm^3 * 3 = 167e3 mm^3). That's pretty optimistic, I'd guess we
need at least 2x the coil volume.

But for e.g. eye work, you could envision these being on a frame under/around
the head, where the coils are (more or less) orthogonal, equidistant from the
workspace, and all pointed toward it.  (?) Not sure this arrangement is
actually numerically favorable for pose solution, though it does give a higher
signal on all axes vs. the concentric design.

I think this idea might also work for a "magnetic mirror" shielded panel
design where all sources point straight up.  The main idea is to preserve the
axial symmetry of the coil.  I'd expect then that the ideal aspect ratio would
be reduced by the apparent reflection caused by the shield plane, which is
desirable, since you don't want your panel to be too thin. Maybe a vertical
sandwich like: coil, ferrite tile, mu-metal, aluminum.  The ferrite avoids too
much loss in the other layers.  Perhaps copper + mu-metal would work too
(thinner/cheaper), with maybe a spacer between core and copper.


________________________________________________________________
Older stuff:

Power analysis: https://www.statstodo.com/SSiz2Counts_Tab.php

 ==> Compare tracker outputs during bunny, cadaver trials.  Flag periods of
     excess disagreement and tracker error status.  Characterize occurrence
     rate and duration. Correlate with video recording and notes to determine
     at-fault sensor and any likely causes of tracking failure (blocked
     sightline, EMI from equipment, etc.) 


Where is failure identified:
Engineering characterization, lab human tests, integrated on instrument, onsite.

Time-varying performance: interference bursts and dropouts. I looked up
stationary processes, but a top answer said that it's more for econometrics
than the real world (suggested frequency domain methods). Anyway, we know
error process is not stationary.  The trick is to identify periods of
degradation and quantify them.  This is not so tricky given the two data
streams.  We just need to characterize the change in the pose difference.  For
example, we partition into offset and noise components, analogous to tracking
change in the mean and variance.  More generally, we could look at energy in
different bands using a time-frequency approach like STFT.  This is uglier for
Navio, when comparing high rate high res to low rate low res.  It is of course
hard to know what to make of changes that can't be resolved by the low-res
sensor. These might be below the noise floor of a difference signal, at least
in the time domain.  One approach would be to use system identification to
model the response of the low-res tracker, then use this model to predict the
low res output from the high rate. If there is discrepancy there, then we can
say this is a real disagreement, not just different sensitivity. We would also
expect there to not be too much ILEMT output not explained by the low res
tracker, that is the HF content would not have huge amplitude.  But we
certainly will see some mechanical vibration, tremor, and so on, which may go
unnoticed below the low-res noise floor.


[30 June 17]

Looking at SNR issues again.  Commercial audio interfaces may be reaching -120
dB in the real world.  As spec'd the UR44 is -101 dB, and as measured in
loopback, only -94 dB.  So it seems highly doable to get a 3x = 10 dB
improvement, and 10x or more may be possible. Not to trivialize the problems
of high-performance audio design, but I think we have some slack here due to
not trying to make a general-purpose audio interface.  For us the noise floor
is a much bigger problem than distortion or some sorts of narrowband
interference, and our analog front end needs are pretty basic, so we can drive
down noise there. Saw claim that many audio interfaces spec directly from
converter datasheets rather than actually measuring.

PCM4220 is still top of the line for TI. Maybe I understood this better at
some point, but I can't recall exactly how dynamic range is defined.  In
particular, looking at FFT output, the PCM4220 noise floor is around -150 dB.
Oh, right, as you increase the FFT size the noise floor keeps going down
forever because the bin bandwidth decreases.  Dynamic range or SNR for audio
are typically based on A-weighting, which is a bandwidth of about 10 kHz
(though going down lower than we care about).  So in 1 kHz BW we would expect
about 10 dB better dynamic range.

It seems that audio ADC top of the line dynamic range has not really been
increasing. The 120 dB ADCs are niche, though there are high end products that
are squeaking up higher.

The non-audio region is also promising, and seems to be more active in pushing
dynamic range.  Because of the use of A-weighting in audio we need to be
careful with comparisons.  For e.g. LTC2380 bandwidth is apparently Nyquist
limited, so far higher. 112 dB @ 2 Msps with 16 decimation, 125 kSps output,
to compare 62.5 kHz BW to 10 kHz, add 16 dB, giving 128 dB. Also, THD seems
quite low, -117 dB = 1.4e-6.  Audio ADCs quote THD+N, which can't be compared
directly.  Another difference is that the LTC2380 FFTs are on a linear scale,
so they can't be directly compared.  We can't see the LF noise here, but we
don't care about it that much anyway (though we do have the low carriers).

My feel is that we should see what results we actually get with the
oversampled SAR ADCs because there may be pleasant surprises related to
reduced IMD and noise flatness.  For dynamic range we do also have to worry
about the DAC+driver.  This could push us toward digital output, either
direct to the driver (as discussed long ago), or to an audio ASIC class-D
driver.  Lots of digital-in amps are targeting small low-end devices, but TI
for example has a bunch of >50W options.  One thing is the PWM-input
amplifiers.  See for example TAS5631B.  


[15 Jun 17]

Current round of work triggered by getting back up to speed on EMI rejection
to write paper.

One thing I'd like to try out is how well it works putting the filters
on the demodulated outputs.  It doesn't seem that we really need them
to get a residue clean enough for the impulse filter to work, since
even impulses way above the residue noise floor produce only just
noticeable output disturbance.  This should certainly be more
efficient, and I think might also give better hum rejection in the
face of non-line-synch sample clock due to the relative frequency
resolution being lower.  In the current location the hum filter does
not seem to suppress output hum very well. [Also because hum may fall
within the channel passband, so not appear in residue?]

Found the explanation for why we are doing distortion filter the way we are,
from [29 Dec 15].  If the filter runs on residue it can only cancel stuff that
is *not* passed through to carrier model (and hence to output).  Looking back,
I left the hum filter attached to the residue, and didn't comment on it, so I
think I didn't realize the crucial broader implication: running ANC on the
residue cleans up HF stuff in the residue, but *precisely* the stuff that we
don't care about (that isn't passed through to the output).  This is why the
hum filter cleans up out-of-band hum nicely, but does not reduce output hum.

One advantage of running ANC filters on the output is that we don't have to
worry about notching the carriers (except at DC).  Originally I had hoped that
by doing ANC at input sample rate I would somehow avoid cancelling genuine
periodic motion.  After more experience with Fourier domain processing, and
especially after reading up on sub-band filter theory, I realize that this is
not so.  We can still get away with this filtering without disturbing the
output too much on the theory that strong suppression only happens for signals
that are closely phase coherent with the selected repeat rate (this is one
advantage of only cancelling certain periods, rather than general ANC).
Unfortunately genuine line-synched mechanical vibration can exist, and we will
end up notching this out too.
 ==> Hmmn, also we don't need FFT filter at all, since we can notch out DC
     from the signal model just by subtracting the mean.  So a really big
     efficiency win.  Then we can't do any limiting in the frequency domain,
     but I think the FD limiting came about in the first place because it was
     compatible with carrier notching, rather than because it was otherwise
     desirable.

However, if the high carriers were located off hum harmonics then we could
frequency shift EMI hum away from DC.  If we offset the carrier by say 25 Hz
then EMI hum would appear as 25 Hz and 35 Hz harmonic series which could be
distinguished from genuine 60 Hz vibration.  But for stabilization, losing
signal at 60 Hz is less problematic because we can't cancel worth a darn at
such a high frequency anyway.  We can't do the offset thing anyway without
reclocking the ADC.

w.r.t. mystery of why distortion filter is helping noise floor, a story which
is perhaps complementary to the "noise shaped IMD" theory is that the ADC
output contains pseudo-noise, a component that appears noise-like but is
repetitive.  This is why the energy is smeared out in the noise floor rather
than being concentrated into noticeable spurs.  If this is true, then FFT at
larger block sizes should show a pattern where noise varies by bin position
like odd/even or larger spacing for FFT size > 2n.
 ==> Given that I don't understand why DF is helping noise floor, it is
     conceivable that DF might work better at input rate.  But given the
     subband filter concept this seems unlikely.  Whatever part of periodic
     noise that ends up in our channel passband should still be repetitive,
     and so cancellable at output rate.

Can't find it now, but ran across an old question: "how does the
distortion filter help noise floor and spurs inside the LF cut region?"  The
answer is that these are not in the LF cut region, they are sidebands on the
carriers.  The idea behind the LF cut is to regularize the distortion model to
avoid including LF effects that we don't expect to exist.  The 200 Hz cut is
set to be below the lowest low carrier.  I don't recall whether the LF cut has
been tuned at all.



[14 Jun 17]

Playing with the hardware at home.  A big misfeature at the moment is that the
distortion filter is destabilizing the feedback loop creating a LF ringing
related to its repeat period.  Also the impulse and distortion filters get
caught in a loop pumping each other up.

I did catch the impulse filter actually working on a wild-caught impulse.  The
residue amplitude was 1.7m, way above the residue noise floor, but the spike
in the demodulator output is only barely visible.  So it seems that we could
do impulse filtering on a significantly higher residue floor.

b.t.w., interpreting the outputs in the frequency domain is a bit subtle
because the clamping action introduces a high frequency component that is
relatively large compared to HF noise.  You need to zoom in on the lower
frequency carrier region to see the vertical splat from the impulse and how it
is reduced by clamping.

Using this test case I can also see that impulse being on is somehow trashing
the distortion filter, creating a ~1 high rate block of (carrier?) at 19m
peak.  This is ~10x larger than the actual impulse that was there in the
residue (when it wasn't clipped).  Seems we are doing something wrong with the
impulse flag.  This is currently going to the "noise" input in the filter,
which is or'ed with unlock.  If not a bug, then this is a way too drastic
"soft reset".
 ==> OK, see why this doesn't work.  We are splicing in a segment of the
     filter mean model into the input, but the input includes all the carriers
     (which get notched out in the FFT update step).  Splicing in a chunk of
     residual level signal into the middle of the carrier sequence is *not*
     benign. 
 ==> Also, was confused by bad wire label.  We do not at any point 'or'
     impulse and unlocked.  The noise argument of both filters is the
     (non-latched) current cycle impulse flag.

Also, I notice that for interference filter we are using "Error mode" false,
which results in not doing time-domain limiting on the input.  This is not
really desirable, but is not clear how to avoid because (as noted) the input
contains carriers at high level so we can't clip them.

Recall the reason we are not running the distortion filter directly on the
residue is because we were having problems with glitches getting stuck in the
distortion filter (not adapting out) when we ran the filter, see [30 Dec 15].
This created noise in the high output at the low rate.  However the current
code is more recent.  As I recall, I was attempting to move the adaptive
filters out of the KF feedback loop.  It's a little confusing in the block
diagram, but the input to the distortion filter is now completely independent
of the carrier prediction, so is "out of the loop".  

The only possible sneak path I see is through the hum filter, which still does
update based on the residue.  I think the reason we have been able to get away
with that in the hum filter is that it isn't coherent with ??? the low rate?
Also maybe because there is no LF cut in the hum filter?  If I ever had a
precise understanding of what the problem was with running the distortion
filter off of residue, I don't seem to have written it down.  

 ==> Why is there the asymmetry between mean and sigma updates in
     split_averaging_filter_nu? We just sum, no multiply by 1-alpha.  I can
     see why this mostly works, but it does mean there is no open-loop decay
     of the model amplitude.  I can see how this could lead to taking a long
     time to adapt out junk that gets into the filter model (if the FFT
     processing even allows it at all).  At the moment I can't think of any
     reason why using the decay term would "not work", such as because we are
     working in the FD.  ***Oh, we need "integrator action".  At least we do if
     we are looking at residue where our signal has already been subtracted.
     At least this is how s_a_f_n is currently set up, we integrate the error
     between prediction and signal, rather than just averaging signal (which
     is how the orignal 1DOF version worked, IIRC).  Is this different?
     Certainly the dynamics are different.  What we are doing *is* the typical
     ANC signal flow, AFAIK.

Because of the FFT filtering it is kind of tricky to "not update" the
distortion model for a short segment (less than the entire filter order).
 ==> Why are we averaging in the frequency domain?  I think this dates back to
     the earliest days of the 1DOF prototype, where IIRC I just decided to do
     it that way.
 ==> How much does averaging in time domain help?  I guess we can just
     hold the old data in the tainted window, but this will likely create some
     detectable splatter in the frequency domain due to implied
     discontinuities in the spliced signal.  But there are still problems if
     the input contains the steady-state carrier (as currently) unless we
     split the non-notched mean input off from the notched prediction output.

I no longer recall exactly how we ended up running the carriers into the DF
except that it came from the idea of getting the DF out of the KF feedback
loop.  If we get rid of the carriers by running on the residue (subtracting
the carrier prediction) then we are back inside the loop.
 ==> See above [15 June 17] discussion of [29 Dec 15].

There is not really any other way to get rid of the carriers other than by
notching, more or less as is being done currently in s_a_f_n.  Because of its
slow settling it is problematic having the distortion filter inside the KF
loop, and the residue settling dynamics create stuff that the DF ends up
spuriously responding to.  There is some virtue in feeding the filters a
signal with the other filter applied (as we are currently doing) so that the
filters are in some sense in parallel, and in particular don't see the
variance that the other filter is able to predict.  I think this mainly
affects the variance estimate not the distortion model, and we are not
currently using the variance.
 ==> I seem to be settling on the idea of time domain averaging where the mean
     is not notched, only the prediction output, allowing us to just not
     update for a high rate input block.  A little more confused about the
     integration of error vs. just average the input.  Some suspicion that
     these are algebraically the same (tho subtraction happens a different
     place), modulo issues of interactions with notching.


Interesting other musings about signal processing issues back on [30 Dec 15].
I noted something which I had forgotten, that the distortion filter seems to
improve the high rate noise floor (as well as prominent spurs).  This is
puzzling.  By construction the distortion filter can only affect stuff that
is phase coherent at the low rate (which of course true noise is not).  Yet
whatever it is doing is coherent with the FFT demodulation, so the raised
floor can't be explained by leakage either.  My one thought is that there is
IMD buried in the noise floor, created by either the sigma-delta ADC or
driver, and we are reducing it even though it is not clearly spectrally
distinct.  That is, is the (phase incoherent) noise shaping turning IMD into
spectral noise floor?


[16 Sep 16]


Categorizing tests?
 Controlled environment
   Investigating interference in controlled env
 Uncontrolled
 Robot



Experimental evaluation for R01.

Design goals:
 -- achieve similar or better metallic interference performance than Ascension
    or Aurora, 
 -- Give much better bandwidth/resolution figure of merit than existing EMTs.
 -- Maintain low noise levels under adverse EMI conditions.


What are our hypotheses?
 -- Our basic design goal, phrased as a hypothesis, is that that ILEMT will
    
 -- Due to the wide bandwidth, EMI will be a 

What are criteria for success?



Both metallic and EM interference are highly dependent on the local context,
so it is difficult to specify interference rejection as a quantitative
requirement. Both can be made arbitrarily bad, though I don't currently have a
good handle on the EM side. From an engineering perspective, I can document
that under some particular condition, there was an X% improvement by applying
a particular algorithm, and if this improvemnt holds across conditions of
varying severity, then we understand something about the merit of the technique



What is going to sound good to medical audience?
 - comparison to commercial products under same conditions for context.
 - we know what we are doing, know about issues on the OR. 
 - statistical methods, power, etc, always popular. But no human subjects. 
 - figures.

Stage evaluation of static accuracy, effect of fixed and moving metal. Tests
with simple metal shapes, and also Navio and Micron (power off).  Tests for
metallic interference caused by operating room instruments (microscope) and
furnishings (operating table).  Also interesting to evaluate derivative
nonlinearity.

Dynamic accuracy: ASAP ILEMT simultaneous measurement of hand motion.
Frequency response: motorized periodic motion

EMI testing:
 - long duration (two weeks) test with fixed sensor on surgery floor in
   hospital, assessing interference from distant sources such as power switching
   in HVAC equipment, medical devices in adjacent rooms. 
 - tests with medical equipment likely to cause interference, such as
   electrosurgery (Bovie), external defibrillator.
 - tests with small electric motors near the sensor (typical for handheld robot use).
 - tests with sensor mounted on Micron and Navio when actuators are running.

Description: 
 - figure showing workspace shape, source, sensors
 - table of planned tests

In the medical literature there is a lot of discussion of metallic
interference, much less EM. I am less concerned about metal vs. EMI, except
for near field metal, such as in the tool itself, or other
instruments. Because of very wide bandwidth we are much more susceptible to
EMI, and for cancellation we are also tolerant of absolute position error.

But for selling, we can target a larger market. The main market currently is
for static navigation with image registration. There are solid commercial
options, though, so not clear what demand there is for an open design. But if
technology could be transferred into commercial applications that could be a
selling point. Dual frequency could also be applied for small sensor low
rate. As SNR degrades, the high/low crossover has to be at a lower frequency.


[8 Sep 16]

Spent some more time looking at ADCs.  There are not as many superior options
as I had been hoping.  Both the oversampling SAR ADCs such as the LTC2380-24
and sigma-delta ADCs are mostly falling around 110 dB SNR at the kind of
sampling rate we want (> 40 ksps).  Top of the line audio test sets are only
claiming 120 dB.  
 ==> these numbers are *not* directly comparable.  See 30 June 17.

It seems that audio ADCs tout their dynamic range, which is a SNR extrapolated
based on a -60 dBfs input.  In other words, this is a noise floor less
distortion.  Top line audio ADCs (like the PCM4220 and CS5381) tout 120 dB
dynamic range.  I think the difference between dynamic range and THD+N was
part of what was confusing me here.  Dynamic range on Audio ADC datasheets
also varies in what measurement bandwidth is used, sometimes it increases with
sampling rate, sometimes not.  Usually it seems that the output rate selection
affects only amount of decimation.  Then there is probably no system SNR
benefit to using a higher rate ADC output rate.  Potentially there could be
time domain benefits for impulse filtering, but currently we run everything
through a LPF before doing anything else in order to clean up the residue.

Let's look at this as a spot noise density and equivalent resistor Johnson
noise.  Although operation at full scale may not be entirely practical, these
converters do typically support +/- 5V balanced differential input, which is
what our full scale reference would be.  So:
	-120 dB re. 5V = 5 uV peak = 3.5 uV RMS
	3.5 uV / (sqrt(22 kHz)) = 24 nV/rt Hz
        noise of (24/4)^2 = 36 kOhm

It is plausible that upstream signal processing could have noise below that
level, even in a general audio signal chain. Hmm, I wonder why the audio test
sets aren't better?  But to contribute negligibly the spot noise needs to be
about 3x down, 8 nV/rt Hz, or 4 kOhm noise resistance.  That's certainly in
the low noise bipolar range (though not ultra-low noise).

I need to investigate what SNR we are achieving on the current digitizer box
(UR44). According to:
    http://prosound.ixbt.com/interfaces/steinberg/ur44/2444.shtml

they find only a dynamic range of 94 dB (A weighted) in loopback mode (which
is basically how I am using it), which commentors say is unimpressive for a 24
bit ADC. Certainly in comparison to ADC specs alone, this is indeed 15-20 dB
worse.  So it is possible that a custom front end and optimized ADC selection
will improve broadband SNR by up to 20 dB.  Specific tweaks like line locking
the clock, or perhaps dither, different converter, etc., could reduce the LF
noise boost I'm seeing, which is more problematic than its contribution to the
broadband noise would suggest, due to it falling within the bandwidth of the
manipulator mechanics and of the KF hand dynamics model. ANC can also help
with hum in the demodulated outputs.

Improving the input SNR might just result in us becoming driver noise
limited. This could be helped to some degree by using the reference channel
feedback, and of course by reducing driver noise by whatever means.  My
original idea of open-loop direct digital modulation of the driver output
seemed to offer the possibility of low noise (assuming supplies are quiet).

I am unsure whether the oversamping SAR would outperform the sigma delta.  The
sigma delta has weaknesses with multitone signals, which is definitely what
we've got.  In RMS terms, most of the distortion is going to be from the
carriers, because they are so far above the ambient noise (hum, etc.) and
converter noise.  But everything in the input does get distorted too.

There's also a concern (in any ADC architecture) that synchronous sampling
concentrates converter noise into the carrier channels. This may be the cause
of the excess LF noise I'm seeing. Sometimes dither can help this if the input
is too low noise.  This may be more of an issue with the oversampled SAR.

We could greatly reduce the demands on the ADC dynamic range by doing the
subtraction of the carrier model in the analog domain, but this would be at
the cost of a high performance DAC and summing amplifier per input channel (in
addition to a residue ADC).  It seems DAC SNR and perhaps distortion are a bit
better, for example the PCM1794A has ~129 dB dynamic range. But that would
still be a lot of complexity to gain at most 9 dB improvement.


[13 June 16]

Have done a bit more thinking about the dual frequency approach, now that Nick
is closing in on getting a Matlab prototype working.  For now we are going to
just use the "low = more accurate" approximation, and are working the the 6DOF
pose space, rather than complex carrier amplitude.

It took a number of iterations to get the Labview code right that determines
the coupling (carrier amplitude) sign from the phase.  

If we're going to detect metallic interference from phase shifts, then we're
going to need a pretty fine phase calibration (perhaps per sensor channel),
since the phase deviations I saw in my previous experiments (in draft tech
report) were < 0.5 degree.  For sign determination, I've been considering 1
degree as "pretty close", though I think in many cases we're a lot closer than
that.  
 ==> It looks like there is at least three degrees of mismatch between sensor
     channels. 

It's hard to see how to make any direct use of the phase info in a pose-level
KF, though we could pass phase info around the side and use it to weight the
HF.

In the lab, we're seeing significant 60 Hz hum, which interestingly shows up
mainly on the pose Y (after pose solution).  This is dominating the HF RMS
noise.

Also, I think that the guard band noise estimates may be higher in the lab,
since I had to crank the LV KF process noise up to 100u to get ~ 300 Hz
bandwidth (according to noise rolloff).  

Looking at the last proposal, I was definitely shading a bit optimistic on the
noise performance.  The position noise is indeed a good bit larger in the pose
solution, though the hum makes this hard to tell.  The radial noise is
actually less than what I was seeing in the single-axis LV measurements,
perhaps a consequence of averaging across the three carriers.  Also, my
recollection of the 3 orders of magnitude figure-of-merit improvement applies
to Ascension, not Polhemus (where the projected difference is only 50x).  

Also, I was playing a bit fast and loose with the ILEMT bandwidth, saying it
was 500 Hz at 1 ksps.  As it is, at 1.5 ksps, with order 2 FFT window, the
theoretical bandwidth without any contribution from the KF demodulator is less
than 325 Hz, about what I actually observe with the process noise ramped up.
But then for Ascension and Polhemus, the max bandwidth is only a wild-assed
guess (likewise for the spot noise).  ASAP is now also using an order 2 Hann
window, so it's bandwidth is down around 250 Hz, not 500 Hz.  Even with no
window, the -3 dB frequency is short of the Nyquist frequency.  But for the
max bandwidth on the other trackers, I'm assuming full Nyquist bandwidth.

Latency is really more relevant for the application 


[21-23 Jan 16]

Now that I have displays for all three source axes, it's clear that the spurs
and noise peaks differ across the channels. Since the measuring channels are
AFAIK identical, this presumably has to do with either differences in the
driver channels (such as output resonances, clock frequency) or different
distortion in the AD/driver/DA chain as a consequence of the different carrier
frequencies.

It seems that only one sensor channel at a time gets the big residue spurs
(two-tone sum giving the appearance of modulated residue).  Interestingly,
this seems to move between axes.  When I first noticed that I was puzzled,
but if I rotate the sensor, it seems to be following the X source axis.
If you turn off the lowpass filter it becomes clearer that each axis has a
noise hump and a forest of spurs associated with it, where the frequency
order roughly corresponds to the carrier relative frequency order, that is
X, Y, Z.  The humps are at roughly 15.6, 23.5 and 25.4 kHz.  With the
lowpass off, the Z residue is much bigger than XY, and this may also have
led to my impression that the big-residue axis was switching, 

My noise floor experiments showed that the noise hump is due to the driver,
since it's present even when the output is muted, but only when the driver is
on. This suggests that the spurs are also from the driver, not the DA/AD,
since the spurs rise out of the hump.  The hump height only increases a few dB
when the output is enabled, but the spurs are only present when there is
output. 

The different positioning of the humps likely has to do with output
resonances, but the frequency is wrong for the first (parallel) resonance.
And hey, presto!  Putting approximate X source inductance into the spice
model, I get a 10 dB peak in load current at 16 kHz.  I'm guessing that if I
add a snubber for this resonance it will help with that crud.  It also wouldn't
hurt to make the X capacitor a bit smaller, since the simulated parallel
resonance is 6.9 kHz, not 7.5.  The 16 kHz resonance seems to be mainly set by the
series resonance of the two output filter inductors and the second stage
output capacitor.  Reducing the second output inductor has only a modest
upward shift on the frequency because it is in series with the first stage.
There is also a third resonance in the 60-90 kHz range, which is outside our
passband so is not going to be directly visible in the output.  

Hmmn, it seems that the first resonance is roughly determined by the parallel
combination of the two output capacitors and the source inductance.  So
subtract 0.47 uF from the value.  If we shift the resonance to 8.5K, the
output capacitance is 1.28u, and the second resonance shifts up to 21 kHz,
from the current 16 kHz.  That would put it pretty well into the stop-band of
our low-pass filter.  And the input current is still 11 dB down from
what it would be without the second filter stage and resonance.  When I
originally realized I needed to consider the resonance was when I had shifted
the corner so low I was working on the upper side of the resonance, where the
input current exceeds the output current.  Working on the low side of the
resonance is not so bad.

I added the "Zobel network" snubber that's onboard to the model.  It
does significantly damp the second zero/pole pair, but has no effect
on the 16-20 kHz series resonance.

If I shunt the second stage output inductor with a 4 ohm resistor, this damps
the second resonance (16 kHz) quite a bit (20 dB), and completely eliminates
the higher resonances, while having nil effect at the carrier, but it changes
our high frequency response from fourth order to more like third order,
reducing attenuation at 400 kHz by 25 dB.  This is still presumably 25 dB
better than what it would be with no second stage filter.  Because there are
only a few volts across the second stage inductor, the shunt resistor would
have little loss.  2-4 ohms seems to be the best value for damping, with the
lower value pushing the resonance to a higher frequency.  We can't fully damp
the series resonance this way because the first stage output inductor isn't
damped.  Putting an inductor in the 5-20 uH range in series with the shunt
resistor restores the HF attenuation, but also starts to undo the the damping
at the first series resonanace.  There seems to be a range of values around 10
uH where significant HF attenuation is added with minimal effect at LF.  This
could be a lossy inductor.

I think it may be that the driver's sigma-delta loop is being in some way
destabilized by the load impedance (or perhaps stabilized, defeating noise
shaping), and this is what is creating the spurs.  It would be interesting to
experiment with some of these output filter variations to see what the effect
is on the spurs.

It's possible that the position of the second resonance near the carrier
second harmonic is causing problems.  For example, the bad spurs are 15.4 and
14.5 kHz, which bracket the 15 kHz second harmonic of the 7.5 kHz X carrier.
Those are awfully close to the to the second resonance.  The peak of the noise
hump is currently 15.7 kHz.  I don't know if this is causing the driver to
increase those distortion products, but the filter gain peaking is certainly
passively enhancing whatever is present, even above what they would have been
if there was no second filter stage.

Hmmn, that ~500 Hz offset between the spurs is a lot like the low carrier
frequency, isn't it?  15k +/- 457 =
	14,543	15,457

These are a precise match for the spurs in the sensor signal.  Maybe it *is*
worth predistorting the output.  I'm not sure that will work though; if the
distortion filter isn't doing the job, then it seems that the spur isn't phase
coherent or something.  But the spur is quite sharp...


If these spurs are at -80 dB, then there really isn't much power in them.
That's -74 dBc.  Working back from L = 200 uH, I = 2.5A and f = 7.5 kHz, then
V = 24V RMS.  So the spur is only 4.8 mV RMS.  But if we drove out a big
signal at that upper resonance on that channel, then a lot of power would get
dissipated in the shunt damping resistor.  With 1 volt, the power peaks at
almost 1W, and at 25V it's 450W!  So don't do that, kids.  In reality the amp
would start complaining well before then, but you could certainly smoke the
resistor. [Actually, with LP prefilter off, the spur is seen to be -54 dBc,
but the point still holds.  A 1W resistor would be fine, wire-wound is fine
too.]
 ==> Referring to out_filter.sxsch, without the R6 damper and with 2.2 uF C2 output
     capacitor (best model of present filter), the peaking is 30 dB at 16 Khz,
     whereas with 4 ohm 10 uH damper, and changing C2 to 1.28 uF, the peak is
     +13 dB at 23 kHz.  As well as being 16 dB lower, it is shifted far into
     the stopband of the 14.5 kHz lowpass prefilter in ilemt_ui.vi.  The IMD
     spurs will presumably not move, but the peaking at 15 kHz is only 4 dB,
     not 17 dB (a 13 dB or 4.5x improvement).
 ==> With 4 Ohm, I measure 50 mA RMS, about 125 mA peak. Almost all of this is
     either at 7.5 kHz or 400 kHz (roughly equal amounts).  Reduction in noise
     hump is in line with theory.  I haven't changed the capacitor yet, but I see
     15 dB reduction in the spur, and about the same reduction in the spur.
     So it doesn't seem that the resonance is causing the spur, just
     increasing the amplitude.  The reduction is pretty considerable even with
     significantly higher damping resistance.  

Adding 6 uH (10 turns on Amidon T68A 26 LF powdered iron toroid) does significantly
reduce damper current, especially at 300 kHz.  Here the effect does not agree
that well with the simulation. The improvement in (no signal) output ripple is
6 dB with this inductor, but the improvement with the shunt resistor out is 15
dB.  So this inductor is creating a significant effect, but the result is much
closer to the no-inductor case, not the no-shunt case.

With a 125 uH hash choke, having the shunt R actually reduces the total output
by a modest 1.2 dB because it improves the visible time-domain noise peaking.
If I cut out the 500 kHz, then the noisy signal (which looks very much like
the residue I see in labview) is reduced about 3 dB.  20 turns on the above
toroid (24 uH) gives more like the desired effect.  With this, the total
output RMS is actually ever-so-slightly reduced with L+R shunt vs. open, and
the spectral noise peak drops 10 dB and the spur at 15.457 is reduced 12 dB.

In 30 kHz BW, best reduction of noise peaking is seen at about 7 Ohms (with
or without inductor), and this is a fairly broad minimum. With inductor in,
at 7 Ohms, the RMS ripple+noise in 1 MHz bandwidth is reduced by 12 dB.
Looking at the signals in Labview I can't see any difference with inductor in
or out, except a very slight increase in the noise peaking with it in.
Looking at the sensor signal, with sensor at 70 mm (!) using 7A22 in 1 MHz BW,
the difference in RMS amplitude with inductor in is 1 dB.  On the scope it is
hard to see even the effect of turning the driver on and off.  So I think we
can say that the amount of PWM ripple present in the output is not coupling
through to the sensor at levels that have any effect on performance.  It is
possible that it could be an EMI compliance issue, but that is not a current
concern.  The source coil SRF must be well below the PWM frequency, so it is
not very effective at radiating.

Connecting 7A22 probes at source coil rather than in driver gives better
ripple measurement.  There was pickup in the probes near the level of the
actual output ripple, seen in common-mode test.  Under this conditions, the
suppression of the ~16 kHz component is quite dramatic, and the RMS level
drops by a full 1 dB.  You can also see that the amplitude of the noise
peaking is itself noisy.  The 323 meter reading becomes much more stable with
the damper in.  At 30 kHz bw, the noisy envelope is easily visible at 10
ms/div.  Extent of a burst is only a few ms, but the overal average is varying
too, seen in meter reading, which swings by nearly 1 dB.  With the damper in,
the trace looks like white noise, and RMS drops 6.5 dB (30 kHz BW).  With
driver on or off, peak amplitude at the sensor output is dominated by a
line-synched impulse with ~100 us of ~200 kHz ringing.  This seems to be
capacitively coupled into the sensor cable, because it is responsive to my
touching the cable or junction box.  Touching laptop screen increase the
amplitude when the adapter is plugged in.  In the new direct connection to the
preamp there is no snubber.  The ringing may be the sensor resonance (with
this load), in which case it is poorly damped (as I would expect), and a
snubber might have value.  With carriers muted and lowpass off, grabbing the
sensor cable increases residue RMS ~1 dB.  Residue RMS decreases 10 dB with X
axis damper in.  This is about 1 dB lower with inductor out.

Source impedance test on GR6108-A at 1 kHz, including cable:
	X: 236 uH	Q 3.4	ESR 0.435 Ohm
	Y: 223 uH	Q 3.3	ESR 0.438 Ohm
	Z: 183 uH	Q 3.0	ESR 0.383 Ohm

On GR1833-A @ 10 kHz, 0.5A AC, DC 0A, DCR from Fluke 8810A 4-wire ohms:
	X: 236 uH	Q 37	ESR 0.401 Ohm	DCR 0.334 Ohm
	Y: 222 uH	Q 36	ESR 0.387 Ohm	DCR 0.350 Ohm
	Z: 184 uH	Q 29	ESR 0.398 Ohm	DCR 0.345 Ohm

[DC current had nil effect, expected given open magnetic circuit.]

The inductances agree remarkably well across the two instruments.  The Q
for the GR1608-A are on the low side.  This may be due to poor test
connections.  So there is really very little variation in ESR at 10 kHz, and
this is mot much higher than the DCR.  

According to simulation, we can approximate the series resonance by the
parallel combination of C1, C2, C3.  C1+C2 is 570 nF

1.05 uF - 0.570 = 0.480 uF
755 nF - 0.570 = 0.185 uF

From simulation:
		L3	C2	R6	C4	R7	Parallel res	Ser res
	X:	236 uH	820 nF	7.5 Ohm	open	open	8.5 kHz		25 kHz
	Y:	222 uH	470 nF	15 Ohm	open	open	10.6 kHz	32 kHz
	Z:	184 uH	100 nF	open	100 nF	43 Ohm	13.9 kHz	45 kHz

R6 becomes increasingly ineffective at damping the series resonance as C2
decreases.  All the shunt damper is doing on the Z axis is damping another
higher resonance.  L2 does however still reduce HF ripple.  And with these
smaller caps, the series resonance is pushed up to much higher frequencies,
all well in the stopband of the lowpass prefilter.  The caps were just wrong
before, too big because I didn't allow for the driver onboard capacitance.
This would likely solve the noise peaking residue problem all by itself, even
without the shunt damper.

The C3/R5 Zobel network is not very effective in its current onboard
position.  If we move it to the output and increase C3 to 200 nF, then the
series resonance is much better damped on the Z axis, and HF attenuation is
also improved.  It's slightly better to remove the onboard network than leave
it, because it drags down the parallel resonance, and doesn't have much
damping effect.  It seems we can simply leave out the shunt damper then,
significantly improving HF attenuation.

On X, the shunt damper is absolutely necessary, though the Zobel gives a
slight additional damping.

 ==> Oops, not paying attention to power in the Zobel resistor.  To keep
     under 1W, I changed back to 100 nF on Z, and increased R to 43 Ohm.  We
     were overloading the old onboard 1W 10 Ohm Zobel resistor (though they
     didn't blow.)  I decided to use just the shunt damper or snubber,
     depending on the channel frequency.  Only Z gets the snubber.

 ==> OK, with these changes, the residue is *much* cleaner, about 40u RMS.
     The 14,543 spur is still biggest, but now it's down to -87 dB with the
     prefilter on.  Since a workaround for power spectrum scaling bug moved
     things up by ~13 dB (or sqrt(20), if you must ask), and it was about -80
     dB before with filter on, this is a 20 dB improvement.  The hum filter is
     having problems because of dropped blocks due to running behind, but the
     model at least looks like hum.

 ==> The problem with the impulse filter seems to be an interaction with the
     distortion filter?  There also still seems to be some really slow
     settling?  But it doesn't seem to be chattering the way it was.

 ==> Because driver noise peaking is bringing driver noise up above the system
     noise floor, it seems we are not too far from being driver noise
     limited.  According to tests on 2 Jan 16, driver noise with no input is
     below the noise floor, except at 13.5 kHz (due to noise peaking examined
     here.)  If we increased ADC dynamic range, then we might become driver
     limited.  Note that if we normalized the signal by current sense at full
     rate, then this should largely cancel driver noise, but then sense noise
     (and dynamic range) becomes an issue.


Also:
 -- The impulse filter is completely not working (constantly triggering and
    causing noise on YZ channels).

Thermal expansion of source radius should be in the tens of microns range.
Cte for ferrite is ~10 ppm/degree C, copper is 18, plastics up around 100.
Using r=0.02 m, temp rise = 20, and cte 100 gives 40 um.  This is in a similar
ballpark to the drift I'm seeing after current correction, but likely well
less, given that much of the source is copper and ferrite. If change is in
this range, it could possibly be measurable with calipers if well defined hard
reference points were created. The fiber optic sensor could work too.

Portland cement with no filler can match copper. With fillers, they bring tce
down to the ferrite range or below.  So potting in cement could be a good tce
match, much better than unfilled polymers.


[20 Jan 16]

Have the three sensor axes sort of working.  We are running a bit behind, and
dropping input.  Haven't gotten profile results I can make sense of yet.  I am
nervous about paying too much attention to other results when we are dropping
data, but also bear in mind that when there are NaN's floating around this can
cause numeric speed to go to hell.

Distance calibration seems to be a bit closer to working that it was a couple
of days ago, but there's still something wrong.  It's converging with low
residuals, but generating a negative gain that causes an error when we use
it.  Possibly I need to implement the axis equalizing calibrations below.
It's also possible there could be sign flips on the sensor, though I don't
think anyone would care about this yet.


Calibration:

Now that we have all these source and sensor axes, I need to do something to
calibrate them:
 1] Calibrate the relative gains of the sensor axes so that we can measure
    vector direction.  This can be done by rotating the sensor to measure the
    source X axis using each sensor axis.  This is done at the nominal 200 mm
    distance, with sensor input gains adjusted to -6 dBfs on the X high
    carrier.
 2] Calibrate the gains of the source carriers from drive amps to sensor
    volts re. FS.  This could be expressed as volts/amp, or Ohms.  We expect a
    value in the milliohm range for the high carriers.  Then we can divide
    carrier amplitude by current and to get an amplitude normalized w.r.t. the
    nominal -6 dB operating point. [This assumes current sense has been
    calibrated, a separate procedure already implemented.]  This can be done
    using YZ amplitudes collected in the same three-orientation procedure as
    sensor gain calibration. Under the dipole model, at the sensor position
    (200, 0, 0), I believe the YZ carriers should be 6 dB down relative to the
    on-axis amplitude.
 3] Adjust carrier drive levels to equalize measured amplitudes across high
    carriers to -6 dBfs under equivalent on-axis conditions.  Assuming noise
    is flat, this equalizes carrier SNR, and also allows use of vector sum
    across source axes in the range calibration.  It may be useful to automate
    this, given a power budget and specified AC resistances.  [Then you have to
    repeat above steps.]
 4] Calibrate range, using a grand vector sum amplitude across sensor and
    source axes.  Would also be possible to calibrate individual
    per-source-axis X offsets.


It would then be possible to calibrate kinematic data such as the sensor
centering on the puck using a six-orientation test data (+/- XYZ).  If we
could index source orientation too, then we could calibrate the 3D source
position offset.  Assuming the index pucks are actually square, 1/2 of this
eccentricity can be fed back into the above calibrations (now that we know the
sensor ranges were not in fact identical in the three-orientation test.)  It
is also possible to calibrate or mechanically trim axis alignment errors
by looking at the magnitude of axis amplitudes that should be zero when
source and sensor are aligned.  At some level this will be sensitive to
spurious electrical cross-coupling of axes.
 ==> Rather than spending too much time on more precise hand calibration
     procedures, it might make more sense to move on to using the calibration
     stage. Then we can fit a big model of as-built kinematic and gain
     parameters based on minmizing calibration residual.  In addition to
     software development, this requires some care in setup.  The main concern
     that I have is that the metal (and perhaps stepper drivers) may interfere
     with the tracker operation.  One sanity test is to fix source and
     sensor on a (non-metallic) bar and by hand move this setup around close to
     the stage, looking at how much pose deviation there is.  In order to know
     the significance of these deviations we need at least a rough
     calibration from a manual metal-free fixture.

Though if you looked hard enough you could presumably detect small differences
in the sensor axis frequency response, for now I'm willing to just assume that
the only difference between sensor axes is a fixed gain term, and to assign
all frequency dependence to per-carrier source gains.  Gains definitely differ
between source axes other than due to frequency because the turn count is not
the same.  The low carrier source gain should be lower by about the high/low
frequency ratio, but we will calibrate this, rather than assuming.

Because we have a current measurement per-axis and also determine the drive
voltage, it would be possible to measure the source impedance under operating
conditions. This would be interesting for better understanding of the source
dissipation, but is not something to do right now (though it would enable
precise allocation of the power budget).  Knowing the source impedance also
enables compensation of warmup related source drift and detection of big metal
near the source.  The main difficulty is other sources of phase shift that may
exist, such as buffer latency and frequency response of the driver and current
transformer.  These could be calibrated using a "loop-back probe" which is
just a set of attenuators connecting the driver output voltage back to the
sensor input.  This could be built into the driver box, with a modular output
connector.  Measuring the high carrier loss requires phase accuracy because
the Q is reasonably high.  At high carrier the CT phase should be pretty
accurate, but driver low-pass response may start to create noticeable lag.  A
buffer time shift could also be calibrated from the current signal alone if
one of the impedances is known.  For example, a 1 kHz bridge measurement could
be used to estimate the impedance at low carrier, which is not very sensitive,
since Q is low.  The ASIO interface is set up so that phase alignment might be
preserved across write/read buffers, but I don't know if it actually works
that way.
 ==> My *guess* is that source axis inductance is much the same at high/low
     carriers, and is fairly stable over temperature, while AC resistance will
     differ at high/low carriers and over temperature.


[19 Jan 16]

Kalman demodulator settling problems likely due to discontinuity in carrier
model interacting with noise adaptation. Discontinuities are visible to
STFT. Try changing carrier model to first order hold.  See also "It's a bit of
a puzzle where all of those spurs in the carrier model come from" on 30 Dec 15.

Simple IIR notch filter will not work on ADC spurs because it will ring on
impulses. Instead use bandpass filter to predict from residue, suppressing
impulses into filter.  Should also try averaging filter with different period.


Older notes pulled off of Google tasks:

There are several products, and many people seem to have developed EM
trackers.  Many people claim to have developed clever algorithms to solve some
problem or other. This is mostly consistent with what I believe, that the
basic function it isn't that hard, and there are a wealth of things you could
do that would work to some degree.

Discussion of EMI is much less prominent. I get the impression that especially
for the position solution work, and to some degree for metallic interference,
these people are more physicists or something, in love with the math. Perhaps
even without so much as a working system, just paper brilliance.  Not to say
that this skill set is useless, but not the person who's going to be thinking
about EMI.  "Instruments, alas, are made from circuits and code", not ideas.
Anyone who knows the history of this area would want to see at least a working
system.

There has also been a lot of academic work with VR people taking one of the
black box proprietary trackers and trying to correct their output.  This
suggests that metallic interference is a problem for VR. Possibly EMI too,
though that is less likely to respond to static calibration.  Evidently
Polhemus has done similar stuff for plane cockpits, etc., presumably in a more
magnetically informed way.

I have to say that it does make me wonder if I'm not so smart after all,
beating on this same idea and thinking I'm going to change the world.  One
thing it clearly says is that doing what Polhemus and Ascension did, getting a
product out there and making it a working business, is a significant and very
different thing than just making a tracker that functions.  The open angle is
enough different that it could possibly make something different happen.  In
the patents I get a definite feeling of lots of different silos, people
reinventing this wheel with different numbers of flats on it.  And then the
creator moves on and the prototype ends up in the trash or their basement.
So, I guess it's a fun thing to invent.  Many people had whiled away the hours
working on it. I can relate to that. Maybe 7 or 8 on the misunderstood genius
scale.  It also seems to have attracted people who are interested in the idea,
but are not getting traction.  And that puzzling French sponsored project,
which seemed to be overkill without a clear idea of what they were killing.

I'm sure that if potential funders were aware of all this, then it would
reduce my chances.  Am I one of the ones who can make it work, and even if I
can, won't the world yawn once more?  I do strongly feel that the last thing
the world needs is another EM tracker patent, or even another proprietary
tracker vendor.  Selling ILEMT hardly seems like a viable business, niche
within niche.  Selling it as a turnkey black box also undermines its
usefulness.

It's also possible that the legal situation has played an important behind the
scenes role in shaping what I see.  I've been supposing that the area is no
worse than any other so far as the confusing overlap of prior patents goes, or
the activity of owners in enforcing them.  Maybe this is not so, or maybe
their are many technologies that have this pattern of lots of false starts
without much evolution of the state of the art.  When I compare to medical
robotics, there are some similarities. It often seems that overall progress is
driven by progress in underlying technologies such as computers and actuators,
rather than much in the way of cumulative development of the technology
itself.  Hundreds of medical robots have been prototyped.  Only one company
has made a sustained go of it, after at least three decades.

One difference with trackers is that a lot of the robotics work fits into the
academic research model, is published in that literature.  (In academic
research it's hardly a surprise, and not much of a misfeature, that few things
go anywhere.  The main product is the students.)  Not so much with trackers.
There are some papers on new principles, rotating fields, different
source/sensor arrangements. Maybe trackers suffer from some combination of
lack of sexiness/novelty and lack of feasibility in the academic setting. Of
course even a decent robot arm is hardly feasible to develop for a PhD, but
that doesn't stop us.  There are a few freaks thrown up, like the RAMS eye
robot, developed by mechanical genius engineer at JPL.  Well, possibly it
didn't work well, but from this distance it looks darn good.  If the world had
been waiting for this, then it should have taken off like a rocket, right?
But you can have what seem like good ideas, and even show they seem to work,
and then the money runs out.

You'd think people would be interested in good ideas, and often they are, but
the firm is almost the only model we have for putting solutions in the hands
of users.  Now open source is a new thing that's come along.  It can work in a
different way, and involves less wasteful reinvention, but has challenges for
hardware, which has to be manufactured, and needs particular proprietary
components.

When playing by the rules of the firm, that shapes what you undertake.
Interesting and important problems are not the easiest way to make
money. Medical seems important, and there is money flow, but there are many
challenges in intercepting a money stream, regulations, established practice,
etc.  The apparent importance means there is funding, but doing relevant work
is particularly challenging.

I guess my overall conclusion is that there may be technical and legal
complications bigger than I currently envision, but that the big message is to
really play up the open source angle.  Not only put the design out there, but
also massive documentation on how to apply it.  Literally write a book.
That's what academics do.  I'm also happy to work with any companies that want
to apply it. Collect resources like links to coil winders and fabricators.  Do
a crowd funded production run.  Getting all the way there will require
implementing in more awkward ways, moving code out of labview, maybe Eagle
CAD, etc.  For that purpose, it's fine if it's just another tracker.

So far as speed/resolution, I'm sure we can do much better than existing
trackers.  For EMI, I'm hovering between "never was such a bad problem" and
"solved problem", but there can be bad cases such as narrowband interferers
and perhaps motors.  My current thinking on metallic interference is that
probably many proposed multi frequency algorithms work to some degree, and
that class D drive is an important missing piece.  Also pushing down to such a
low carrier may be an audacious move that hasn't been seriously tried.
Anyway, it should be possible to do at least as well as existing products.

Can test tracker frequency and step response with numerical injection, most
straightforward as AM on DAC command. As with micron, much easier and cleaner
than trying to move sensor with high speed and fidelity.

For magnetic interference inside handpiece, reference pickup near offending
source could be useful, especially for motors.

FPGA is ideal for FIR ANC, simple compute intensive. Not sure we need,
though. General ANC in fd rate output, perhaps synthesize reference input from
inputs averaged across all sensors. Distant EM sources correlated.

ILEMT FPGA: put fs rate path in fpga.  For periodic filters, just the buffer
of stuff to subtract and buffer of new data.  FFT notching is done in
software.  FPGA and processor talk on each high rate sample, updating carrier
amplitude.  Most complex part is probably generating carrier model from
complex magnitude.  Do want FFT for demod, i think, but can make do with 16
bit, likely available open source, since many bits of dynamic range are taken
away when we subtract carrier.  Includes high rate part of demodulator update,
but not KF itself.  Best to stick with labview software for the rest, adding
cores as needed.  parallel across sensors and measurement channels.  Not sure
it's even worth using DSP blocks.  Not all that much cycles by FPGA standards.

As alternative to iterative robust 2d regression, could just do one iteration
on each update.  Weight according to preceding fit, then regress. Does still
require division.  Much easier with floating point, too.  Probably open source
options.  Should only need a single instance of FFT and regression, and they
work together.  Or offload to board level controller.  
  ==> just doing robust mean now.  Even simpler.

UDP seems most straightforward for labview data transport.  Would require some
local controller to do protocol.  Wouldn't need real-time toolbox for non
real-time tracking.  Wouldn't need ni FPGA board or toolbox at
all. Architecture gives us some slack for late packets, since KF can
interpolate through, and carrier model doesn't have to be updated each
cycle. That's the critical high rate feedback internal to KF demodulator.
There are various embedded boards with FPGA and Linux that come with glue
code, drivers, etc.  Could start out just by shipping raw data out over the
net.  Much cheaper than ni FPGA board.


[17 Jan 16]

Built junction box and wired the Sumida CAS143-47 3-axis 125 kHz RFID
anntenna.  Note: connection diagram in datasheet is *bottom* view.  It seems
to basically work, but it's going take some time to work out details like the
phasing.  It could be that this part does involve coils wound on a single
core, because the X/Y specs have different Q.  Measured DC resistance is
significantly different, too, 60 vs. 70 Ohms.  Hard to see how that would
happen if there were just two rod cores and a flat bobbin in there.
 ==> Thinking about buying 10-20 of these, because no telling how long it will
     be available.  A three-axis component is rather unpopular, and if these
     coils are concentric this may have significant advantages for us.

b.t.w., I've pretty much resolved the noise floor question.  Using sniffer
probe, noise floor (not obviously hum-related) in my shop is orders of
magnitude higher than outside.  In particular, the excess ~10 kHz noise is
coming from my "Not as we know it" sculpture, which is PWM multiplexed driving
64 incandescent lamps.  At the top of the hill in the Smithfield cemetary, the
noise floor is down to the noise of the sniffer probe together with the
current transformer transimpedance amp, but there is still clearly visible hum
harmonics.

The FFT noise floor goes up and down more or less in proportion to the hum
signal.  This is consistent with my idea that power-line switching impulses
are a significant cause of interference, and that some are non-periodic.
Long-term, it seems there is some prospect of using the patient sensor (or
other low-dynamics reference sensor) to measure the ambient noise field, so
that it can be subtracted out.  Also, in my shop we are currently ADC dynamic
range limited, with the exception of the 10 kHz noise.  It seems likely that a
hospital would have higher noise.

Started working on the code for three sensor channel support.  I'm nervous
that my laptop won't be fast enough to do all the processing, but it should be
possible if I turn off the periodic noise filters.  We can also transition to
using a faster desktop in the lab.  It should be possible to get channel
processing running on multiple cores (seems to be happening already, CPU usage
is above 25%, and hyperthreads are fairly balanced).

In working on fixing the current calibration code, this has me thinking about
the whole calibration problem.  I had been planning to not even report
accuracy measures when we write this up.  I was hoping we could get accuracy
high enough so you could "see it working" without any major calibration
effort.  

I think it may be a problem (as one reviewer suggested) that our sensor is
large w.r.t. the source/sensor distance.  Since the field gradient is strong
and nonlinear, it seems likely that parts of the sensor closer to the source
will contribute more signal, and that with the nonlinearity this will not
average out to a center point for that coil.  Similarly, and perhaps more
important for overall accuracy, the effect of sensor rotation will likely
differ from the ideal cosine relationship because portions of the non-point
sensor move nearer to the source as it rotates.  It would be possible to model
these effects using a 3D magnetic simulation, but only a few special
geometries using FEMM.  It seems likely we could fit a decent reasonably
low-order polynomial model to these effects, so it wouldn't require abandoning
a closed-form model.  Another way to generalize the pure dipole model would to
have an apparent pose offset of the source and/or sensor coils that depends
on an emperical table of deviations based on field angle and source distance.

For characterizing sensor response experimentally, it might be useful to have
a smaller source, which would more closely approximate a monopole.  This could
even be single axis.  I am no longer so sure how useful a Hemholtz coil would
be.  It would allow calibration of gains and orientation of the sensor coils,
and do this independent of the source.  Because of the uniform field, these
can be evaluated without any confusion from source/sensor coil position.  But
using the RFID sensors should reduce problems with sensor irregularity.  With
the CAS143, the axis orthogonality is likely to be pretty good (although not
specified, nor particularly important in the intended application).

It will likely be possible to jointly calibrate source and sensor using a
model where each coil has six degrees of freedom (XYZ, tip/tilt and gain).
That would give 36 unknowns, but we can easily generate 100's or 1000's of
calibration data points.  The question is how well will the dipole point
approximation work.  


[10 Jan 16]

The TI LME49724 fully differential audio amp looks like a good ADC driver for
differential signals in our speed range.

OPA827, low-noise JFET, 4nV/rt Hz.  Also OPA1652,4 4.5 nV audio opamps.

LT1792 4.2 nV/rt Hz, LT1113 4.5 Hz

Analog devices seems to have given up the low noise JFET space, but the AD8241
is an interesting bipolar-in instrumentation amp with source-limited noise
performance for impedance in the 2K-100K range.  En=3 nV/rt hZ, In=200 fA/rt
Hz.  This is plausible for transformer-coupled search coils.  Noise resistance
15k.  

The OP27/37 are also in the running for source impedance in this range, En =
3.4 nV/rt Hz, In=400 fA/rt Hz.  Noise resistance = 8K.  This is single-ended,
so would suffer a bit more w.r.t. AD8241 in a instrumentation amp configuration.

Compare LT1028, En=0.85 nV/rt Hz, In=1 pA/rt Hz, noise resistance = 850 Ohms.

For triangle w/ 4' sides, loop area is ~0.74 m^2.  11.4 Ohms 22 ga = 700'.  58
turns, L ~= 14 mH, based on same-area square loop (0.86 m)
    http://www.eeweb.com/toolbox/rectangle-loop-inductance

Johnson noise ~= 0.43 nV/rt Hz, based on DC resistance.  Likely somewhat
higher due to skin effect at higher frequencies.  For amp noise to contribute
negligibly, we'd need perhaps En*3 = 800 Ohms apparent source resistance.

So impedance 
Tx ratio	DCR	100 Hz	1 kHz	10 kHz
(none)		11.4	8.8	88	880
1:10		1.14k	880	8.8k	88k
1:20		4.56k	3.52k	35.2k	352k

This makes antenna reasonably well-suited to LT1028 w/ no transformer, but is
not source resistance limited at any frequency.  For bipolar amp, we want more
like 1:10 than 1:20.  It looks like by adding a transformer we could get a 2x
to 3x improvement with bipolar.  With low-noise JFET, could use 1:20 or even
higher.  LT1792 with 1:20 would be source limited over entire band up to ~100
kHz.

Consider Talema MET-23 or MET-35:
    MET-23: 108/0.8 DCR, turns 22.4:1
    MET-35: 60/1.5 DCR, turns 8.66:1


Samsung GS3 phone full-scale input is 50 mV RMS sine = 70 mV.  Input
resistance is 2.2K Ohm.  When run with bare field sniffer sensor this gives a
transimpedance type response (flat flux to voltage over frequency) above 1.4
kHz (L = 250 mH).  This is why high and low carriers seem nearly equal with
sniffer.

Sniffer probe: 74 Ohms, 250 mH, ~1000 turns 33 ga on Amidon 0.5" x 3" #33
ferrite rod.


[7 Jan 15]

These guys say 5 fT/rt Hz:
    http://www.vlf.it/iverson/elf-lf_sensor_design.html

Another interesting page:
    http://sidstation.loudet.org/antenna-theory-en.xhtml


[6 Jan 16]

Fig 2. in Meloni15 "Background electromagnetic noise characterization:
the role of external and internal Earth sources" looks a lot like the integral
response to the external noise that I'm seeing, in particular the 10 kHz hump,
which would be even more accentuated using the differentiator response of the
inductive pickup.  At that peak, magnitude is about 1 fT/rt Hz.  The FFT
measurement bandwidth is only 6 Hz, so not a lot higher, say 2.5 fT.  

Hmmn, maybe that's not what I'm seeing.  That would be -198 dBc (re
carrier). The scale is pretty compressed, the upper line might be say 4 fT.
But what I'm seeing is -124 dBc.  74 dB is a pretty big discrepancy.  Is it
possible that whatever mechanism that creates the natural peak also works on
electrical noise when in a developed area?  As I read the literature, the peak
is really the cessation of a notch caused by ionospheric resonances in a
spectrum with an overall low-pass rolloff.  The notch is the "quiet zone".
The overall rolloff is not too far from 1/f, though 1/f^2 is supposedly a
better fit over wide ranges.  So for us the floor would look fairly flat
except for a peak and subsequent rapid rolloff.


[3 Jan 16]

Sensor (off the shelf):

RFID antennas look even better than inductors, since you can get ones designed
for field pickup. There are packaged three axis ones, but Z axis isn't
matched. Rods are more sensitive per volume than flat bobbins. Three axis
common in RFID to get around directionality.  

Impedance (7 mH) is much higher, 600 Ohms inductive at 13.5 kHz + ~140 Ohms
resistive (based on Q at 125 kHz design frequency).  Concerning input
capacitance, we'd need about 4 nF to load resonance down to 30 kHz, and UTP
cable is only 50 pF/m.  This in the impedance ballpark that audio typically
works with, so I think it's likely that we won't benefit from an external
preamp.  As mentioned yesterday, we are ADC limited.

According to ilemt_tr, the source field is in the range of 10 uT (1 carrier),
and 17 uT combined (RMS?) according to my ELF hazard note.  I'm running at 50%
higher current now.  For input gain, we only see one axis of the field, so the
crest factor isn't that high.  My FEMM simulated 13 mm air-core sensor has
sensitivity 2.8 mV/uT @10 kHz. Compare to EPCOS B82450A7004A, which has
sensitvity 52 mV/uT @125 kHz.  The rated sensitivity has to be scaled by
frequency difference.  The RFID design freq is 125 kHz, whereas our nominal
freq is 10 kHz, so sensitivity is 12.5x lower, or 4.2 mV/uT.  That's a bit
more sensitive.  Based on field strength estimate, the expected output would
be about 71 mV RMS.  I'm getting 40 mV peak amplitude out of the present
sensor coil.

    10 uT * 1.5x current * 1.4 RMS peak conversion = 21 uT peak

At 2.8 mV/uT, that would be 59 mV, but current coil doesn't match to the
FEMM simulation.  So we are decently close to theory here.  Assuming the
discrepancy is all in the current sensor, output with RIFD antenna would be 88
mV peak amplitude.  That corresponds to -22 dBu or -24 dBv.  This is in the
range of both the microphone and line inputs on the UR44.

FWIW, turning up the gain so that the noise floor according to carrier display
is -111 dB, with normal sensor hookup, vs interface input shorted, the floor
goes down to -132 dB.  So our noise referenced to preamp output is 21 dB above
the UR44 HI-Z preamp noise floor.  With input shorted, noise floor is -127 dB.
Spot noise at 7,477 Hz is -130 dB with LNA input shorted and -119 dB with coil
attached as normal.  With this preamp gain, the spot noise is -134 dB with
UR44 input shorted.

Hah!  With sensor inside mag shield, spot noise is -130 dB.  So the extra 11
dB of noise is actual ambient noise.
 ==> Preamp voltage spot noise is above UR44 preamp noise floor too, but most
     (>70%) of the LNA/sensor noise must either be coming from interactions of
     LNA current noise with the input impedance or due to ambient magnetic
     field that appears as broadband noise.  It is not due to sensor coil
     thermal voltage noise because the resistance is too low.


[2 Jan 16]

I've been working quite obsessively on the tracker, and throwing off all kinds
of ideas, many of which are not really current concerns, but are puzzles and
might be interesting someday.  Also see drawings of source enclosure in paper
lab notebook.  Overall, I seem to be a bit short on total mysteries, but
working out loop settling issues and moving high carriers off of hum multiples
will take some work.

If I set the KF bandwidth too wide, then demodulator loop diverges, likely in
interaction with hum filter, since we see a strong 60 Hz peak in the
demodulator output.

See notes below about using RFID antennas and no preamp.  But FWIW, I'm
strongly suspecting that our noise floor is currently limited by sigma-delta
converting multitone distortion into noise floor, rather than by preamp (let
alone sensor thermal) noise.  Going to try a few tests to get a handle on
this, but noise floor clearly goes up a lot when those carriers are coming in,
which would not happen if sensor/preamp noise was dominant.
 ==> Nope, overall noise floor is not affected by carrier presence. 

[dB FS as reported by current spectrogram display, which is actually about 10
dB low, I think, but can't figure out why (see earlier).  Also with averaging,
which reduces the peaks of the noise grass 5-10 dB below what you could
actually observe with order 1 low rate FFT.  See "What does it mean" at the
end of this noise discussion.]

Noise floors:
Source box off, amp on, sensor present:
	500 Hz		7.471 kHz	10.471 kHz	13.471 kHz
	-135 dB		-140 dB		-129 dB		-143 dB
Interface input shorted:
	-143 dB		-146 dB		-144 dB		-145 dB
[1/f noise corner about 200 Hz]

Preamp and cable attached, sensor disconnected and cable clips shorted:
	-144 dB		-145 dB		-143 dB		-144 dB

Sensor as normal, driver box on, but outputs muted:
	-137 dB		-140 dB		-130 dB		-137 dB

Carriers on, everything as normal:
	-136 dB		-139 dB		-130 dB		-132 dB
[The difference in noise floor with carriers on/off at 13.5 is 2 dB at most,
not really 5 dB.  Some non-reproducibility, rounding error or something. There
is a 2 dB increase in the 15.7 hump with carriers on vs. muted.  This
presumably affects 13.5 similarly also.  In this case, "500 Hz" is actually
486.3 Hz.  w/ Blackman window 500 Hz is in skirts of Y carrier.]

Input/output channel cross coupling on carrier freq (input shorted):
	-135 dB		-125 dB		-128 dB
[These go away when digitally muted, and drop about 5 dB when outputs are
unplugged.  This cross-coupling is likely very stable, so shouldn't have much
effect on performance.]


OK, that's interesting.  The 10 dB 10 kHz noise hump is not internal to the
interface, as I had assumed it was.  At 7.5 and 13.5 there is a hint that
noise floor of something external is visible.  RMS signal drops about 24 dB
with input shorted. (Almost all hum with sensor attached).  I haven't been
trying to get values to within 1 dB, so I don't really thing there's reason to
suppose the preamp has -1 dB of noise.  Let's just say whatever there is we
can't see it, at least when inputs are shorted.  Sensor resistance is 18 Ohms,
so thermal noise is way less than amp noise.  Then something we are picking up
is causing the low-Q 10 kHz peak?  Yes, if I put sensor in my magnetic current
transformer shield, there is no 10 kHz peak, or only the barest fraction of a
dB.  Turning off the lights doesn't seem to make any difference.  Maybe this
is a resonance of the household wiring, interacting with impulse noise?  This
is an example of why it could be useful to adapatively choose channels.  Huh,
with averaging of 2, you can see the noise hump is sloshing back and forth at
varying rates.
 ==> Would be interesting to see if this correlates to the varying LF ripple
     in high rate output.
 ==> As noted later, the sloshing noise hump is "Not as We Know It" light
     sculpture. 

So in short:
 ==> No, we are not currently preamp noise limited, let alone sensor thermal
     noise limited.  In order to be thermal noise limited we would need an ADC
     with higher dynamic range.  And this is even without all the ADC crap
     that happens when the source is driven.

With driver on, but no carriers, there is a repeatable 6 dB increase in noise
floor at 13.5 kHz.
 ==> Hokay, the whole 15.68 kHz hump is due to the driver.  At -114 dB when
     driver on, -143 dB when off, so at 30 dB hump.  This is part of what
     motivated me to reduce the LPF cutoff, so this could be something
     to think about someday. But this hump is not mainly what is causing my HF
     crud when the carriers are on. Then it's spurs at -80 dB.

With everything going, there is no increase in noise floor (but a lot of
spurs).
 ==> The ADC noise floor does not seem to increase with signal, but we do get
     narrowband spurs.  Hum peaks and channel cross-coupling also vary across
     the test conditions.

Note: current sense noise floor is higher because it is scaled to
current. About -119 dB re 1A.  Connecting CT (with source power off) has no
effect on current noise floor.  Carrier coupling into current channels with CT
cables connected is about +15 dB above noise.  In current sense channels, some
of the carriers drop as much as 10 dB when the cable is disconnected, though
still clearly visible with averaging.  Cross coupling also varies by carrier
and by sense input.  For example, the Z carrier on the Y sense is about 23 dB
above the noise floor (-97 dB) This presumably has to do with details of the
interface internal layout, what wires and ground current pass near what
converters.  The region of increasing current sense noise from about 200-2000
Hz does go away when driver is off (as expected from explanation that this is
because of flat output noise going through inductive load).  This increase in
current noise does not affect low carrier floor because the sensor gain is
dropping at exactly the same rate.  We are still ADC limited.

What does it mean:

How do these noise floor numbers relate to present performance?  Noise floor
goes up or down depending on measurement bandwidth.  Low rate STFT is order 8,
and sees a lower noise floor than the above numbers because the overlapped
STFT reduces the bin bandwidth.  And the high carrier sees a higher noise
floor because its bandwidth is much wider.  But this doesn't really matter for
the measurements because they relate to spot noise density.  The spectral line
peak amplitudes do *not* change, though, so in a wider bandwith, spurs that
were visible at very narrow bandwidth become submerged in the noise floor.
They are still there, and are visible in the demodulated spectrum, and all
together add to the "noise", but in the time domain they individually
contribute very little to peak amplitude.  If we take the noise bandwidth of
the above measurements to be 5.8 Hz (order 1 low rate) and the high rate
bandwidth (order 2) to be 375 Hz, then the high carrier noise floor is 18 dB
higher.  So the high rate noise floor should be about -117 dB.  A spur needs
to be above 117 dB (and gets attenuated by distance from channel center by
window response.) There's only a few spurs that are above this
threshold, and the only one that is near a carrier center is the one at 13.629
kHz (120 Hz sideband?) at -101 dB.  Then there are the 14.543 and 15.475 spurs
which are causing all the residue, though isn't a problem for DFT itself.
 ==> FWIW, the "dB" display in the "High carrier X" reads about -117 dB (RMS re FS
     sine). Not sure how much this counts as confirmation of the high carrier
     noise floor, because I'm not sure about the consistent scaling of all the
     different measurements.  But this does suggest that absent LF effects
     (below 5.8 Hz) we are indeed noise floor limited.  But we are getting
     about 10 dB of extra LF noise in the high rate output, discussed
     earilier.  I believe this is due to the hum harmonics landing on the high
     carrier, but that seems to be about -125 dB.  This is sort of in the
     right ballpark, though, and if I dial up the KF bandwidth, then the LF
     stuff does get pretty submerged in the broadband noise.


Stuff I pulled off of Google notes, where I dump ideas I don't want to forget:

Ethernet UTP patch cable would be a pretty good sensor lead for single
sensor. Unfortunately only four pairs, not six.  Hey! Also a good source for
twisted pair for DIY cables or internal wiring.  

Thinking about whether there is any virtue in locating preamp on cable or in
handpiece. In current magnetic trackers there is a junction box between
patient lead and big wire to electronics.  Could put amp there, but that would
likely increase magnetic coupling into junction box, which is already
vulnerable because twisting is interrupted, and is near patient.  I wondered
if that weird collar on the medsafe junction was a magnetic shield (though it
looked like solid machined plastic).  But for active handpiece, biggest pickup
in cable is likely cross coupling from power and digital lines. It would help
if signal was big and low Z going into cable, but six amp channels inside
handpiece is a lot, and would be vulnerable to magnetic pickup, both from
source and from actuators. The easiest electrically good approach is a
separate sensor cable.

Clean up residue with adaptive multi notch IIR filter. Use FFT at low rate to
locate top HF spurs. Set notch frequency from center and Q from peak width at
say 30 dB down.  Have fixed number of notches and deploy on top spurs.  Run
all channels (including sense) through the same filter so we don't create any
carrier phase variation. Do this filter (and adaptive FFT) right after lowpass
so that hum and distortion filters don't have to deal with this crap.
Cleaning residue should help the hum filter, since it feeds on residue.
Making carrier not hum harmonic may be problem for carrier notching in hum
filter? Really we should be notching the entire carrier bandwidth, not just a
single bin. But if carrier is not synchronous, then leakage will be a problem.
Would need windowing or something.  But not a lot of carrier in residue most
of the time, and almost all in center peak.
 ==> FWIW: It seems like reducing the LPF corner from 14.5 kHz to 14.0 helps
     somewhat in reducting the cruft amplitude during to loop setting.  This
     suggests that the spurs in the residue are indeed a factor in the poor
     settling. 


What's tracking like during motion?

Switch KF to amplitude and phasor for phase. Then phase can have much lower
process noise.

RMS above a certain threshold is a "noise" status, doesn't affect anything,
just a notification. KF dynamic limiting is an interesting status.  Also
report out state variance, measure noise, etc.


ILEMT sensor: [Also from Google notes]

Hey, I think for preliminary results, we should try using standard unshielded
inductors, non concentric. AFAIK, dealing with non concentric coils is not
that hard, and commercial inductors will be a lot more similar than anything
we can make, and are available with very thin wire, etc.  
[See RFID antennas above]

Could deal with the three axis arrangement by using flex circuit, then
wrapping on three sides of a cube. Put indents in cube that inductor body fits
into. Vertical bobbin inductors are good because the axis can be precisely
indexed from the core top. Round holes are easy to machine, too. There will be
some issues with the cores of other axes distorting the field we are trying to
measure, but this can be reduced by putting the inductors relatively far
apart. I think this may also largely match across axes if we put the cores on
the sensor axes, at equal distance from the origin.  Not sure what shape is
best. Old fashioned pie wound inductors look more like a search coil than a
bobbin does.  Bobbins have large area compared to size, but the flanges give
it a more nearly closed circuit.  Rod antennas would be another possible off
the shelf solution, RFID or AM radio.  Some quite small.

I've proposed making a concentric ball sensor not so much because we would use
it for micron as because more ideal geometry should help with getting good
preliminary results and in calibration infrastructure.  General idea is to
wind on rapid prototype form. Circular windings seem more ideal, but for
mounting during winding and in tests a cube outer profile is handy. Winding
channels in form can be circular (with flat facets so winding crossing over is
not raised up much). Winding sensitivity will not match perfectly, and should
be calibrated in Helmholtz coil, but if number of turns is the same,
inductance should correlate closely with sensitivity.  Also want to rapid
prototype a holder for the coil form for use in winding that can grip on the
three axes. I started thinking some sort of taper method, because that would
deal with loose RP tolerances.  But now I think what would be better would be
a snap-on feature where the holder grabs locking ramps on the edge of the
form.  This uses elasticity to cover up tolerance.  I'm imagining relatively
big snap fingers where the ramp forces the flat faces of the form and holder
together.  Maybe holder could be FDM so it's real plastic?  Maybe good for
form too.  Dimensional stability is more important than resolution. Planarity
is likely good in the fiber direction.  Could square up other faces wrt that
by lapping. The issue is that it's handy to use the faces to index the axis
orientation during test and operational mounting.  We have the problem that
the sensor is large relative to the operating distance, so the field is less
uniform across the coil.  This is less true for a smaller sensor, but smaller
is harder to fabricate accurately. A ferrite core is a help here, since the
sensor can be smaller for given SNR, and core shape largely determines
performance (variations in winding area matter less). Core permeability
thermal drift will have some effect, but will be common to all axes. Could
make two part coil form "bobbin" that fits over core.  For field mapping,
would likely be best to use stage to mechanically rotate so that axis is
assigned *by nulling off axis component*. Much more sensitive than maximizing
on-axis signal. Field direction error contributes much more to overall
position error than magnitude error.


Hum filter:

The hum filter is not working very well, I think because of all the HF
residue. As I noted yesterday, some hum peaks are suppressed well, and others
not.  Possibly this is because of the time-domain approach on spiky signals
with poor time synch.  Interesting that there is a fairly strong 120 Hz peak
in the residue (but not the filtered signal) which is not affected by the hum
filter, even though adjacent hum peaks are well suppressed.  Look at 50-500 Hz
with linear scale.  I think this has something to do with the 120 Hz in the
high rate output.  But something in the residue is something that is *not*
being tracked, and is therefore not in the output...  The peak is still
visibile in carrier model, about 10 dB down.  Possible noise adaptation is
helping us to not track this hum, since our overall response is not 10 dB down
at 120 Hz.


Glitches:

Looking at an output glitch correlated with the furnace, the high rate
disturbance closely tracks the low rate distuurbance (especially during period
of increase).  Also, the number of strongly affected low-rate samples is 7, a
lot like the low-rate window order of 8.  Basically the low rate is seeing an
impulse disturbance (on it's 0.17 sec time scale).  My guess is that mostly
the demodulator is being disturbed by the low carrier error, though there is
an extended recovery tail that is presumably caused by the mutual dependence
between high and low rate (because low rate operates on filtered signal).
Interestingly, only very minimal high carrier disturbance at the time that the
impulse must have happened (the block before the first affected low-rate
sample).  There are several excess spikes throughout the block, but magnitude
is only 7 um.  This is nothing compared to the 150 um low carrier glitch or
the correlated 100 um high carrier hash 


Warmup drift and current sense:

An hour and 20 minutes after powering on driver box:
		High	Low
Position drift: 71	151 (um)
Rel dB:		0.012	-0.161	
Rel phase:	0.109	2.385 (degrees)
Norm rel dB:	0.009	-0.020
Norm rel phase:	0.053	0.861

This should be improved for the reference design, but is not at all important
for right now.  The simplest way would be an emperical drift model based on
temperature estimate from low carrier phase (or just based on phase itself).
Re discussion of capacitive coupling into current transformers below, it's
worth investigating that.  

[Especially given low carrier apparent current cross-coupling [31 Dec], and
that we now know the low wire running through current transformer is going to
have 10's of mV of signal on it, should investigate (sometime) whether
electric shield around wire will have any effect.  This can be done pretty
easily by pulling front panel, removing filters from terminal blocks and
partly untwist to make slack in wire so some can be pulled down through the
transformer shelf.  Then lay copper tape along wire and wrap around.  Leave
tab at end to attach to bottom of shelf.

But as long as the cross coupling is stable and not really going into source
field, it doesn't matter.  If there is any significant mixture of carriers
being sent out, then this will have somewhat screwy effects on field, though
to first order this will just shift the field orientation a bit, which is
something we plan to calibrate out.  One advantage of eliminating spurious
coupling to the sense signal is that it will let us see how much actual
cross-coupling there is.]


Thinking about warmup drift.  My feeling is that the main effect is increase
in winding resistance in the source.  This will cause low carrier current to
decrease (because at LF current is largely determined by resistance), and will
cause positive phase shift as the relative contribution the lagging inductive
component decreases.  This will also cause amplitude decrease and phase lead
in the emitted field.  This is what I see on the X axis low carrier. 

The low carrier phase drifts by > 2 degrees, and I think would make a pretty
good measurement of source temperature, which could be used for temperature
compensation.  

If resistance change were the whole story, we'd expect the same change
direction on the high channels, but reduced by the high/low ratio (15x, for
the X carriers).  In fact, the amplitude change is about 14x smaller, but has
the wrong *sign, and the phase shift has the right sign and is 24x smaller.

Once we allow for current sense normalization, though, the low/high drift is
only 2.2x, which agrees quite well with the 2.4x ratio of position drift.
That is, the current sense compensation is much more effective on the low
carrier.  Interestingly, the current sense compensation for high carrier does
have the correct sign (does improve drift), but only accounts for about 1/2.
That is, both current and phase lead have *increased*.  It may be that second
order effects like mechanical orientation drift or driver effects are
significant for the high carrier.

It's hard to tell what the YZ axis changes mean using the X sensor.  Mostly
the change is a lot more, but this could well be due to minor changes in field
direction due to slight mechanical rotations caused by expansion.  This will
even have some effect on the X-X coupling.

We expect that thermal expansion will increase loop area (and also inductance)
by the square of absolute temperature change, which will reduce current at
constant voltage.  Unlike adding turns, which increases emission proportional
to turns, but inductance as the square, increasing loop area increases
emission proportional to inductance increase.  When driven by constant
voltage, this effect should cancel.  Because the huge "air gap", the effect of
any thermal permeability change in the core should be minimal.  Core loss is
also quite low because of the very low flux (compared to typical power
cores).  IIRC, FEM said core loss was < 1mW.

Effects in the driver itself should be pretty well controlled by the sense
feedback, I think.  The current transformers don't change in temperature much
at all, and should be pretty stable anyway.  The main effect that I can think
of is that changes in the amplifier operating point caused by the resistance
change may couple into the current channel capacitively or magnetically.


[1 Jan 16]

After moving the ground and adding gain trimmers on the drivers, cross-axis
coupling on the low carriers is about -50 dB on the low carriers and -70 dB on
high carriers.  X->Y LF cross coupling still improves about 10 dB when I
disconnect the Y input cable.  Muting Y still doesn't do anything.  My feeling
is that this 10 dB is somehow happening inside the driver box, rather than
being cross-coupling inside the interface.  The -60 dB that's left when the
cable is unplugged may be inside interface.

Glitching by 10's um to low 100's seems to be pretty common right now.  Will have to
investigate this at some point.  

Simulating line sags using the variac, there is a shift in the high carrier
output (10's of um), but the current sense compensates this on the next
update.  There is no visible change in performance until the line goes below
75V.  The audio interface also is fine down to 75V (didn't check how far I
could go).  IMO this argues against short line sags as a cause glitching we're
seeing.  When the space heater cycles on and off, the high rate output glitches
about 5um (nil effect on low carrier).

I closed up the box (except for the back), and performance is noticeably
different, I think better.  For some reason I don't understand, the loop gain
seems to have increased, or something.  I had to back off process noise to 20u
to get the ~200 Hz bandwidth I was getting before.  With 50u, the bandwidth
was more like 400 Hz, and there was a high frequency loop instability.  Maybe
the adaptive noise is lower?

There seems to be some magnetic pickup of the source in the preamp.  It's
sensitive to the orientation of the preamp and distance from the source
(perhaps magnetic coupling through desk).  It does't make any difference if
the preamp is sitting on the interface or source boxes.  Seems like we want a
SMT PCB for the amp, just to keep loop areas down.  It's not terrible,
though.  10's of um.

I'm seeing some low frequency ripple (a few Hz, amplitude of 2 um or so) in
the high rate output. This varies in frequency and amplitude, and I wonder if
this might have to do with our sample clock drifting relative to line
frequency.  Some looking at the hum model together with the chart seems to
support this.  I don't recall seeing this before, or at least didn't notice
the frequency varied.  This may have been masked by the problem with the
distortion filter causing Fs low ripple.  I suppose this could be the cause of
the excess LF noise, when the sample rate is close to matching.
 ==> With more staring, I'm now quite confident that shift in hum phase is
     associated with the ripple.  When stationary, there is no obvious
     periodicity, and ripple rate varies depending on phase drift rate.

Interesting how the variation in the ripple seems periodic.  Minimal, slow,
faster, slow, minimal...  Surprising if either a crystal clock or the power
line is varying like that, but IDK.  Hum phase drift about 7.2 degrees/sec,
which amounts to a frequency error of 1/3000 Hz (300 ppm). I guess what
happens is that our modulation architecture multiplies the hum frequency
error, because the 7.5K carrier is the 125th harmonic.  It is plausible that
the power line frequency is periodically varying by less than 1e-3 Hz, but the
beat frequencies we are seeing are in the multiple Hz range.  300 ppm of 7.5
kHz is 2.25 Hz.  That is indeed the ballpark of the ripple we're seeing.  The
hum filter can't have any effect on the hum harmonic that is nominally at the
carrier frequency, since that is notched out.  And the notch width is a few
Hz.  Assuming that this is simply the hum landing on the carrier, the only
solution I can see is phase-locking the clock to the line, which is something
I had been considering.
 ==> Or if we could hand off to the low carrier, which is *not*
     located on a hum harmonic.  
 ==> Should I revisit the decision to locate the high carrier on a hum
     harmonic?  My thinking was that drift at near-DC was better than 30 Hz.
     But I decided this before I had the hum filter. The filter ought to be
     able to get at hum harmonics that are not on the carrier.  I think this
     may not be possible with power-of-2 FFT without a custom sample clock,
     but that's easier than a phase-locked clock.  It might even be doable in
     software, supposedly some sound devices support oddball sample rates.

Looking at hum lines in the raw signal that aren't at a carrier location is
one way to get a handle on the magnitude of hum on-carrier.  For example, at 6
kHz, there is something there about 10 dB above the noise, flanked by similar
(slightly lower) peaks at +/- 60 Hz.  In the filtered signal, for some reason
the filters pretty much knock out the central peak, but not the sidebands.
The distortion filter is helping some with the central carrier channel, but
the hum filter does more there too.  For some reason, the hum filter is having
nil effect on the sidebands.  The carrier is an even hum harmonic, and I
noticed that these usually have lower amplitude. Of course (as noted above) if
there were a carrier there, then not only could we not see the (probable) hum
line, the filters would not touch it, since it would be notched out of the
filters.
 ==> 19 Jun 16, how is it that the hum/distortion filters are helping
     interference at frequency that is supposed to be notched out of the
     filter? 

Looking at the Chart high spectrum on a linear scale, it's not hard to imagine
that the peak near zero is a continuation of the series of hum peaks.

b.t.w., I noticed some 10's of mV of 60 Hz in the driver output.  I don't
remember the exact conditions.  I think maybe the driver input was
disconnected.  On the X channel only, I see about 10 dB of 60 Hz above the
noise floor.


[31 Dec 15]

Well, there is plenty of work to do on signal process, Kalman demodulator,
etc., but the top priority is to get a 6DOF measurement.  For this, the
three-axis source and demodulation is working pretty much well enough.  Top
priorities are then:
 -- Finish up the driver box.
     - Modulation frequency/current measurement?
     - Attenuator/LF coupling issue?
 -- Three-channel preamp.


Later may want to revisit the power supply issue.  My tentative conclusion is
that the big gliches I was seeing before were due to windows sound lossage,
and seem to be gone.  But with the new power supply there are what seem to be
*new* small glitches (< 100 um, rather than meters) which I think are related
to line dips.  So I may still want the post-regulator idea, either with the
switching supply or the transformer.  Possibly using the aux supply to feed
the driver board low voltage would help too.
 ==> Variac testing suggests we won't notice line dips.

Revisiting low-carrier cross coupling.  I turned off high carriers to simplify
things.  On "Current Y", X carrier is 31 dB down with normal connection.  If I
unplug Y driver, X carrier (cross coupled) goes down to -66 dB, a huge
difference and a much more reasonable level.  But if I just set the Y low
carrier to 0, there is no change.  And when I use another source for the Y
driver (HP 209A), then the cross-coupled X carrier is -72 dB.  That rules out
the possibility that the cross-coupling might be happening in the driver, such
as through the supplies.  So this does look very much like some problem with
the unbalanced connection to the audio interface, perhaps in conjunction with
whatever sort of hack the audio interface might have for unbalanced output.
The UR44 manual block diagram kind of suggests that the outputs aren't
actually balanced, in that it doesn't show a balanced driver.  But the jack
connection it shows also wouldn't work, either with balanced or unbalanced, so
it is presumably doing something else.  Since I don't have a TRS plug, it's
hard to tell what a balanced input would see, but the resistance from
unbalanced ground (ring) to the case is < 0.2 ohms.  Also, the cross coupling
amounts to 3 mv P-P referenced to the driver input.

I measured the ground current on the RCA ground, and got 15 mA p-p.  The only
thing that seemed to change the current was messing with the TR to RCA patch
cables I used to connect the line out on the audio interface to the driver
boards.  At first I thought this might be inductive pickup in the loops formed
by the several shields, but it fluctuated quite dramatically with no
particular correlation to loop area.  

Also, I discovered that disconnecting the source coil or disabling the drivers
caused the ground current to go away, but that moving the source coil had no
effect (not inductive pickup from source).

I checked conductivity on the patch cables, and things seemed pretty o.k.,
except that the RCA plugs were the type with a ground contact that has no kind
of grounding ramps or fingers.  One of the center conductors showed resistance
of up to 3 ohms as I messed with the cable where it went into the TR plug,
which is not great, but doesn't explain any kind of ground current
variation. I bent the shell ends of the RCA plugs to create contact areas and
reassembled.  Now I got a current of 150 mA flowing in the RCA ground!

I retried unplugging the XLR feedback connectors, and this time I did get a
clear indication that one of these had to be connected for the ground current
to flow.  Once again, there was substantial difference between the cables,
presumably in the resistance of the ground connection.

So the deal is that on the driver board there is 25 mV of voltage difference
between the RCA and the power ground terminal, even when only one driver is
running. When I lift the ground connection on the supply common bus, the
ground current drops to a few mA.  Grounding the RCA input to the chassis
plate actually reduces the RCA ground current by 12 dB.  What is left is
pretty much unaffected by whether the driver power is on or off.  Is
line-synchronous bursts of hash at about 150 kHz, presumably from SMPS.  This
is coming over the USB cable from the laptop. This I was able to reduce quite
dramatically by running the USB cable through a toroid seven times.


[30 Dec 15]

I think I am going to want to do something about the new HF spurs.  The
residue amplitude is way up, and is substantially periodic with a period of
about 106 samples.  This is composed of bursts of the 6 sample repeat (16
kHz), which I think is showing up in the spectrum as the spurs at 15,457 Hz
(and surrounding hump), as well as 14,543 Hz.  Though the filtering has mostly
adapted, this does interfere with our ability to identify smaller impulses,
and I suspect that it is also causing lock-in problems.  Now, after a glitch
it takes a noticeably long time for the high carrier noise to settle back down
to where it had been.  Correlations are fairly flat out to 416 samples.  106
205/211 416, the correlations drop off.  The 9th peak is at 944 samples,
suggesting a "true" repeat of about 104.9 samples.  Although the amplitude
varies, the location of these residue bursts is syncrhonous with our low
sample rate (and the excitation sequence).  This seems to be what the
sigma-delta converters do when given this multi-tone signal.  SAR converters
would presumably avoid this, though might have different issues.
 ==> The averaging filter approach would surely help some, but the *amplitude*
     of the pattern is so variable, that it might work better to have a filter
     with just two taps, at 6 and 105 to 106.  Or even better would be some
     sort of "matched notch filter" that uses many samples in 100-400 sample
     window.  There is much correlation between 6-cycle repeats within a
     burst, and also some correlation between successive burst amplitudes, as
     reflected in the relatively flat correlation between bursts at lags of up
     to 4x the burst period.
 ==> This bursting is basically just the sum of those two spurs (above).  The
     difference is 914 Hz = 105 samples.  So notching both spurs should clean
     up the mess a lot, and without requiring a complex or high-order filter.
     IIR notch filters might do the trick.  The Q could be 20 or more,
     reflecting the tendency of the amplitude variations to extend across
     hundreds of samples.  There is really a big mess of spurs around and
     above the 13.5 carrier.  The next biggest after those two is 13,629 Hz.
     Some kind of FIR filter may make more sense, though, since the spurs are
     phase coherent or something.  One way to think of this might be as a
     noise whitening filter.  Do system ID assuming the output is a dynamic
     system excited by white noise.  Of course noise whitening is 
     something like the opposite of the noise shaping that a sigma-delta
     converter does.



Anyway, changing distortion filter to work on "Filtered signal" (residue +
carrier model) seems to have licked the distortion-filter-causing-ripple
problem [See 29 Dec 15].  Looking at the overall spectrum of output,
the most noticeable features are humps surrounding 60 and 120 Hz (though not
right *at* hum harmonics); These are presumably hum modulation.
 ==> After jacking up the bandwidth (below) there is still some base
     broadening in the carrier prediction which is not present in the raw
     data. This local noise floor rises about 8 dB when doing RMS spectral
     averaging.  With vector averaging, this is not seen, so whatever this
     crap is, it isn't phase coherent (as would be expected from the non-line
     spectrum). Interestingly, the hump is bigger for the higher carriers.
     Note that much of the perception of humps in the carrier model is because
     the carrier model contains little noise away from the carriers (below the
     input noise floor.)

Turning off hum filter shows it is attenuating 120 Hz sidebands of 7.5 kHz by
about 10 dB, looking at the filtered signal.  But it isn't entirely clear that
this is reducing below the raw signal levels, switching between raw and
filtered, with hum filtering on.  It doesn't seem to make any difference in
the hum sidebands in the carrier model (which is what is getting through to
the output.)


Interestingly, with RMS averaging, the local noise floor near the high
carriers in the filtered signal is 5-10 dB lower than in the carrier model
(worse for higher carriers).  What does that mean?  I think it means that the
KF is tracking noise in the input.  When we add the carrier model back to the
residue, then what?  It seems we've just moved noise energy from the residue
to the carrier model.  When we split the input into carrier and residue, are
we adding correlated noise to the two which cancels when we add them together
again?  It's a mystery, though once again, likely not a very important one.
But we will see the noise floor of the carrier model, since that is what we
output.

Another interesting related contrast is between the noise floor in the residue
in the high carrier region and above.  (clearest with RMS averaging, from 5 to
20 kHz, log scale) The noise floor is raised in the residue near the high
carriers, but this isn't present in the filtered signal.  I think these are
all the same thing, given the simple additive relation between filtered
signal, carrier model, and residue.  Similar humps are in the carrier model
and residue, except that the residue includes the peaks at 9.5 and 15 that are
present in the raw input.  Then when we look at the filtered signal, the
"noise" in the carrier model and residue seem to cancel out, resulting in a
filtered signal much like the input.  But our demodulated noise is determined
by the carrier model.  So it would seem that something about the Kalman
demodulator is increasing the noise floor in the regions that we actually care
about.  Noise that "should" have been left in the residue has been moved into
the carrier model, or something.  Would be nice to know what this corresponds
to in the time domain.  Changing the KF process noise doesn't vary the humping
the carrier model, it just moves the entire spectrum up and down.  

Though the effect on the input or output spectra often seems pretty minimal,
the distortion filter is pretty dramatically reducing the high rate "noise",
almost 2x with process/measure 50u/100u.  Overall RMS in the chart high
changes from 1.5 um RMS to 2 um.  Given that there is LF noise, the effect at
HF alone is bigger.  Likely another aspect of this is the reduction in spurs
in the carrier model spectrum, also quite visible when we turn the distortion
filter on.  It's a bit of a puzzle where all of those spurs in the carrier
model come from, but I think it's most likely due to discontinuities in the
model update and high and low rate block boundaries.  These are far too small
to be visible in the time domain, but are still there.  It might help noise
performance if we smoothed these transitions (might even have to do with the
hump effect).  Turning detrend on and off doesn't seem to have any effect on
the HF noise floor (or anything else that I know of.)  The carrier model and
residue also have a lot of HF spurs well above the LPF cutoff in the input
filter.  But once again, these are not seen in the filtered signal.  In fact,
the HF is a bit lower than raw data in the filtered signal.


The hum filter has minimal affect on output RMS, but it looks like it may be
helping with some spiking that increases the max a bit.  I have no idea why
that might be so.



There is still the noise density increase at low frequencies  The RMS noise
of the low carrier is much lower (due to much lower bandwidth), but the LF
region has a higher spectral noise density.  DC stability seems good, so this
isn't 1/f noise or drift.  Possibly stability is better than before?  We seem
to only have drifted a few um in 10 minutes. Possibly mechanical changes
(varnished coil, foam/cable tie mount) could be a factor.

Source coil is running barely warm to the touch, with currents:
    [2.486, 2.335, 2.041]

When I increase current to:
    3.528	2.934	3.023	

Then noise does improve.  At this current, the source is somewhat hot (takes
10s to get uncomfortable).  Audible noise increases too.


Mixer settings have the outputs at -18 dB, so we are losing several bits of
D/A resolution.  The drivers have 3K input impedance, so any earlier
attenuation has to keep the source impedance fairly low.  For one thing, there
is an RF filter capacitor whose corner will droop.  I'd rather not mess with
trimming the gain setting resistors because this may couple crap into these
sensitive nodes.  It seems the best approach would be an upstream voltage
divider with total resistance 1-5K.  We don't want to load down the interface
too much because that will likely increase distortion.


When I was messing with calibration, the loop did unlock once, and I wasn't
able to get it back without restarting.  Not sure what happened there.  Moving
does cause transient unlock indications.  Muting the X axis for a bit seems to
cause stuck-in-unlock also.  Initial convergence may be benefitting from our
forcing us to start in unlock after reset.

Distance noise performance seems roughly the same as before, or even a bit
better, which is interesting, given that we are now combining all three
carriers.  

There is a fairly strong close-in spur at 13,629 Hz.  This is attenuated only
about 6 dB by the distortion filter.  This is pretty heavily attenuated by KF,
just because bandwidth is low.  It no longer works to run with KF off.  The
loop seems to go unstable.  Based on noise spectrum, it would seem that KF
bandwidth is now only about 10 Hz.  It seems that the guard band noise
estimate has gone up a lot.  This may be partly because of adjacent channel
interference, but I'm guessing that there's a lot more spurs and crud with the
full set of carriers.  Increasing process noise from 5u to 50u is giving a
bandwidth of ~200 Hz, with 1.7 um RMS range noise.  Noise density is about 20
dB higher below 0.5 Hz than above 1-2 Hz.  Although this is roughly the
high/low carrier crossover, it doesn't seem to be a high/low carrier effect.
The high output also increases to the same level at the same frequencies.
Though there clearly is some knock-on effect of low carrier estimate on the
high carrier demodulation, this output is supposed to be purely the high
carrier amplitude.

There are (near) 60 and 120 Hz hum peaks in the high carrier output which come
and go, and don't seem to be affected in any way by the hum filter (on the
high rate data).  Although spectrally quite distinct in 10s of data, there is
enough broadband noise to swamp this for most purposes.  
 ==> These appear as sidebands in the carrier model, but interestingly *do
     not* appear in the raw data.  This suggests some sort of demodulator
     misfeature.  Possibly this could be due to noise adaptation to hum peaks?
     When KF goes instable, seems to feature 60 Hz, perhaps due to hum filter
     interaction.

I noticed that the noise estimate varies wildly from one block to the next.
I've increased the minimum measure noise from 5u to 100u. It's hard to tell
what effect anything has on the output hum because these peaks are constanly
varying. Likely the noise estimation can be improved in various ways.
Currently estimate is based on only first channel, which is wrong.  But there
is also a lot of adjacent channel leakage.  Unless there is some sort of
glitch, leakage is mostly what we are seeing.  It might be more accurate to
use the *mininum* guard band level, as this would respond most strongly to
whatever carrier happens to be weakest (due to sensor orientation).  Also, we
are already doing a longer window DFT in the impulse filter for noise
adaptation there.  This would have less carrier, giving a better noise
estimate.


Huh.  The effect of my hand on distance measurement is now nil.  That's good,
I guess.  Maybe this is because of the carrier combination in measurement?
Vector magnitude not changed?  Or it could well be an EMI thing, since even
without the enclosure, the layout is a lot more compact and on a ground plane.
Metal objects large w.r.t. workspace size start having an effect on the
measurement at about 18" off axis.  Sensitivity is higher behind the source
than beside it.  I see little difference between the high and low carriers
with 0.064" (1.66 mm) aluminum plate, but with 0.42 mm aluminum the high
carrier is much more strongly affected.  As I guess you would expect from
physics, the size of the interferer seems to matter more than the mass (once
it's thick enough).  A thick steel pipe has less effect than a larger area
sheet.  Outside the workspace, it seems as though eddy current may be
dominating, even for carbon steel (both increase apparent distance, tracking
each other fairly well).  As you get closer, various flip-flops happen in the
effect of large steel objects on the low rate measurent.  Even aluminum foil
(approx 20 um thick) has considerable effect on the high carrier, but
basically none on the low carrier.

Looking at X (7500, 457 Hz), the high carrier typically shows an order of
magnitude more phase shift in the presence of metallic interference.  If there
is phase lead, this seems a sufficient indication for presence of a soft
ferromagnet (like steel).  Phase shift is a fairly reliable indication of
metallic interference, though the relationship is complex.  Ignoring oddball
materials like ferrite, 1mm errors seem to be associated with phase shifts
approaching 1 degree, which is way above the phase noise floor.  If there is a
departure from this 10:1 ratio, and one carrier is much more affected, then
this often seems to point the finger correctly.  With ferrite between the
source and sensor, there is very little phase shift, even with large error,
but there was substantial phase shift on the Z axis.

Phase shift has to do with loss, either eddy current or ferromagnetic.  If
material is non-lossy, like thick copper or ferrite, then there can be
significant error without much phase shift.  

Phase shifts on the off axes can also be quite informative.  With ferrite
between the source and sensor, there is very little phase shift, even with
large error, but there was huge phase shift on the Z axis low carrier (44
degrees if core laid along X axis!).





Still some confusion about what axis is what.  I've been randomly swapping
connections trying to get it right. o.k., the X and Y sense connectors were
swapped at the interface.  And now output connections seem to line up as I
would expect.

For off-axis coupling on current, when I disconnect the Y driver, the Y
current drops to the noise floor (as expected), but the coupling from X and Z
also drop 9 dB.  Muting the output seems to have no effect on the
cross-coupling, but disconnecting the cable does.  This suggests
cross-coupling between output channels in the interface or cable.  Output side
coupling is more critical because of the >10x lower source impedance. 

Would also be interesting to compare to loop-back configurations, as this
separates out the contribution from the interface.




[29 Dec 15]

o.k., think I understand the distortion filter lossage now.  I had observed in
the past that once low rate ripple gets into the distortion filter it seems to
stick.  The other key pieces are that the ripple appears (as base broadening)
in the carrier model and (mostly) not in the residue.  The reason the ripple
grows and doesn't adapt out is that the filters are working on the *residue*,
which doesn't have the carriers (or base broadening) because those are being
tracked by the KF and passed through to the demodulated output.  So the
distortion filter is not getting any error signal for the ripple.

While it would work to broaden the notch, this would also defeat any
usefulness of the distortion filter for removing close-in spurs.  [Note that
the hum filter already has a fairly broad notch (60 Hz), which must be at
least part of the reason why it doesn't cause so much trouble.]  It would still
remove spurs not near any carrier, but those are precisely the ones that (to
first order) don't affect the demodulated outputs.  It's as though (through
the wonders of feedback) the distortion filter is taking noise and evolving it
into biggest signal that the KF is willing to track.  [Removing hum and distant
spurs is still somewhat useful for cleaning up the residue in the time domain
so that we can do impulse detection.]

Fortunately, there's an easy good solution, which is simply to take the
filters out of the Kalman demodulator feedback loop.  I had put them there
when I was thinking of use ANC, and came up with the idea that since the
residue has (nominally) no signal I could filter there without worrying about
removing signal.  But then I ended up putting back FFT notching to get the
thing to work, and now there isn't any reason to keep it inside the Kalman
demodulator loop anymore.
 ==> I ended up doing something a bit more complicated, and it took more
     fiddling to get things locking in again.  The hum filter has to continue
     to work the way it was, on the residue, since FFT notching of the low
     carriers doesn't work with only 60 Hz resolution.  I also changed the
     unlock indication to be more sticky, which greatly speeds up
     convergence.

There's still something not quite right, where settling seems to happen all
over again once the unlock indication goes off. This is something related to
the first data block after reset not getting tagged as noisy (unlocked).


[28 Dec 15]

Have code with three channel demodulation turning over.  Unfortunately, it
rapidly diverges when carriers are present.

Looking at placement of low carriers.  I'm thinking that using alternate
channels may be too close together, though with the high low carrier window
order (8 currently), that may be o.k.  It also helps with cross channel
leakage that the low carriers are not very strong.  It looks bad in the
spectrum display of raw signal with B-H window, but that's because our window
order is only 1.  I think that looking at the STFT low result with *no*
display window is more accurate.  Interestingly, the noise floor is about 8 dB
lower, and some of the strong hum peaks (420) are basically eliminated.  

It's notable that in the raw signal odd hum harmonics are much stronger than
even ones.  My guess is that this is a consequence of the higher hum harmonics
being due to asymmetric spikes.

The old low carrier, 562.5 (96) does seem to be well located, but I think it
would be better to go down rather than up.  78 and 87 look good.

o.k., after fixing a couple bugs and adding a feature where we use a much
higher fixed measure noise when unlocked, the loop is now locking up.  I think
we need to allow more than one high-rate period for lockin to happen because
there is a feedback loop through the low-rate.  It still takes puzzlingly long
to settle, and interestingly doesn't seem to settle at all if the hum filter
is off.

A new thing, possibly multitone effect, is that there is a relatively strong
line peak (-80 dB) at 15.475k.  This is very much dominating the residue
spectrum, and appears to be degrading the lockin.  There is also a broad noise
hump slightly higher, near 15.645 kHz. When I reduce the LPF frequency to 14.5
kHz, lock-in and residue amplitude are much improved.  The distortion filter
wasn't cleaning this up because the amplitude limit was too low.  I've raised
from 100u to 1m.  Now it only takes ~6 input blocks to get out of "unlock".
Then the distortion filter has to settle for the result to be completely
settled.

Another thing I haven't noticed before is that there is significant broadening
of the base of the carrier peaks in the filtered signal, and on downstream
(including low rate STFT).  This somewhat visible in the residue also, but
magnitude is not so high.  This is quite significant, -141 dB on "Raw data"
vs. -107 dB on "Filtered signal" at 7.453 kHz (37 Hz away from carrier).  

This seems to have somewhat to do with the large Fs low ripple (60 um peak
amplitude) that I'm seeing the high rate output when the distortion filter is
on.  When I turn it off the base height drops quite a bit, down to -128 dB,
but not as low as it is in the raw signal. The distortion filter interaction
is not new, but was intermittent before.  Now I'm seeing a lot of HF content
at Fs low harmonics, cutting off quite abruptly above 76 Hz.  RMS amplitude of
high rate output is 25m.  With distortion off, it's only 3.4m.  So distortion
the filter is really not helping.
 ==> Maybe this is due to carrier notches in the distortion filter being too
     narrow? I remeber thinking about this before, though I can't find it in
     these notes.  The distortion filter may be a bigger offended because it's
     synchronous with the modulation (?)

Note that "low" frequencies we see in the demodulated output don't correspond
to the low cut in the distortion filter.  I haven't noticed any effect of
changing the low cut.  Low frequencies in the output *do* correspond to base
broadening (or sidebands) at the input sample rate.  This output ripple is
pretty deterministic too. 
 ==> Uh, yeah.  So the base broadening is sitting right there in the
     distortion filter.  Clearly we could get rid of it by broadening the
     notch, but this would also largely eliminate any benefit of the
     distortion filter for in-band spurs, since we have to go about +/- 60 Hz
     to get the base broadening.  One thing I wondered is if there may be some
     bad effect of the brick-wall notch.  This is related to FFT windowing by
     time-frequency duality.  I guess a sharp frequency feature results in time
     domain leakage, analogous to how a sharp window causes frequency domain
     leakage?




We do expect some base broadening in the carrier model, representing the
carrier variation that the KF is tracking.  But this should not be above the
raw noise floor.  (Consideration may be needed for difference between noise
floor seen by KF and what we see at the read-rate spectrum.)  Likely this
would be improved by changing the KF to treat phase error differently from
amplitude error.

Not entirely sure what to make of it, but on the sense inputs there is a lot
more off-axis current signal at the low carriers than at the high carriers.
An explanation for why this could be real is that smaller drive voltage
variations create larger current variation due to the lower impedance.  This
could be either due to coupling in the coil or in the D/A and analog
connection.  The low carrier amplitudes are also just plain lower, so
output/input channel cross-coupling within the audio interface could also
explain some or all of this effect.  On "Current X", low carrier Y is only
-8/-42 or -34 dB down from low carrier X, whereas high X vs high Z is -11/-61
or -50 dB.  Though I might be wrong, if there's no design blunders in the
interface, then the cross-coupling should increase proportional to frequency,
rather than being higher at lower frequencies.  Y low carrier coupling is even
worse, only -30 dB.  And Z isolation is best at -8/-53, or -46 dB.  This
amount of variation suggests to me that it's something on the output side,
probably outside the audio interface.
 ==> I'm currently attenuating the drive digitally in the interface.  
     Cross-coupling would surely be improved by moving the attenuation to the
     driver box, where it would be referenced to the driver ground.  Digital
     attenuation is upstream of whatever analog nonidealities happen in the
     interface, whereas analog attenuation at the driver would be as far
     downstream as possible.



Another odd thing is that the spectrum display disagrees with the carrier
levels display, about 10 dB low in the spectrum.  I vaguely recall something
like this from before, so probably not new.  Seems to be a spectrum display
issue, since the time domain looks about right. Almost 4V P-P on the low
carrier, which should indeed be about +6 dB.  I suspect this is some sort of
normalization issue.  10.7 dB, 3.423?  





[21 Dec 15]

Output connector on driver box:
2: x+ 
5: x-
7: y+
6: y-
3: z+
1: z-

[### maybe.  Got x/z swapped by the end of the cable first time (Looking at
wrong side of connector?) ]

The cable, wire with the print on it is +, except for X, where they are
swapped.

Wound Y and Z coils.  Was going for about 40 turns, same as X, but ran short
on wire, so ended up a bit shorter.

Y coil: 39 turns, 184 uH, Q=4.0 @ 1 kHz
Z coil: 35 turns, 158 uh, Q=3.6

I also removed the entire partial layer on the X axis.  I don't seem to have
recorded the new inductance, but it was about 200 uH.

Using 2.2 uF, 1 uF and 0.82 uF for XYZ resonant capacitors (7.5, 10.5 and 13.5
kHz design freq).  





[9 Dec 15]

re. [13 Oct 15] notes about source winding design.  Yes, we want to dedicate
more copper and power budget to lower frequencies.  But if anything, we want
to reduce N as frequency reduces, so we can drive in even more current than
the frequency reduction.  That is, if we believe the [16 Nov 15] results of
gain decreasing when we hold source voltage constant.  My intuition is that
the linear inverse relation between current and frequency should cancel the
proportional dependence of dB/dt on frequency.
 ==> Do we have a frequency^2 response at constant current?  What exactly are
     we seeing, and why?

Increase in current at LF is partly made up for by lower AC resistance, but
ideally we should increase the copper cross section (Litz count).  For source
design, the key limits of the tradeoff space are the source voltage (initially
set by safety concerns, and now by the driver) and the source dissipation (for
burn hazard).  In this scheme, we want to add turns until we reach our voltage
limit, since more turns increases the field.  More turns also increases the VA
as n^2, which is a nuisance, but those losses or costs take place in the
driver. 

Specs on as-built core from [22 Sep 14]:
    45 turns 33x20, with slightly larger core more accurately representing the
    40mm cube I've got.  Max voltage with this amp is something like 42V, so
    having a few turns less gives me the possibility of cranking it up to 11.
    The larger core boosts flux per watt, of course.

    That's 302 uH.  As built, I measure 250 uH. I ended up with 2.6 layers,
    spread over almost the full core width.  In FEMM the winding has larger
    margins.  Then there's the issue of approximation of the square section.
    Possibly effects due to the non-solid pieced together core.  Reasonably
    good agreement, I would say.  Equivalent to 4.5 turns, or 10% of turn
    count.

Removing the entire partial layer would be overkill to make resonant with 2.2
uF at 7.5 kHz. We would only want to take off 5 turns to bring down to 200 uH.
But reducing turns is not so bad for efficiency with the resonant output
filter, since driver doesn't supply the current.  Would be better for
efficiency and cable emission if capacitor was at the source coil, but that
doesn't work quite so well with current transformer feedback.  We could also
go back to the idea of sense windings at the source.  Removing the partial
layer should bring us down to about 150 uH, or 3 uF capacitor.

Note that r.e. copper scaling, at least 1/2 of the current is at low carrier,
so the difference is not that big.  Might be a better idea to add a shorter
thicker winding for 7.5 kHz.  Would also be interesting to do before/after
tests of input power for the same winding after new windings are put on top.



When it's time to design digitizer boards for reference design, need to look
into ADC options more carefully.  There are options at a higher (but not
insane) price point ($50/channel) with higher SNR and SFDR, such as the
LTC2380-24 24-Bit, 2 MSPS SAR ADC, which has a builtin SINC decimation filter
and a simple interface.  At sample rates of interest we'd get a dynamic range
approaching 120 dB, with full-scale input.  Unlike the PCM 4220, which seems
to distort somewhat above -20 dB input, the LTC2380 is rated at full-scale
input.  The high frequency spurs I'm seeing with audio converters are also
disappointing, though lowpass filtering is a workaround.
 ==> Need to understand better how datasheet SNR and dynamic range relate to
     my FFT plots.  AFAIK, the datasheet must be quoting w.r.t. some specific
     bandwidth, as SNR keeps improving as you narrow FFT resolution.  That's
     the whole principle of the ILEMT FFT demodulation.  In the LTC2380
     datasheet, SNR is sometimes read off of a FFT, in which case measurement
     bandwith is clear.  In other cases, for SINAD, etc., this is presumably
     for some "full" bandwidth, perhaps the Nyquist bandwidth.


There could be benefit from direct digital output to the driver.  I
haven't done a quantitative comparison, but my impression was the the spurs
were pretty similar with converter loopback vs. with the driver, amp and coil
in the system.  If true, we are being limited by the converters (and/or
internal analog effects in audio interface).  Direct drive would avoid the
DAC, if that's an issue, and would also avoid driver analog noise.
 ==> Would be interesting to do tests of SNR and SFDR at frequencies of
     interest, comparing loopback and full signal path.  The design goal is to
     make sensor or LNA noise the limiting factor, so if spurs or converter
     noise are dominant, then we can likely do better with a better converter
     or more care to signal integrity issues, etc.  Noise floor will likely
     increase when source drive is present.  If increase is bigger in full
     path vs. loopback cases, then this is presumably diver noise. 

I looked into generating a line-synchronous conversion clock.  It does seem
likely that this would help hum performance somewhat, especially for
hum-impulse adaptation.  Silicon Labs has a number of interesting digitally
controlled oscillators with good performance and narrow range tunability.
Some present the classic VCXO analog interface, but you can get the same
effect by digital control.  It would likely be possible to modify one of the
digital audio interfaces to use a phase locked clock.

A software approach to the same issue would be add a phase tracking feature to
the hum filter.  Since we are already doing frequency domain processing on
each filter update, it is a small matter to do a fractional cycle phase shift
of the hum model.

I am not currently making any use of the hum filter sigma output, which would
especially benefit from a solid hum phase lock.  One issue with using the hum
sigma is that it wasn't clear how it could converted to an in-channel noise KF
measurement noise.  But perhaps it could be thresholded and made use of in the
time domain somehow.  Also, with suitable scaling, integration of the hum
sigma over the high rate block could give a noise estimate, for example under
the assumption that the variance from the hum model is white.  I can also
characterize the spectrum of the hum model residue, either offline for
modelling, or as a real-time adaptation.

Clock phase noise could cause 1/f noise in our demodulated outputs, though I
think this could be avoided if everyone is using the same clock.  Perhaps we
could also see driver effects in the base-broadening of the carrier peak, such
as from non-synchrony of driver clocks.


[23 Nov 15]

Unresolved issues:
 -- Hum impulse performance not so good. Line phase lock of ADC clock
    might help. Make use of hum/distortion sigma values?
 -- Lockup mode where impulse and distortion filter create large ripple at Fs
    low which doesn't go away.
 -- Sensitivity to line sags?  Furnace blower effect.
 -- Large glitches.  Due to windows sound driver?



[21 Nov 15]

Up and running with new PS.  THere seems to be periodic impulse noise that
comes and goes.  I also noticed a big effect on low carrier of whether I
connect the scope ground to the base plate.  Phase of low carrier has been
less stable too.

Impulses rate is every 1600 samples, with anti-correlated impulse at 800.
What we see in the demodulated output is just hum (60 and 120 Hz).  Amplitude
varies hugely, from near zero to +40 dB.  Looking at hum sigma is
fascinating.  Pattern is huge and not stationary.  Maybe due to sampling not
being line synchronous?  At the moment, I'm getting 2mm p-p hum on output, and
it keeps growing.  That was being caused by the hum filter somehow (without
impulse/distortion).  But with all filters off there's huge 300 Hz ripple.  It
seems that the hum filter is turning 300 Hz into 60 Hz, while keeping the
amplitude about the same.  Looking at the residue, the hum filter is actually
cancelling a lot of hum, leaving only the impulse.  Looking at the setup, I
think it's coming from the variac, which the supply is currently hooked to,
and is pretty close to the sensor.
 ==> Yes!  Clearly we need some improvement in our hum impulse handling.
     Also, we may want to line-synch sample clock.  But hum filter was
     adapting pretty much fast enough, I think.  Just that the impulse was
     moving too fast, and we aren't currently using the hum sigma signal IIRC.


Oh, and there is still big glitching.  So now my top suspect is windows sound
I/O.  This mode looks a bit different than I recall, though, in that we see
stepwise motion (at low sample rate) in the in the high rate output, and this
exceeds in magnitude and leads in time the change in low demodulator output.
Maybe the magnitude difference is just because I haven't recalibrated after
increasing low carrier drive current?  But that should be compensated by
current sense, right?


Also, with driven load, measuring B- current, efficiency seems farily flat at
the upper 1/2 of the pwm freq trim range (degrading at low end).  Trim does
not currently go high enough to show a clear minimum.  B+ bus current might
tell a different story, since gate drive comes from negative bus.

Can definitely increase low carrier current 2x without overloading driver.
I've been puzzled by why current RMS seems to disagree with current spectrum
w.r.t. the relative amplitude of the two carriers.  I think this was just a
confusion.  I had adjusted the low carrier so that low alone was 1A RMS and
high+low was 2A RMS.  So I was thinking this meant the were each 1A RMS.  But
I see that high alone is 1.75A RMS.  The thing is that since this is a mixed
frequency system, the crest factor is higher.  The two carriers partially
interfere, resulting in a sum current that is lower than the sum of the two
RMS values.  If I make both 1.75A, then the total RMS current is 2.5A.  And
now the two currents are exactly the same in the spectrum.
 ==> The current sense will also be non-flat (of course), which will give some
     error in the relative strength and phase measurement.  We don't want low
     carrier to fall in the rolloff region because this will compromise the
     accuracy of the current stabilization.

On the current channel, there is noticeable increase (1 zero?) in the noise
floor below about 1.5 kHz.  Probably this has always been there, and is
presumably due to flat LF noise in the driver being passed through the source
coil inductance.  This seems to level off below about 100 Hz.  This low corner
roughly matches the LR time constant expected from the source (and lead) L/R
(160 Hz for 240 mOhm at 240 uH).  The apparently flat noise floor from ~2 kHz
- 20 kHz is most likely due to noise current falling below the measurement
noise floor.


From Fall '14 entry:
    On GR 1633-A, L = 285 uH, Q = 25 @ I = 1A AC 10 kHz, R AC = 0.7 ohms
    (including 2m leads).  This would give dissipation of 2.8 W at 2A, fair
    agreement with VAW result.  Rac would be lower with the current partly at
    low carrier.  Rdc = 0.373 including cable.  Coil alone something like
    0.240 ohm at DC.  FEMM says 301 uH, 0.243 ohms, but that should be an AC
    resistance, so there is some simulation problem.

On GR 1608 @ 1 kHz, small signal, including the cable, I'm getting 245 uH and Q
of 3.4.  That would be an AC resistance of 0.453 ohm.  This is consistent with
earlier measurement, given the lower frequency and drive level.  Lower
inductance value a bit puzzling, in that I was wasn't expecting much of the
core nonlinearity that tends to increase inductance at high AC levels.  Of
course one or both could be out of calibration too, though bridges tend to be
pretty stable.

Started thinking about the interaction of output filter with source coil in a
broader way.  This was triggered by my realization that the 2.2 uF output
capacitor was way too big, because the high carrier was creating a current in
it that was ~5A, much higher than the source current.  Then I realized that if
I chose the output capacitor correctly, the source coil and capacitor would be
*parallel* resonant.  So rather than sucking down current from the driver, the
output capacitor would be supplying almost all of the high carrier current to
the source.  Then the driver supplies mainly the low carrier current (and also
the high carrier losses).  This does indeed work.  One thing I've noticed is
that there is less audible whine.  This was likely mostly coming from the
output filter components.  

I would expect that losses would be lower, but this would require a more
careful experiment, because both the input and output current varies as I vary
the output capacitor.  Using a resonant output capacitor seems a no-brainer,
though it does add an interaction between the source coil design and output
capacitance.  We are not relying on a high-Q resonance, so the true resonant
frequency doesn't have to precisely match drive frequency.  All this does is
reduce losses in the driver (TBD).  We can even drive at a non-standard
frequency to avoid narrowband interference; it's just that the driver will run
a bit warmer.  The capacitors we are using are already in this order of
magnitude anyway.  

Convention: I'm going to refer to the drive channels as XYZ, and make the high
carrier frequencies go from low to high across these.  

I have a partial theory for why the distortion filter is causing output peaks
at the low sample rate (and harmonics).  The partial theory is that if we have
any error in our distortion prediction (noise or adaption to non-repeatable
signals), then it will create a strictly repeating disturbance, hence the
spectral peaks.  What I can't explain is why we sometimes get stuck with
really big visible ripple, and we can usually recover this by restarting.
This seems like some sort of stuck mode.

I have increased the low-carrier window order from 4 to 8.  As expected, this
reduces low carrier noise and increases lag.  We'll have to see what happens
with we have KF fusion of the two carriers, but it currently seems that if we
have equal high/low noise, we get a significant LF noise boost.  We could also
increase low carrier amplitude even further.  Maybe also relocate low carrier?
562.5 Hz is moderately close to 540 Hz, but our bandwidth should be much less
than that with 8x window order.

Noticed a 15 um glitch when the furnace switched on (main blower), with fairly
long time constant, maybe line droop?  Aprox 3s long, but with a relatively
rapid rise on the high rate output of about 100 ms.  This is nothing like a
switching impulse.  Maybe we do need regulated supplies.  This is repeatable,
too. The only anomaly I can see other than in the demodulated output is a big
change in the trend residue, where (during the same high rate block that the
output step appears) we suddenly start seeing a bunch of sine residue (which
is removed by detrend).  At first I thought this was low carrier, but it's
actually ~60 Hz hum.  There is a big increase near 60 Hz in the input and
detrend residue, etc.  Peak is somewhat broader than baseline hum.  This may
not be strictly synchronous due to induction motor slip.  It may be that the
hum increase is not causal in the output glitch, though.  It could still be
due to power supply droop.

Still seeing the thing where we can get stuck in impulse mode due to an
interaction between impulse and distortion filters.  Turning either impulse or
distortion off for a bit recovers.  Sometimes when you turn impulse off you
can see there is a big glitch in the distortion model which takes a while to
adapt out.
 ==> Basically, it seems impulse filter is clipping an impulse error in the
     distortion filter, preventing it from getting adapted out. The impulse
     likely got into the distortion filter in the first place when there was
     an input impulse of some sort.  Or marking the impulse as range to not
     adapt to.  There is already some handling of the interaction between
     these two, but clearly not right yet.


[19 Nov 15]

I replaced the bus capacitors in one of the amp modules with 1000 uF low-ESR
capacitors.  Measurement suggests this is not likely to do much for the 300
kHz ripple; although the ESR is somewhat lower (7.8 mOhm vs 11 mOhm), measured
at the capacitor in-board, even at 160 kHz the impedance is inductive (~ 15
mOhm), and about the same between the two capacitors.  It may make some
difference for high-carrier ripple, where the capacitance difference could
come into play.

I looked at pickup of 300 kHz switching hash from the current transformer.
This seems to be almost purely capacitive (especially the harmonics).  With
foil sheild around the transformer and connections (7A22 differential hookup)
there is still some fundamental, ~50 uV p-p at reasonable distances.  That's
2.5 mA, or about -60 dB re high carrier.  Without shielding, pickup is maybe
5-10x more.  Not clear how much of a problem this HF pickup would be; you'd
think audio interface should do a good job removing this stuff.  But might as
well take reasonable measures to minimize, since getting RF into the audio is
possibly problematic.  Also, I'm switching to a 100:1 CT with a 10 ohm burden
resistor, which will give a 5x larger signal.  This should help to improve SNR
and reduce effect of interference.

I also looked at pickup from driver at sensor.  Even with the LPF in the amp,
some 300 kHz does make it through if you get close enough.  I expect it will
be fine with the amp in the box and sensor at a reasonable distance.  Using
the sensor, the field from the power supply becomes much more noticeable.

Messed around with PWM frequency setting in drivers.  At least with no input,
power input is lowest at the highest modulation frequency currently possible
(a bit less than 400 kHz).  That was a bit surprising.  I think a major loss
mechanism is the ripple current in the (on board) output filter inductor.
Inductor current decreases proportional to frequency, so copper losses tend to
decrease.  From voltage ripple amplitude, it appears current ripple is about
2A, which is similar to load current.  It looks like I will be wanting a fan,
since the heat from the inductors was getting the capacitors quite hot, even
in the open air.  Because the driver is a hard-switching design, switching
losses will increase as load current increases.  But watching supply current
while tweaking the PWM rate is a simple way to evaluate.  So far as I know, I
hadn't done this before, and had supposed that losses would only increase at
higher modulation frequency.  Currently with both boards running (but idle),
draw is about 200 mA, or about 35 W.  This is rather more than noted in Fall
2014 (13W for one board, including output).  No load current was only 0.066A
per board, not 0.1.

May want to synchronize the drivers; we'll see.  Buss ripple does become much
more repeatable if I tweak the two drivers on one board to run at the same
rate.  There is only a very slight tendency for them to synchronize due to
incidental coupling.  This might be advantageous even if the clock is not
synchronized to the modulation, though 32x one of the high carriers would be
in the right ballpark (240 - 430 kHz).

The switching power supply board is looking quite good, except for the output
being a bit on the high side due to 120V vs. 110V.  There does not seem to be
any line regulation to speak of.  It runs very cool under no load, and seems
to generate very little switching hash.  It runs at only 40 kHz, and has a
resonant design.  I can't see any of that ripple in the output when measured
at the amp board terminals, only the 300 kHz hash.  120 Hz ripple is only
about 20 mV with the drivers running (but no signal yet).  300 kHz hash is
much larger.

I had to modify the overvoltage zener in order to get the drivers not to shut
down.  The component values on the boards don't quite follow any of the power
variants in the IRAUDAMP7 design.  They are mostly according to the 2x200
variant (70V bus), but the overvoltage diode was more like for the 60V
version.  b.t.w., the specified bus voltage range for the 70V version is
60-80, so where we are at (just over 80V) is not terrible.

It also looked like some of the current limit values were just wrong (too
high).  Also there is no thermal compound around the thermistor.  The
frequency trimpot had way too high a value (2k) (fixed resistor in ref resign,
I see).  I've switched them to 200 ohm, but this seems a bit too low now.
We'll have to see about efficiency under load.


[16 Nov 15]

High carrier is -18.3 dB normalized at 7.5 kHz.  Compare -15.4 @ 10.5 kHz and
-13.3 @ 13.5 kHz.

freq	     db re. FS  V/FS	sense current
----------------------------------------
7.5 kHz	     -18.3 dB	0.121 	2.589A
10.5 kHz     -15.4 dB	0.169	2.031A
13.5 kHz     -13.3 dB	0.216	1.781A

Sensitivity is closely proportional to frequency, with coil held constant.
Current does not drop quite as fast with increasing frequency as ideal
inductor model would suggest (because copper losses increase w/ increasing
frequency.  The difference in DC input to the driver is small, but input is
lowest at 10.5 kHz, which seems reasonable, since I optimized the design at
that frequency.


13.5 is 1.8x 7.5, almost a factor of two.  This is a significant relative
frequency span.

There does seem to be considerable LF noise increase on high carrier output
below about 2.5 Hz (the Nyquist frequency of low rate output), maybe 6-10 dB.
My guess is that this is because low carrier noise is effectively added to the
high carrier noise at low frequency, when the low estimate is fed into the
high carrier demodulator.  This effect should be significantly reduced when we
have a KF implementing the high/low cutover.  Low carrier output seems fairly
flat below about 1 Hz, excepting the occasional glitches.  Note that due to
window order 4, the actual low carrier bandwidth is in this ballpark

There are fairly prominent spurs in the current sense spectrum, but they are
at different frequencies than the biggest spurs on the sensor input.  These
spurs are also effectively eliminated by the distortion filter.  There is not
much reason so far to think that it will be helpful to use the current
feedback for distortion reduction.  I don't currently have any way to tell
what distortion is coming from my electronics vs. the audio interface, but it
appears from loopback tests that the contribution from the audio interface is
substantial. 


[09 Nov 15]

Another stuck mode I've noticed is that we can get stuck with impulse on due
to an interaction between distortion and impulse filter.  Perhaps a different
aspect of an eariler seen mode.  But if we are getting impulse all the time,
if I jack up the threshold from say 5 to 10 sigmas, and then impulse goes off,
and we have a spike at a fixed position in the output block, which then fades
as the distortion filter (I assume) adapts.  Once this clears up, the
threshold can be lowered back down again (at least until the system latches up
once more).  Interestingly, the biggest effect of getting latched in impulse
mode seems to be a DC shift in the *low* carrier.

Ripple on output at low rate does indeed seem to be caused by the distortion
filter.  When I turn it off, the spikes at 5.9 Hz, etc., go away.  But with
filter off, there are spurs in the output at 187.5 Hz and 375 Hz.  5.9 Hz is
-62 dB typ, with noise floor around -85 dB.  I noticed that the 5.9 Hz peak
gradually wanders up and down in amplitude.  I tried increasing the distortion
Tc from the default 3s to 10s, but this doesn't seem to make any difference
that I can see.  The sporadic glitches (power supply?) come along often enough
to make fine analysis of output LF content rather frustrating.  At times I
have seen the Fs low ripple as quite large, dominating other noise.  Vibration
on the table is in the setup in the lab is perhaps hiding this, or it varies.


[21 Oct 15]

The wild-caught impulse I was looking at really didn't seem consistent with
EMI at all, or at least not radiated EMI.  In addition to the unlikely long
timescale, and being seen in the current sense too, it also didn't look
additive, and yet even being as large as it was, it shouldn't have driven
anything into saturation.  My top theory is still a short-term power-supply
dropout, possibly internal and not due to a line sag.  So getting a new supply
setup is a higher priority than debugging these glitches.


[20 Oct 15]

Saw once again the issue with distortion filter causing ripple at it's lowest
frequency.  Not sure why this is happening.  Something to do with passing DC
component?  I don't think I am doing that for any good reason.

It appears that the remaining effect of big impulses is mediated by the low
carrier sensitivity.  I don't understand how a low carrier error causes high
carrier error, though the data dependency is there.  The obvious fix would be
to add a low carrier KF with noise adaptation.

Unresolved issues:
 -- Ripple at low Fs, caused by distortion filter.
 -- Hum sensitivity.
 -- Why noise level higher than before?  Digitizer noise seems to be a
    factor.  Possibly also leakage of low carrier into high?
 -- Big impulses, low carrier KF?

Missing features:
 -- slower phase dynamics in KF 
 -- rate limiting in KF

Driver box:
 -- Need more output filters: stock inductor, caps
 -- Current transformers
 -- Connectors and cable for source
 -- Connectors and cable for interface: three inputs, three current sense
 -- Decoupling inductors, fuse?
 -- Check ESR of driver current bus caps
 -- Check rating of filter inductors vs. ref design
 -- order box, power entry unit, fan
 -- draw up box mods and send to shop

Moving in direction of 6DOF:
 -- Is IMD still acceptable with 6 carriers?
 -- Wind three-axis source and drive it?



[19 Oct 15]

Added stuff to adapt impulse threshold to n sigma of RMS and also to hold off
unlock indication until a minimum number of blocks.  Seems to be working quite
nicely now on high carrier, but low carrier is still glitching on a large
impulse caught in the wild, and this is having knock-on effects on high
carrier.  I fixed detrend/retrend stuff to not pass LF impulse through to
filtered signal, and this nicely cleans up the spectrum, but had *no* effect
on the low carrier glitch.  WTF?

Ooh!  There's a big glitch in the current sense too.  Interesting.  My first
thought is that perhaps a line sag passed through the supplies.  Anyway, demod
can't be blamed for saying something is going on if our source is glitched.

I've been thinking about power supply design for the driver, and this suggests
a big change in direction back toward SMPS, because they have much greater
ride-through capability.  Even more important than keeping hum out of the
supply is providing a steady supply across normal line sags.

Line sag theory doesn't entirely make sense for this case, though, because the
disturbance is so short, only three high rate samples, about 2ms.  Supplies
ought to be able to ride through that.  It could have sagged *just* long enough
to let that through, but what are the odds?  More likely PS is not working
right, or some other glitch with DAC, etc.


[16 Oct 15]

The 100 us impulse I was using on 14 Oct is really too long.  It was showing
up as a dual bipolar pulse at the amp output.  Pulse top starts rounding off
for input pulse > 2 us.  With overshoot, response extends to about 10 us.
Overshoot is our damped 75 kHz ringing of input network.

HP 8015A pulse generator not working so well.  Pulse leading edge is
completely messed up at low duty cycles.  Same on both channels, so not an
output amp issue.  Also a lot of low frequency noise (not hum).  That's likely
a dirty pot or switch.

Yeah, so without the KF on, it takes no time at all to settle.  The noise
adaptation, I think.
 ==> Yes, turning off noise adaptation when unlocked results in super-fast
     settling.  The first time, it did get stuck in impulse mode, though.
     Recovered by turning impulse filter off and on.  

At 13.5 kHz high carrier, the output noise is about 500 nm RMS in per block
measurement. This is much lower than at 10.5 kHz, I think.  -83 dB is the flat
area in the chart FFT.  I tried this because of the noise hump around 9 kHz I
had noticed before.

It really needs some smoothing, but it seems that noise starts going up below
10 Hz in a 1/f sort of way.  This is still at 13.5k.  

Distortion filter is implicated in the ripple in high carrer at low-rate
ouput.  Sometimes it gets really bad, often not so bad.  Just turning on and
off doesn't immediately fix.  It seems that if that freq component gets in
there (as what?), it isn't getting adapted out.  Is this a consequence of the
LF cutoff?  There shouldn't be that freq per-se in there, and if there were,
not clear it would do much harm.



[14 Oct 15]

As expected, the distortion filter is helping to cancel the sample-synchronous
spikes we're seeing in the power-supply test [sensor sitting on Kepco power
supply], though I was confused at first because I didn't notice the scale
change when I turned it on (and thought it was making things worse).

As I guess I should have expected, detrend is super-effective in chopping out
residue frequencies below the high sample rate.

With impulse filter on, 60 Hz in demod output is about the same, but it clears
up a peak at about 4.4 Hz.  Maybe this is because the impulses I'm seeing now
are synchronous, not hum?  And where is the output 60 Hz coming from?  We see
it as the 60 Hz sidebands, which must be IMD with 60 Hz or actual driver-side
modulation of the carrier by 60 Hz.  There is no sign at all of the sidebands
in the current channel, though.

Comparing raw data to residue, the 120 Hz sidebands are greatly reduced, but
the effect on the 60 Hz sidebands seems mainly to stabilize amplitude.
Impulse filter seems to have nil effect on either, but it does help with the
180 Hz sideband (which is mostly not making it through the KF).


Worked on offline mode so that I can replay data for A/B comparisons, and also
did some parameter tuning.  Note that the KF noise units are output
amplitude level (not distance).  To see what inputs noise currently is, turn
off the KF, then either look at the time domain display, or put the output
into level mode (rather than distance).  

I'm back in the nominal noise condition (not sitting on the power supply).
All the hum and output 60 Hz went away.  Big surprise!  Still not clear on
where the 60 Hz sidebands were coming from under that condition.  Possibly
interaction of source core with 60 Hz fields?  

One thing I can see clearly now is that the high rate output contains distinct
bumps at the low update rate.  I wonder if this is because the low carrier
estimate changes then, and the demodulator loop has to readjust?  This may
well explain the noise increase from the pre-KF setup.  This is not a
show-stopper for now, because it's not much above the noise floor.  Carrier
residue is now less than none [below the noise floor in FFT display], which
makes sense, because the feedback is working to track the noise, cancelling it
out too.  With current 3u/5u KF noise, the bandwidth is pretty flat out to 100
Hz.

For whatever reason, there's a noticeable rise in the noise floor centered at
about 9.5 kHz.  This is present even when the source is powered down and
output is muted, so it isn't due to IMD or any interference from the power
supplies, etc.  Level rises about 10 dB.  I likely wouldn't have noticed this
before even if it was there, because I have a better test setup now.

With drive off, and spectral averaging on, I can see a forest of hum harmonics
in the residue.  Clearly this is part of what makes up the noise floor.  This
isn't visible when the source is on and no spectral averaging, but you can see
it with drive on, too.  It's just less obvious right around the carrier
because it's in the middle of that 9.5 kHz noise hump.
 ==> For some reason, spectrum display is now showing carrier amplitude wrong.
     Appears to be -18 dB when it's really -7.6 dB (I think).

Doing some pulse tests.  With ludicrously big impulses (300m peak amplitude),
seeing various latch-up modes that we can only recover by turning off impulse
filter.  I changed low-rate processing to use impulse filtered signal, and
that works beautifully.  Before it was getting hit much worse than the high
rate.  But oddly the high rate itself doesn't seem be be benefiting.  Though
the residue is far less, what's killing the high carrier output is a burst of
specifically carrier energy, as a kind of ringing right after the pulse.  

OK, I've been tricked at a number of points by auto-ranging, perhaps again
here.  It turns out that the reason I was seeing hardly any impulse in the
high output, which didn't make sense, is that the noise adaptation based on
the guard channels was working.  When you turn the KF off or disable the
adaptation by scaling up the process/measure noise, then you do see large
impulses, and these are indeed cleaned up by the impulse filter.  I don't have
time now to debug where the residual high glitch is coming from, which turns
out to be pretty much the same with impulse filtering on or off, because we
just end up discarding the impulse sample.  
 ==> Given pulse was super-large, maybe not so suprising there is some effect? 


[13 Oct 15]

In thinking about the frequency real-estate needed for three-axis carriers, I
realized that both the high and low carrier groups are going to span up to an
octave of frequency range.  This means that the metallic interference is going
to be noticeably different in magnitude, and in ways that depends on the
object skin depth.  This doesn't hurt the rough "high bad/low good" hueristic
too much, but I was also realizing that it means we are going to have vector
measurements at 6 different frequencies, or approximately 4DOF per vector (2
angle and re/im magnitude), times 6 carriers is 24 DOF, or up to *four* 6DOF
solutions.  This means that even without adding any extra intermediate
carriers we can fit a moderately complex metallic interferer model, like a
cylinder with pose, resistivity, permeability and coercivity.  

Another way to say this is there's info not just the pairwise high/low
mismatch, but also in every carrier pairing (not counting order, and not
allowing self-mismatch).  I think this gives 25 matchups 5^2 (???) or 5! =
120?  
 ==> Should read up on "Magnetic Induction Tomography".  Our added
     complication is that we don't know where the sensors are.

Also, an optimized source design is going to have different windings for each
axis/frequency.  Because getting adequate SNR is easier at higher frequencies,
we want to dedicate more copper and power budget to the lower frequencies.  As
a starting point, probably make the copper cross section, current and number
of turns proportional to 1/sqrt(frequency), so resistance and dB/dt are
roughly constant, and losses are proportional to frequency.

Not sure what the optimal ordering of windings is.  Winding length and loop
area increases for outer windings.  Inner windings will also have
proximity-effect losses from outer windings because the flux is intensified on
the winding side of the core.  The greatest cross-winding losses will
be due to the HF field, since the HF field has equal dB/dt and least copper.
So imagine only the HF winding is driven.  Then the largest copper losses on
the other windings are:
 1] Where a winding crosses over or under perpendicular to the driven winding.
 2] Where the non-driven windings cross across the pole-ends, especially as
    they bend to turn across the corner of the core.

[1] seems to be where over/under interactions might be significant.  Looking
at flux simulations, somewhat nonintuitively, it seems that the flux is less
underneath the winding than above it.  So it would help for [1] if the LF
winding (with most copper) was on the bottom.

For [2], it may be more nearly a wash.  The amount of copper crossing the face
where the LF and MF windings pass over/under is roughly independent of which
winding is on top, and the copper near the corners (where losses will be
highest) varies even less.  Possibly it would help to put a 3/4 round spacer
at the corners so that the overtop windings do not pass so close to the core
corner.  This could be somewhat mechanically desirable too, though it will
make the windings a bit longer.

Under this theory, we should wind in the order: [LF, MF, HF].

Because of the core, the difference in loop area has less effect than with an
air-core coil, but would move us toward making the LF winding outermost.






[12 Oct 15]

  ==> Send chart contents to trace data.  Detrend?


On thinking, it looks like it is going to be hard to make much use of the
per-sample variance estimate.  I think the idea of weighting residue samples
by hum variance before the DFT is not going to work, or at least not without
considerable fiddling.  Though notching the carrier*2 out made things work
noticeably better, this was still the cause of the 1/f noise and drift.
Things behaving much more sensible without this.  As noted, this variance
weighting undermines the frequency-domain effect of the window.  And it's hard
to know how the hum variance is going to affect the channel variance.  It
would make much more sense to estimate the periodic sub-band variance and use
this in the KF measurement noise.  

Limiting on the time-domain residue is the only place I can see a benefit of
working at the input sample rate.  This may indeed be quite significant for
sporadic impulses, TBD and NYI.  Gave some thought to the idea that it might
be beneficial to limit smoothly (rather than hard clipping) in order to limit
energy that might splatter into the carrier channel.  When we intervene in the
time domain there is the risk that we may be removing stuff that wouldn't have
fallen into our channel in the first place, and in the process corrupting the
signal in a way that increases the channel's noise bandwidth.  A smooth
nonlinearity.  I had also thought that perhaps we could make use of a template
for a prototypic impulse in some sort of matched-filter approach.

Thinking about proposal deadline too. Need to generate some sort of usable
results today, even if this is not the KF demodulator in it's full glory.

I realized that I should be exploiting the phase stability of the carrier in
the KF.  Dynamics of phase are going to be quite different than amplitude.
Being phase selective should get a sqrt(2) noise benefit. As per my lock-in
analysis for ASAP, modulating in order to be phase specific does not give a
benefit, because the channel bandwidth is twice the signal bandwidth due to
double sidebands, but once you've modulated, you should make use of phase
selectivity.

Having some problem with KF instability on startup after settling.  This is an
interaction with the distortion filter, which is picking up on the
not-yet-cancelled carrier residue.  I think this is similar to, but more
severe than the 10's of seconds of ringing that we also see on startup.
 ==> Should be amplitude limiting with lower limit for distortion.
     Specifically for startup, could hold off on distortion filter until other
     stuff has settled, as this is a fine trim.
 ==> Separated limits for hum and distortion, added time-domain limiting at
     3x, and added a "settled" detection based on residue carrier amplitude
     (or at least stuff in the channel).  KF settled really fast at one point,
     now seeing about a 10s 1/4 sine settling thingie.  Is something not
     resetting right?  

 ==> Also reverted averaging variance back to time domain, with no carrier
     notching.  Now that is working much better, though we aren't using it for
     anything currently.  Could revisit the per-cycle weighting, because the
     old variance was crap.  Amplitude seems much bigger, though?

b.t.w., the large difference in hi/low noise amplitude I'm seeing in the
loopback tests is because I'm not normalizing by carrier amplitude.  The low
carrier has the same relative noise, but much lower absolute amplitude.  Drift
(and some 1/f noise) may be due to relative drift in references for the DAC
and ADC (likely the both have internal references, and are different chips in
the UR44).  This should be compensated by the current reference as long as
both signals are acquired using the same reference.  The 1/f is
definitely a lot better without the variance weighting.

 ==> Note that if KF bandwidth is very narrow, this creates the impression of
     1/f noise, because we never settle during the entire low rate block,
     acting like an integrator.
 ==> We can test the KF bandwidth by adding extra noise to the signal, and
     then finding LP corner in the noise spectrum.  Getting full bandwidth
     does require Kalman gains not much less than 1, corresponding to a small
     spread between process and measurement noise.  But when we are noise
     adaptive, we can slow down temporarily in response to ride through.


OK, now set up back on the hardware in the shop.  I'd say that basic
noise/drift performance seems pretty similar to before.  It hasn't glitched
yet, which is nice.  HF carrier noise is about 0.7 um RMS, compared to 0.55
recorded before in old proposal.  Note also that the KF is limiting high
carrier bandwidth to what appears to be 100-200 Hz, so the noise should be
lower, not higher.  Not sure what the cause of the difference is, but could be
the new digitizer.

Looks like high KF is rolling off bandwidth at about 100 Hz with 5u/10u
process/measure noise.  There is also a rise in noise below about 2 Hz (?).
Not seeing LF rise now, using power spectrum w/ window.  But based on noise,
bandwidth is -3 dB at around 250 Hz with noise 10u/10u, depending on how we
allow for the hum peaks.  


With default setup (not near power supply), the mean hum amplitude is well
below the IMD spurs, but in the *variance* the line hum effect is clear.  We
are getting about 20 dB of hum reduction at 300 Hz.
 ==> Oddly, I'm also seeing impulses *synchronous* with the low output rate,
     in addition to hum impulses.

When I put the sensor board on top of the power supplies, noise goes up to
around 4 um RMS.  The hum filter seems to have varying effectiveness,
depending on the hum, I guess.  With it off, the residue is 60/180/300 Hz at
about 2.5m RMS.  With it on, residue is about 500u RMS.  At 60 Hz, difference
is about 20 dB in the residue.  The hum filter does make the impulse noise
stand out a lot more.  what I see in the high carrier output is a fairly clean
60 Hz, about 12 dB above the noise floor.  Other harmonics are well down.

Interestingly, the hum filter has just about no effect on the 60 Hz in the
output, but reduces the *LF* noise by about 10 dB, and 120 Hz by 20 dB.  This
is reflected in the output being a cleaner 60 Hz pattern, with fewer jumps,
and lower max excursions.

I'd actually say performance is not that bad, considering that we're sitting
right on a power supply.  There are also nice short impulses in the residue,
about 5 samples affected.  There are what seem to be bipolar impulses, but on
close examination, it seems that these may be just opposite impulses that
happened to land next to each other. It does look like you could extract a
typical pulse, which has a bit of overshoot, about 100 us in length (similar
to high carrier).  

Well, I guess it's time to see if we can do something with those impulses in
the residue.  


[11 Oct 15]

Distortion filter and KF lock-in seem to be basically working now.  In the
loopback test with the UR44 there is a lot of LF noise and drift in the high
carrier amplitude, but not in a low carrier.  Noise seems to be near the
current sampling rate, ~ 6 Hz.  This may be because of the interaction with
weighting by the hum variance.  I'm not sure that is right yet.  But even
setting hum Tc to 50k s has nil effect on the recovered amplitude.  Even when
static, the screwy envelope of the variance is going to have weird effects.  I
think we may want to:
 1] Set a minimum measurement noise that is above the typical levels
 2] suppress HF content in variance?  (but not in mean)  Most of the
    interesting variation in variance related to hum seemed to be multiple
    high-carrier cycles. Also not entirely convinced the frequency-domain
    averaging of variance is working.

Another way to think about the weighting of the residue before DFT is as a
DFT window.  This is mathematically the same, except windows are designed to
have nice frequency domain properties.  The one complication is that each data
block in the cycle gets a different window.

It's hard to tell what the bandwidth is from the noise because there is a 1/f
corner around 300 Hz.  The drift is just going and going too, no sign of
settling after 10 minutes.  Modulation is supposed to get you away from this
1/f crap.

 continuous phase drift suddenly stopped just now.

Haven't really tuned KF yet or tested the noise-adaptive stuff, but the
process noise does seem to start biting below 100n.



[10 Oct 14]

Disappointed that distortion performance with the DR44 is pretty similar to
the old box, including the spur at 33 kHz.  I guess this is a common quirk of
sigma-delta audio signal chain (don't know what is on input vs. output side).

New code is turning over, but many things are still broken:
 -- Phase is drifting steadily?
 -- averaging filter was way slow with 2^16 order.  For now, switching to
    2^14. But seems wrong.  Probably one or more bugs.  Or performance quirk.
 -- Averaging filter time constants not right.
 -- KF convergence rate way way too slow.  May be variance/sigma issues?  Also
    DFT normalization issues in kf_demod_update?  Sometimes takes a very long
    time to "lock in", minutes, especially after a glitch.  
 -- Partly an issue with noise estimation from filter residue. Hum filter is
    still picking up the 33 kHz spur.  Have disabled that noise estimate for now.

Weird, the distortion variance signal is almost a pure tone at 1.125 kHz
(second harmonic of low carrier) whereas the mean distortion is much messier.
This was with low carrier amplitude set to 0.5, matching high.  Effect is much
less with nominal low carrier amplitude (-35 dB).  And hum variance is highly
periodic near 7.8 kHz.

 ==> Seeing varying behavior in residue/KF lock in.  Was looking pretty with
     high level for low carrier, now not so much.  
 ==> Filters are depriving KF of carrier?  KF starts diverging, then turning
     off hum filter brings it back.  Possibly triggered by glitch, or KF issue
     may *be* the glitch.  When glitched, high carrier is only very partially
     cancelled, phase wrong?  Hum sigma has gotten really big, messing up KF
     lock-in.  One fix might be to notch the variance too (at 2x freq), since
     that's where we're stuck.
 ==> Is always some carrier in residue.
 ==> Add KF input and state noise to trace.

 ==> "Hum" filter is doing much more for distortion than the distortion
     filter.

 ==> Really really need to try other interface, and on real hardware, to see
     how stable results are, especially w.r.t. distortion.  And also how large
     these distortion effects are compared to actual noise floor.


There seems to be some glitching with the new interface.  Maybe it's windows
or labview.  


[9 Oct 14]

I was started to get worried about whether the basic KF demodulator idea of
weighting each sample independently was going to work.  If you imagine you
have a clean carrier, and you multiply it by this inverse-variance weight
signal, then what happens in the frequency domain is a mess.  Yes, most of the
carrier power is going to end up in the right bin, but the rest is going to
splatter all over the place.  If you imagine a single variance spike, it's
going to show up mostly at the carrier 2nd harmonic and above (and at DC), but
for broader peaks anything can happen.  I was wondering if this was really
just a limitation of using FFT of the weighted signal for demodulation, and if
there was some better weighted FFT that avoided this, but if there is I
couldn't think of it.

I think it may really be OK, though, because this is the wrong picture to
start out with.  Ideally the carrier residue is zero, or at least below the
noise floor.  Scaling the residue by inverse variance just gives it a
stationary variance model, or just plain noise, which also splatters
everywhere.  If we start to see a carrier component poke up above the floor,
then we adjust the estimate and it goes down again.  It doesn't matter much
that some carrier power is going into other bins because that contribution is
well below the noise floor.  This loss of peak amplitude just creates a slight
gain error in the innovation, causing slower adaptation.  But the adaptation
rate is already emperically tuned using the process noise.

Haven't quite got a handle yet on scaling after the weighted DFT, and how this
noise relates the frequency domain KF measurement noise.  Variance is very
small, so just dividing by that scales the signal up hugely (so variance is
1).  Makes more sense to preserve nominal input units, at least when there is
no "special" variance peaking.



[8 Oct 14]

Notes on the Steinberg DR44 audio interface.  This is of course more
featureful, so has various things that have to be configured, and some of the
terminology is a bit confusing for someone who isn't a sound tech.  The mixer
app is also useful for seeing what inputs and outputs have signal, and what
the levels are.  The hardware "PEAK" lights seem to come on at -6 dB.

In normal audio usage, the main purpose of the builtin mixer is to provide a
monitor output during live recordings.  All the inputs also get sent directly
(no mixing) to the computer for remixing and post-processing.  Yes, you *can*
use the outputs just to output from the computer, but that's a special case.
Our four outputs are two stereo busses called "MIX 1" and "MIX 2".  See the
signal flow diagram in the manual.  In the mixer, the fader settings on the
"MIX 1" and "MIX 2" tabs determine what goes to those outputs.  The "ANLG <n>"
faders route the analog inputs directly through to to outputs.  We don't want
that, so set those to -Inf.  [Yes, if you jumper output to input, and the
total gain goes above one, it will oscillate.]  DAW = Digital Audio
Workstation = "the computer".  We do want those routed to the ouput, so set
the DAW fader at 0 dB.  Also set "MASTER" at 0 dB.

To use all inputs and outputs, it seems to be necessary to use ASIO, which is
possible in Labview, but not something I want to tackle right now (as long as
windows sound is working).  The windows driver only recognizes two inputs and
outputs, ANLG 1, ANLG 2, and MIX 1.  The phone jack inputs for 1 and 2 are
HI-Z rather than line level.  I think this won't really matter, but we'll see.

"MAIN OUTPUT" jacks get "MIX 1" with additional level adjust from the
mechanical "OUTPUT" knob.  I think for now we should take output from the main
output so we have this adjust.  "MIX 1" goes to "LINE OUTPUT" 1, 2, and "MIX
2" to line outputs 3, 4.  By default, the jacks "PHONES 1" and "PHONES 2"
monitor "MIX 1" and "MIX 2", and have headphone driver with level.

It's possible to enable high-pass filters on the first 4 inputs.  I have these
off at the moment, but haven't played with this.  The sensors are quite
high-pass anway.  It's also possible to phase-invert any of the inputs, which
shouldn't really matter.

There are also reverb and effects, which we don't want.  All effects on input
channels should be off.  Set reverb return level "RTN" to -Inf so that nothing
from the reverb processor is added to our outputs.  It does no harm to have
the reverb send levels set to -Inf too (so our inputs are not sent to the
reverb processor.)  The reverb unit gets input from either MIX1 or MIX2 (you
can choose which).  I don't think it matters which it is assigned to as long
as no reverb signal is added (RTN -Inf).

Leave the Mute and Solo ("M" and "S") off and they won't hurt you.  The
centering controls "C" on the ANLG inputs control mixer feed of that input to
left/right channels in the mix, and don't matter as long as no direct
mix-through is used (fader -Inf).  Centering on DAW and MASTER probably act as
balance controls, and should be at "C" to not affect drive levels.


[6 Oct 15]

Preamp was ringing pretty badly at ~75 kHz due to underdamped sensor resonance
with input capacitor.  Fixed by increasing damping resistor from 332 ohm to
1.21k.  I also added a 10 nF capacitor at output to suppress a few mv of AM
broadcast at the output, ~700 kHz.  This had little effect on the 33.6 kHz
spur.  

O.k., the 33 kHz spur is still there when I loop the digitizer output back to
the input.  Mystery solved, I guess.  ?! WTF?  And so is the glitching.

Glitching is quite bad, too.  Piece of shit box.

When signal is at -8 dB, there are 10k/11k spurs at -100 dB too.  That's what
I originally put in the distortion filter to suppress, thinking it was the
driver (!).  Just checked, and that's exactly the same level as when going
through the full signal chain!  I guess -100 dB distortion isn't terrible
(0.001 %), but it's surprising that the *input* side is the cause of the
problem.   It's the -52 dB 33 kHz distortion spur that is really ludicrous.
Not that anyone could hear that, but...

o.k., I've ordered a new Steinberg interface that has 6+4 in/out, enough to do
a 3DOF version.  PCM4220 says THD+n -105 dB, but my guess is that any given
spur will be a good bit lower.  That's a top-of-the line converter, though.  I
think we should plan on a distortion filter, but we'll see.

While I'm waiting, I think I'll look at refactoring the code for the Kalman
filter.  The high/low path separation is screwy currently because of how the
code evolved.

Trying to remember if there should be any special relationship between high
and low carriers.  Low are between hum peaks, whereas high are hum harmonics.
And both have to be harmonics of some repeat rate >= to the low-rate FFT.  

Now that I'm splitting the distortion filter and the low-rate demodulation,
there is not in principle any reason why the distortion filter has to have the
same order as the low-rate FFT.  It could be at any faster rate which still
has all carriers as harmonics.  If all of the low carriers are say multiples
of 8, then the distortion filter update rate could be 8x the low carrier.
This should not have any bad effect on performance (due to e.g. coarser
frequency resolution) because all the frequencies we care about are still
precisely represented, and we would have an improved tradeoff between
adaptation time and noise rejection.  Doing N shorter FFTs is also possibly
faster, because FFT is n*log(n), not linear.
 ==> Noise benefit not as big as you'd think, because the bins are wider, so
     have more noise. Seems like there should be some benefit, though, because
     we are learning fewer parameters (?).  Thinking about it in frequency
     domain bandwidths and uncorrelated variables, it seems like a wash.  But
     there is no reason for our model to represent any longer repeat period
     than there actually is.  We ought to be able to take those duplicate
     estimates, average them, and get a sqrt(n) improvement.
 ==> Now I'm thinking it probably is lower noise to do the big FFT and then
     zero out the LF componenents (which forces the estimate to repeat at
     shorter periods.)  The key idea is that the noise is not white, and those
     LF bins contain a disproportionate amount of noise.  Now, even if we
     shortened the FFT, we could still chop out low frequency bins, but ???
     It's certainly simpler to not add another FFT size.


[5 Oct 15]

The HF distortion pattern seems to repeat every 60 samples, from
autocorrelation. (?)  0.01696  This is not a multiple of hum filter order, but
the correlation at 1600 samples (= 60 Hz) is almost as high, at 0.01670.
Difference may be due to non-periodic stuff, but 0.01702 @ 100, 0.0174 @ 320

The biggest spur near 33.06 kHz corresponds to 2.9 samples.

Tried messing with the driver clock pot a bit.  Couldn't see any effect on
distortion.  If adjusted to 400 kHz at no signal, drops to about 270-315k at
nominal drive.  I've adjusted so 400 kHz at nominal drive.

Spur may be an alias of carrier harmonic, or some such.


Also, though this isn't entirely conclusive, I had a case where rather than
occasionally glitching out, the the "raw" signal was highly mangled, though
scope showed it to be fine (as far as I could see, nothing like what we saw in
windows-land).  Stop and restart of DAQ didn't recover, but unplug and replug
of USB cable did.  So there are definitely some sort of firmware/driver bugs,
but it's possible something else could be underlying the sporadic ~2ms glitch.

Then again, once the scope glitched at the same time, even though the selected
input wasn't connected to anything?  Other channel was connected to low bus.
Not sure exactly what I saw.  It was in recurrent mode, ac coupled.  I tried
setting one-shot trigger looking at busses, and wasn't getting any triggers.
     

[1 Oct 15]

Implemented hum filter, and was surprised to discover that there is
hum-synchronous HF scruff which is swamping the actual hum amplitude.  Might
be less so if the hum was worse, but...  And also, the hum filter actually
cancels it, rather more effectively than it cancels the actual hum.

With carrier off hum cancellation is decent.  @ 300 Hz, -92 dB, vs. near noise
floor, about -115 dB with hum filter.  With carriers, about -92 and -101.  I
wonder if the poorer suppression with carriers is because of some interaction
with the HF scruff and the phase drift in the hum?  Without carriers, there is
considerable cyclic variation in variance, with spiking at times, but the
bodies of the variance peaks are spread out for 1 or 2 ms.  Peaking is up to
4 sigma, or 16x in variance, which is enough to be quite significant.  Phase
drift is 5ms is 11.8 sec, or 424 ppm.  100 ppm is typical can oscillator spec,
so in the right ballpark, though a bit on the high side.  Drift is also +/-,
which would be more puzzling for clock drift alone.  More likely phases are
changing due to variation in the EM environment, what loads, etc.  Might want
to do something like use FFT to tweak the phase of buffer on each "wrap".  But
this will not work as long as the HF stuff is there, because that is
synchronous with the carrier sequence, not actual hum.  

Not sure how much benefit it is cancelling the HF scruff, but it seems to work
fairly well, and may help with (for example) impulse detection, since
currently the residue is dominated by the HF stuff.  In particular, the 33.601
kHz spur is -60 dB in raw data, vs. -82 dB @ 60 Hz, -94 dB @ 300 Hz, the
largest true hum peaks.  We can't see that at all in the time domain
underneath so much other stuff.

First idea is to try another fixed-period filter at the high sample rate to
peel off the HF stuff before it gets into the hum filter.  Then the hum filter
won't get confused when trying to track drift in hum phase.  

In order to reject relevant sized impulses in the time domain, we need the
system SFDR to be on the order of the desired noise floor.  Large spurs are
confusing us.


I'm not sure where the HF stuff is coming from, but it is some sort of carrier
related distortion which is not recurrent at the low rate output.  When I dial
back the output to zero, the HF stuff goes away, and all I see is genuine hum,
which looks rather like I had expected, including a significant spike.  If it
is not from the ADC, then some filtering at the driver output or ADC input
might help.  It could be aliases of the driver ripple frequency.  I'd been
keeping input filtering light to try not to smear impulse noise, but that may
not be our worst problem.

The two obvious suspicions are that it comes from the driver amp or from the
ADC.

Also seeing spiking of +30-50 mm in demodulated high carrier, which I don't
think I was seeing earlier today.  Two modes, one with a long settling tail,
and one without.  Pulse is associated with a far smaller opposite deflection
in low carrier, seemingly due to the sawtooth-like recovery effect due to
disturbance of the distortion filter.  It seems like I can hear the
disturbance sometimes, i.e. the output is dropping out.  I seem to recall
something like this from last year (happened more in the evening?)  I suspect
something DAQ related.  I don't believe spikes of that magnitude could be
caused by anything short of nearby lightning strikes.  And now an 800mm spike!
Always positive (amplitude decrease).  Had four glitches at fairly regular
intervals of about 30s.  Long-settling version at 80m can be at 20m amplitude
for 6-7s. Yet minimal effect on low carrier?  Mainly a negative pulse
immediately after the high carrier pulse, -200u, then a tail of maybe 5u.
Long high carrier tail must be a filter effect, but not always present.

Probably unrelated, I notice a fluctuation of the power supply current meter
at a few Hz, which is puzzling.  I suppose it's possible one of my power
supplies is flaking out or something.



[22 Sep 15]

Was looking at audio ADC spec sheets, and reminded myself something I perhaps
figured out before, which is that (for real-time), we want an ADC with some
sort of reduced latency decimation filter.  The latency of audio ADCs is 10's
of samples, for the TI PCM4220, 39 or 21, in "classic" vs. low-delay.  Since
our high sample rate is maybe 64-128 samples, this isn't terrible, but is
something we want to minimize.  Could be an argument for going for a higher
output rate than we might otherwise use (moving more of the decimation into
our processing).  The low group-delay filter has less stopband attenuation,
passband flatness and cutoff sharpness, but none of these are terribly
critical for us.  Stopband is perhaps more important than the others, but SNR
is so high in the input signal that we are more concerned with raw dynamic
range. 

I noticed in this converter, and likely the case with other audio converters,
that THD+n bumps up when the input is above -20 dB.  The FFTs are shown with
-60 dB input.  This is something else to keep an eye on.  The PCM4220 has the
highest SNR rating of current TI converters, 123 dB, perhaps 20 dB higher than
a typical value.


[19 Sep 15]

An update on the current state of the 1DOF prototype (as I recall it from last
year).  

The driver is IRAUDAMP7 module with split +/- 70V supplies.  Distortion was a
problem, creating a spurious tone in the high rate output (high/low IMD, I
assume).  See distortion filter below.  No plans to try the direct bridge drive
anytime soon, as this seems to be working pretty well.

Current sense on drive via current transformer.  This is demodulated and
used to normalize the measurement, which has been *extremely* effective in
reducing amplitude and phase drift.

For some reason I had been thinking that the receive signal was going to
closely resemble the driver current, when in fact it is much more like
*voltage*.  See for example older notes below.  This means that the
(considerable) output ripple appears on the sensor.  I didn't quantify the
effect of this, but it looked pretty bad, so I decided to add an additional
stage of output filter to the amp.  I played with the idea of getting voltage
boost by setting the resonance nearer the carrier, but this was bad for
overall efficiency because of losses in the amp and filter.  Given that we can
reduce coil Z to increase current or increase voltage by bridging, this didn't
seem like a win.  Voltage headroom is important for low distortion, though.
As noted below, best SNR is achieved below the max possible drive level.


Distortion filter:

Distortion currently cancelled quite effectively by a simple FFT distortion
filter on the receive side.  See fft_filtering and low_rate_processing.  We
maintain an averaged low-rate spectrum, and pull a "distortion model" and a
"carrier model" time domain signal out of this.  The carrier bin alone forms
the carrier model, while all the carrier harmonics give the distortion model.
Theses are converted to time-domain by IFFT.  The distortion model is then
subtracted from the incoming signal before high-rate demodulation.  Since the
distortion is basically static, this approach is quite realizable.  The big
shortcoming is that the distortion is only static when actual sensor motion is
negligible.  Whenever there is gross sensor motion it takes seconds for the
output to stabilize again.  Possibly it would be useful to keep something
like this to polish off residual distortion, but for a real high-dynamic
measurement system most of the work would need to be done by pre-distortion of
the drive signal (as discussed below.)


Preamp:

Preamp is basically as described below, with three LT1028 in instrumentation
amp configuration, Av=25 nom (24.8 measured).  Noise = 5uV p-p at output ~=
1uV RMS RTO in 1 kHz BW, not much difference shorted or open (with 20k input
shunt resistors).  That's 40nV RMS RTI, good agreement with specs.  CMRR =
-100 dB @ 20 kHz.  HF CMRR could presumably be increased by trims, but no
current need.  Main circuit refinement is adding diff and CM filters and 20k
input resistor.  There is no input protection other than the 290 uH CM filter
and 100 pF CM capacitors (no series input resistance or clamp diodes).  I used
1K 0.1% monolithic resistor arrays where matching was needed. 3.3nF in
series with 332 ohm diff mode shunt gives ??? kHz diff-mode roll-off in
conjunction with sensor inductance.

With interfacing via Tascam USB audio interface.  This talks to labview via
windows sound driver.  Works o.k., though latency is poor and driver is
buggy.  Could presumably get better SNR with an optimized design, but SNR is
really pretty decent.  Was leaning in the direction of something similar for
6DOF proto, though this would preclude real-time operation.  IMO would be
premature to invest the design effort when we don't know if the concept is
really usable.


[Fall 2014]
Standard sensor sizes:
Polhemus TX2: 0.9" x 1.1" x 0.6", 0.000056" resolution at 12" range
Aurora min 6DOF: 1.8mm x 9mm
Ascension 8mm: 7.9mm x 19.8mm

There was mention of magnetic field strength in Ascension microbird manual
(this is older version).  Newer ones don't mention at all, likely because with
micro-size sensors they've increased the operating field strength.

Exposure limits: levels seem to be in the "possible concern" range, especially
near the source.  Because of concerns about unknown effects of chronic ELF
exposure, limits much stricter than the thermal effect limit have been
adopted.  Higher limits may apply to just the hands or limbs, but
the eye is specifically excluded because of known problems with RF and
cataracts.  It seems that source positions where the source is much closer to
the patient than to the tool are unfavorable.  


Some limits:
IEEE 95.1 (2005): 163 A/m (general) 400 A/m (controlled)
ICNIRP 1998: 5 A/m (uncontrolled) 24 A/m (controlled)
ACGIH (TLV-PA) (occupational) 60e-3 T / f(hz), 6 uT @ 10 kHz ~= 5 A/m
MRI dB/dt limits: 20 T/sec, = 318 uT @ 10 kHz, = 253 A/m

Even the 163 A/m threshold is exceeded closer than 80mm to the source center,
or approx 55 mm from the housing.

Also, this is just one carrier, whereas the flux from all three axes would be
perhaps at least twice as large (not as high as 3x because they are
orthogonal).  But also that sim is at 2A RMS, whereas we may actually be
running at somewhat lower limits, allowing for the low carrier.  Under the
TLV-PA rule, the flux at 250 Hz would be 240 uT, a lot higher.  I would
suppose that when there are multiple carriers each is pro-rated to the 1 Hz/60
mT budget.  This means that if we hold the low carrier flux equal, then the
contribution is negligible.


full_spectrum.png.  Cursor is on hum peak 540 Hz (9 * 60 Hz) near low carrier,
-105 dB.  Carrier amplitudes: -6 dB and -35 dB.  Noise floor is about -130 dB,
or about 120 dB dynamic range.  Because of 1024x higher sample rate, the noise
floor increases 30 dB, reducing voltage dynamic range to about 100 dB.
Carrier frequencies 10.5k, 509.8, Fs 1.5k and 1.5.  High carrier RMS position
noise is 0.55 um.  That's a dynamic range of 111 db.  The difference between
voltage and distance dynamic range is caused by the 1/r^3 relation.

high_spectrum_vibe with 1um peak amplitude vibration.


This is fair agreement with the theory, for whatever that's worth given the
different sensor coil.

With 500 Hz low carrier setup, 
Using Clarke-Hess 259 VAW, V = 33, A = 2, W = 2 or 3 (res only 1W).
Input power is 140V, 0.092 A, P = 13 W.  No load current 0.066 A, so input due
to load is 3.6 W, not much more than the actual output.

Low carrier alone, 1.15 A, 1.2V, 0.6 W
High alone, 1.68A, 33V, 1W

b.t.w, having the VAW connected seems to degrade performance.  Was having a
big effect on low carrier, creating some kind of 10 micron ripple.  Ground
loop? 

On GR 1633-A, L = 285 uH, Q = 25 @ I = 1A AC 10 kHz, R AC = 0.7 ohms
(including 2m leads).  This would give dissipation of 2.8 W at 2A, fair
agreement with VAW result.  Rac would be lower with the current partly at low
carrier.  Rdc = 0.373 including cable.  Coil alone something like 0.240 ohm at
DC.  FEMM says 301 uH, 0.243 ohms, but that should be an AC resistance, so
there is some simulation problem.

It doesn't seem to be a win to drive the high carrier at higher levels with
this setup, since the noise floor increases more than the signal.  At 3A, for
example.  But pushing down to a lower low carrier with higher current would
make sense.  Leaving the low carrier at 500 Hz for most of these prelim
results.  Bridge mode might change the situation.

It seems the best high carrier SNR is at about 2A RMS total, with input level
adjusted to -6 dB.  Reducing input gain too much does start reducing SNR.



Labview support 1 year, 709.03

Something else we could add to the budget is electronics CAD.  I've been using
a 12 year old version and should really upgrade.  But the P-CAD i'm using is
something I got from Jon Kozar, so it isn't exactly a legal copy, and that
would be possible for Altium too.

Altium electronics CAD, 1 year: $1,750



Timeline:

3 months: design and build electronics (aim 1)
1 month: FPGA firmware for source drive, digitizer and demodulation (aim 2)
1 month: Modification and programming of revised motion calibration system (aim 3)
1 month: sensor fabrication and calibration (aim 1)
1 month: source fabrication and calibration (aim 1)
2 months: software for dual path fusion and pose solution (aim 2)
3 months: performance characterization and tuning (aim 3)

There's not a lot of slack in this timeline, but it seems doable.  If I try to
complete all this work in one year, it will take up most of my time.


Specific aims:

Develop magnetic tracker suited for closed-loop microsurgical applications
    Should meet or exceed ASAP performance in terms of speed and resolution,
    while offering a larger workspace and eliminating line-of-sight
    constraints.  (see comparison table) 

Integrate magnetic tracker into Micron. The larger workspace and support for
three simultaneous measurements will allow tracking of the patient motion so
that Micron can be stabilized with respect to the patient, rather than a fixed
coordinates.  This is an important practical and safety benefit.  Freedom for
disturbances due to unpredictable sightline blockage (assistant's hands),
larger translation workspace and support for arbitrary rotation are also
important to ease integration of Micron into current surgical practice.


Soft iron error:

Steel permeability is approx 3-5x less at 10 kHz than at low frequency. At
least in silicon steel, permeability drop is mainly between 1 kHz and 10 kHz.
However, there seems to be some disagreement over whether this has to do with
peculiar steel properties (magnetic relaxation) or whether this just has to do
electromagnetics and resistivity, and it isn't clear what effect shape has, or
how permeability translates to measurement error, but there is reason to
expect that soft iron error will be somewhat less at 10 kHz than in a LF/DC
system.


Magnetic sensor:

Pickup design:

Note that there is a limit to how much sensitivity is useful because there is
considerable electromagnetic noise in this range, both natural (lightning ...)
and artificial.  Probably both impulse and periodic noise suppression would
be helpful.  

GMR sensors (see NVE) are a possibility, especially at smaller sizes, and when
in-plane sensing is desired.  Would be interesting to play with.  Would reqire
a bias magnet to get bipolar output. But it seems that induction sensors have
better sensitivity at a given size than GMR.  Even NVE only claims they
surpass inductive at 80 Hz or with very small sensors.  Our carrier has to be
at least KHz.  I suppose GMR sensor might increase the viability of kHz as
opposed to ~10 kHz, but the 1/f corner is currently around 10 kHz with these
devices.

"Electrically Small Antenna Design for Low Frequency Systems" is also
interesting, though they are at 1 MHz.  Clearly the radiation efficiency of a
10 kHz loop antenna is going to be quite low, but that doesn't mean it won't
be enough.  Their efficiency is -74 dB.  They say that in the near field with
like antenna types the power rolls off as r^-4 to r^-6 with distance.  The
cave radio people say the dropoff is cubic (at 3.5 kHz, but many antenna
diameters away.)  The magnetic tracking patents also assume a cubic dropoff.

Ferrite cores seem to give better resolution per unit mass, as shown by the
use in space magnetometers, and certainly in resolution/volume.  This depends
on both the aspect ratio and the bulk permeability, but the bulk permeability
has little effect above 500 or so. The need for a narrow aspect requires a
lower area, partly cancelling the benefit.  Long rods (~ 10:1 aspect) give the
best effective permeability.  For the #78 rods in the Fair-Rite catalog, the
rod permeability goes up only to about 50, even though the bulk permeability
is far higher (2k mu_0).  An air-core coil is ideally very short and wide.

See Fair-Rite Antenna/RFID rods, 78 material.

A ferrite core gives some nonlinearity and temperature dependence.  See
"Induction Magnetometers Principle, Modeling and Ways of Improvement".
Sensitivity of an inductive sensor is proportional to square of linear
dimension (sub linear (2/3 power?) of mass).

Ferrite is less dense than copper, but for a fine winding the density may be
about the same due to air space and insulation.  Scaling law says that the
noise floor of an inductive sensor is determined by the dimensions of the coil
(independent of wire gage), though the higher voltage from a fine winding may
be more advantageous for low noise amplification.  Johnson noise is the
fundamental noise floor, but amplifier current noise sees the antenna
inductance which is likely much higher than the resistance.  Since the source
impedance is not resistive you can't use the "noise resistance" criterion, but
a low noise JFET is probably called for.
 ==> Not what I'm doing.  Don't recall looking at the noise
     current/inductance effect.  JFET might make sense with really fine
     winding. 

In LF communications the popularity of ferrite core receiving antennas is
somewhat confounded with the the advantage of loop antennas, since the ferrite
loop is most widely used.  Loop antennas are beneficial because reception is
often limited by man-made noise and this noise is in turn often dominated by
near-field noise (partly because of the large wavelength), and in the near
field it happens that E-field noise is much stronger (due to sources such as
florescent lights and automotive ignition.)  This is not ferrite specific,
though the compactness of the windings on a ferrite loop means that the
E-field pickup can often be ignored, where in air-core antennas it may be
significant.  The main point of this observation for us is that the situation
with respect to artifical noise is not as bad as E-field data (such as
capacitive sensors) would suggest.

In addition, the AM radio antenna is part of the receive tank, making this
component serve a multiple purpose of antenna, selectivity and impedance
matching.  As well as being more compact, the ferrite makes it easier to
achieve the desired tank inductance and Q for selectivity.  My guess is that,
allowing for these puns, the ferrite solution is actually cheaper, or you
would see more AM radios with air-core loops.  For us, we know the signal is
entirely magnetic, so the question is only what kind of magnetic pickup to
use, and needed response is quite broadband, so the loop isn't tuned, removing
some of the ferrite advantages.

A transimpedance amplifier is evidently common in inductive magnetometers,
partly because the shunts the self-capacitance, squashing the first
self-resonance.  I'm not entirely clear on how this affects the interaction of
nearby sensors, which is more of an issue for us than in a magnetometer
application, since we have both output and handle sensors.  With the air-core
sensor, I think the interaction is less with the coil looking into a high
impedance (rather than effectively shorted.)  Coupling between sensors may be
an ugly problem, since the rod is high permeability material that affects the
field by sucking in flux from an area significantly larger than the rod
diameter, while we must respond to quite small changes to acheive the desired
resolution.

Though less sensitive than a ferrite (with respect to thermal noise), my
intuition is that the air-core voltage output coil has relatively little
effect on the field.  If there is no current in the coil, then it doesn't
create any field.  If the wire size is less than the skin depth, then there
won't be much eddy current effect.  Of course there will be some current due
to parasitic capacitance, if nothing else.
 ==> Concerns about affecting the field (supported by simulation), and not
     being anywhere near the self-resonance, drove the decision to use voltage
     output. 


GMR sensors probably couple with each other a lot less than a magnetic core
antenna because the amount of magnetic material is a lot less.  I don't know
how GMR would compare with air core in sensitivity and coupling.  The presence
of bias magnets would surely create some difficulty. How much do they distort
the AC field?  Magnets are ferromagnetic, but "hard" so they don't have super
high permeability in low fields.  It's conductive, though.  Maybe the bias
magnet(s) could be fixed to the handle, or even be part of the field
generator.  But changing bias field would undoubtedly create some nonlinearity
since the GMR responds to the DC field.  Can avoid problems with permanent
magnets by using electromagnetic bias.

GMR and inductive sensors probably wouldn't mix well because coils moving in a
bias field will induce noise voltages.  I guess this is a problem with
inductive sensors in general, since there are always going to be ambient
fields (DC or low frequency) that the coils are moving with respect to. This
dynamic signal is completely different from the desired static response.  One
mitigating factor is that the velocities are fairly low.  This could also be a
reason to favor a higher carrier frequency (10 kHz+), since vibrations drop
off pretty rapidly above the structural resonance.  You need a pretty rigid
and/or light structure to get 10 kHz vibration.

A possible way to deal with the coupling problem would be to use magnetic
sensing only to localize one part, then use another internal measurement mode
(optical, etc.) to localize the other part with respect to the measured one.
That's ugly because of the difficulty of finding another compact way to do a 
6ODF measurement, and also because of the need to design two different
systems.

Another way to think about the coupling issue is consider if there is a
tractable interaction between the two coils, like in mutual inductance.  It
seems this would work better with air core, both because the coupling will be
less and because the large diameter means that the coupling will vary less
with relative motion.  It might also make sense to look at the difference
between the corresponding coils early in the signal processing path as an
indication of the relative pose of the two parts.  The difference signal
depends on both the field gradients and the pose difference.

[2 June 13: 

Given the approx 2x gain from wrapping around the outside of a toroid, that
might be sensible on the pickup side.  This uses considerably less ferrite
(hence less mass) than a ferrite cube.  We could use three separate coils,
increasing packaging flexibility.  You could have a large air-core Z coil,
with two smaller toroid-core XY coils inside the diameter.  The cores reduce
the height, which is a critical dimension.  There is the issue of ferrite
creating cross-coupling, but it may be that the effect is acceptable if we can
get the tip and handle sensors far enough apart.

]

[14 March 13:
To clarify: GMR sensors are not in the running. Even not counting the bias and
linearity issues, the noise performance just isn't there. For LF response
flux-gate would be the way to go.  For short rods (such as around a toroid),
the effective permeability might allow reducing diameter by up to 2x.  The
simulated effective permeability of the "square" cylinder source winding is
nearly 4.  sqrt(4) gives a 2x reduction in diameter.  The situation with mass
is less clear.  co-axial pickups do simiplify the pose solution.
]


Source design:

[19 Sep 14]

Designing source for 8 ohm impedance for using audio amp in prototype.  This
calls for inductance of 127 uH, maybe as little as 30 turns.  Not clear that
this actually makes sense, though, given that we are most strongly limited by
dissipation in the source coil, and FEMM seemed to be saying that we got
better results a high impedances.  Indeed it seemed I was constantly reducing
inductance to keep the voltage down, even though the result had good
efficiency.  Given that the amp has the voltage capacity and is stable, may
want to run at much lower current.  This reduces thermal issues, since audio
amps are thermally sized at 10% output.  

With IRAUDAMP, dissipation seems to be mainly determined by clock frequency
(fixed), plus load related, presumably I^2.  In bridged mode at 50V, the
distortion wall is at 400W = 57V RMS, at which point the current is 7A.  Our
current would be much lower.  Current-wise, 10% output is 2.2A.  At +/- 70V
bus we would get about 80V RMS.  Note that distortion starts going up when
peak voltage is about 80% of total bus voltage (in bridged mode).  This is the
same voltage ballpark as the '13 proposal reference design (106V).

If there are stability issues running with light load, I can change the loop
compensation, but it seems that the peaking under light loads is mainly due to
output filter resonance, which we can simply avoid exciting.  Note also that
the low carrier hardly uses any of our voltage headroom. Concerning shock
safety, with bridge drive, the voltage w.r.t. ground is less than the coil
voltage.  AC is 1/2 and if there are split supplies there is no DC component.

A bit worried about the dynamic range with the IR amp.  It seems the
sigma-delta approach does a good job of generating noise rather than
distortion, but the noise floor increases at LF when there is signal.  Our
preferred tradeoff would be low noise, high distortion, which we could get by
direct digital bridge drive.  (Assuming low noise supplies.  There is no
supply rejection at all with direct drive, whereas the IRAUDAMP7 has about 40
dB.


[older]

Of course we want to emit a strong field, and it seems that given the use of
strong fields (in MRI for example) that any field we can reasonably generate
will be acceptable from a safety perspective.  There are field standards for
pacemaker interference, etc., which are much lower than MRI, but are still
unlikely to be a problem.

The source needs to be relatively close to the pickup, and that means
relatively small.  Most obviously, if the source is too far relative to its
size, then the strength will be too low for high resolution measurement of the
field.  Contrarily, if the source is too large relative to the spacing, the
field gradient will be too low to give an accurate distance reading.  Somewhat
less obviously, the source center needs to be close to the pickup to minimize
the moment in the polar measurement, because the resolution tangent to the
sphere degrades proportional to the range even if the angular resolution were 
constant over distance (which it won't be.)

I'm now thinking more of a mini-tower than a big cube, with the coils up at
the top, near the workspace, and electronics down nearer the bottom.  The
cross section would be a 25-50mm square, and the height several times that.
The idea of attaching it the patient is cool too.  Ideally all the drive
electronics would be inside to avoid in-band emissions and EMI from the lead
wires.  Or using parallel resonance you could put just the resonating
capacitors in there.

Some magnetic tracker designs also use coils that aren't nominally co-located.
For example, in one patent a driven moving coil is tracked using several
distributed pickup coils.  One could imagine creating controlled field
gradients, rather like in MRI, so that position could be read off as a
Cartesian coordinate rather than polar.  Remember, though, that we aren't
tracking something inside the body (in the eye), but rather the handpiece,
which is in the cluttered surgical field.  We can't surround the surgical
field with large coils, though several small coils might be usable to
multi-lateralize a fix.  More coils is necessarily somewhat more intrusive,
though that could be offset if they were smaller.  Existing tracker products
do seem to mainly use the single source, moving pickup approach.
Conceptually polar localization using this approach seems simple, but given
the numerous approximinations of misalignments, non-dipole sources,
nonlinearities, etc., it may be that the actual advantage is less.  The
advantage of distributed coils would seem to be mainly for translation
measurement.

It's looking like in the transmitter the benefits of a ferrite core may not be
all that great, even ignoring the issues of nonlinearity and possible losses
in the cores, given our packaging situation and the need for three orthogonal
coils.  The effective loop area is increased by the mu_rod factor, but this is
only 40 or so.  The effective increase in diameter is then only 6.3, which is
less than the rod length.  You would do a bit better by making a air-core loop
with diameter equal to the rod length, or perhaps a square loop.  Even though
the selectivity is a more of a nuisance on the transmission side, the Q is
still relevant, since it is a representation of the lossiness.  If the
air-core loop is more lossy that would offset some of the advantage.  The
air-core certainly involves a lot more copper, though the greater coil area
would also help with dissipation.  

The focus should be on efficiency rather than Q per-se.  The Q needs to be
adequate to give acceptable distortion, but adding turns increases the
inductance more (n^2) than the flux (n).  Generally this increases the Q, but
also increases the Q needed to drive the current through (in the series
resonant configuration.)  If nothing else, this requires high tank voltages,
but losses will surely increase somewhat too.

One issue is that it may be difficult to get a small air-core coil to resonate
with adequate Q at these low frequencies, and even if so, the impedance may be
awkwardly low (high tank current) which will demand large capacitors and a
lower impedance from the driver.  I think the advantage for the common scheme
of driving a parallel tank from a tap or small winding is that it doesn't
require a low impedance from the driver (a high impedance and/or loose
coupling is desired) and the entire tank current doesn't need to flow in the
driver.  But at these frequencies it is easy to get quite low impedances from
MOSFETs, and the coil is simpler.

An alternate way to drive the source would be to give up on the idea of
resonance and drive the source using an audio power amp.  This lacks finesse,
but audio amps are a commodity product, and this avoids the whole issue of
tuning the source.  An 8 ohm impedance at 10 kHz would be 127 uH, which seems
easily attainable in a small air-core coil.

Small size makes power dissipation difficult, as well as reducing field
strength.  It was part of my plan to do as much as reasonably possible on the
source side to maximize field strength.  For one thing, this overpowers
interference.  Field strength is ultimately thermally limited, though with the
quadratic nature of both core and copper losses, thermal management is only
going to get your a factor of two or so.  It's not going to be sane to
dissipate more than maybe 10W in a 25mm cube near the patient's head and
surgeon's hands.

Thermal management is challenging because metals can't be used near the
source without creating losses and field distortion.  Air cooling (possibly
forced) is good, and pyrolytic graphite sheet may also be helpful.  Though
conductive, PG sheet is approx 300x more resistive than copper, and is also
thin (100 microns or less).

Somewhat more extreme would be a heat pipe concept.  Though heat pipes are
usually metallic, there isn't any reason why the material needs to be
conductive except where the heat transfer takes place.  If the source is
*inside* a non-metallic heat-pipe structure then only the more distant
radiator needs to be metallic.

[2 June 13: 

With packaging considerations, the most compact source for a given strength
and power consumption is a ferromagnetic core cube, as used by Ascension.
Inserting a core while holding the losses constant in a FEMM "square" cylinder
model increased the field by 2.9.  To get this increase without the core would
require scaling up the linear dimension (including copper section) by
sqrt(2.9), or 1.7, an increase in volume to 4.9x.  That's noticeably bigger.
Squareness is also a virtue, because in the packaging world a cube really
isn't any bigger than the same diameter sphere, and the area of the square is
1.27x that of the circle.  Also, an interesting geometric fact is that the
perimiter/area ratio of a square and the inscribed circle are identical
(presumably holds for any regular polygon.)  This means that (to first order,
at least) there is no effect on losses by expanding from circular to square
section.  A ferrite cube also has the advantage of being considerably easier
to fabricate than a sphere.  The combined benefit of ferrite cube vs. air-core
sphere is 2.16x in linear dimension and 10x in cubical volume.  As well as
being convenient, smallness is benefical for measurement because the dipole
assumption applies over a wider range, but this may be offset by field
asymmetries created by the square vs. circular section.

The cube core is a no-brainer for Ascension because they can use any old hunk
of steel.  We'd need to fabricate a ferrite cube somehow, which is going to be
more expensive and/or more work, but it seems doable.  We could stack up tiles
(100 x 100 x 6 mm), and Ferroxcube also stocks bars, and will machine them.
Water jet cutting might work.  It seems Polhemus only uses air-core in their
long-range source, with the smaller sources using a ferrite cube, judging from
dense cube format (which is consistent with the field strength figure in their
AC/DC paper.)  The Polhemus 2" source on the Liberty is actually pretty
strong, 100 uT at 100 mm. Compared to my current 38mm square cylinder
simulation, I'm getting 200 uT at 100 mm and 1W.

The Patriot uses the same source at 1/5 the current, but even the Liberty
probably isn't running it very hot.  Also, they time-domain multiplex (AFAIK),
so the total power isn't as much as it would be for me.  They presumably
didn't include the longer range sources at short range because it would blow
their argument that their field is weak (whatever that is supposed to be
about, hypothetical ELF risks I guess.)  Are they actually confused about
about the effect of the geomagnetic field on pulsed DC?  Or just trying to
confuse their customers?  The Ascension field can't really be all that much
stronger because of the very limits that they discuss.  Magnetic media erasure
occurs at 16 KA/m, which is way stronger than the fields we're talking about
(at the sensor).  It's possible that magnetic media placed right on the
Ascension source might be erased.  The Ascension duty cycle is somewhat lower,
I guess, especially at lower measurement rates, so that keeps down the average
power dissipation, allowing a higher peak flux.  It could be that the
Ascension sources at full rate and full range do get quite hot (they use power
level control.)

Because of the open geometry, the effective permeability of the shape is quite
low (that ~3 factor), so the flux in the core is low by the standards of power
magnetics (maybe 60 mT), and core losses are negligible (microwatts according
to FEMM).  Also, error contribution from core nonlinearity and tempco are
small, and the initial permeability also has little effect as long as it is
greater than ~100.  This means we can use just about any ferrite whatsoever,
though a power mix would be the most obvious choice.

A low-budget approximation of the ferrite cube is to use three toroids wound
along the short axis.  The hole in the middle has very little effect on the
performance, and if the height is as little as 1/2 the diameter, which is
easily available, then the core helps about 2x.  A downside is that the coils
are no longer concentric, which would require compensation in the pose
solution.

Another cube approximation could be made from stacked pairs of E-cores.  If
necessary the ends could be capped with pieces of tile.  It's really only the
outer surface of the core that is doing any work.  I guess you could make a
hollow block from 6 tile pieces too.  That would be more of a win for a large
source, like 100mm, the tile size.  You wouldn't even need to cut them.  Just
make one end be the full tile size, then wrap four around the sides, leaving
triangular gaps at the corners in that direction.

Especially as we push down to lower impedances or reactive power > 100W, the
use of Litz wire becomes important.  Otherwise AC resistance can easily
dominate DC, even at 10 kHz.  There will also be some harmonic content due to
the PWM drive.
 ==> Not sure how much good this extreme thermal concept does.  The main
     problem is not so much getting the heat off of the coils as keeping the
     housing from getting too hot.  This is limited more by surface area of
     the housing than by thermal resistance of all the stuff in between.  I'm
     guessing that if you cast it in thermal resin, then there's not going to
     be 40 degrees C drop between coil and surface.  Coils are pretty good
     with running at 100 C, whereas the housing can't get much more than 60 C.
     It'd help a bit to cast fins into the housing, though that will increase
     the bulk somewhat.  Also it looks like for a 60 C case temp at 25 C
     ambient, the power is more like 5W unless we add fins or a fan or
     something, given 10 W/m/ degree K resistance on 5 exposed surfaces, 50 mm
     square.

Extreme thermal concepts: I think a good alternative to heat-pipe would be an
oil fill.  Oil will convect and maintain a far more homogenous internal
temperature than a thermally conductive potting compound.  As well as heat
transport, a benefit of oil is that I'm guessing it would highly effective at
noise damping, which is going to be an issue.  A minor advantage of oil fill,
is that the oil temperature is a pretty good measure of the temperature of the
entire assembly, which would be advantageous for temperature compensation.
 => silicone oil.  Non-flammable, inert, low viscosity

The housing needs to be non-metallic and reasonably thermally conductive, but
there are thermally conductive moulding compounds available that could be used
to create a housing.  Thermoplastics are available as shapes:
http://www.coolpolymers.com/shapes.html, and there are also epoxies that could
be used to cast a housing.  A housing could be made from reinforced resin
using classic "surfboard" techniques.  Carbon fiber has higher thermal
conductivity than fiberglass, and would be preferable.  See
http://compositeenvisions.com/raw-fabric-cloth-2/carbon-fiber-97/

Maintaining a stable output: feedback from a reference coil would definitely
be more accurate than current measurement.  The main thing we would be
compensating for (aside from driver effects) are thermal variations in the
source.  Controlling current would control for coil resistance changes and
driver effects, but not thermal dimensional change in the source or any
core-related effects.  We could control those by temperature measurement,
though, and current measurement does enable overcurrent shutdown.  And a
reference coil that is part of the source is going to have thermal effects
too, so it doesn't provide such a clear advantage.  At this power level,
current limiting doesn't reduce the shock hazard, so it gives mainly driver
short-circuit protection.  For safety, a ground fault sensor would be more
valuable.

]


Given I1, find I2 (RMS currents) such that (I1 + I2)^2 R = P
    I1 + I2 = sqrt(P/R)
    I2 = sqrt(P/R) - I1


Simulation notes:

On 6 June 13:

Source:
63 turns 33 ga x 40 strand litz, on 38mm dia x 38mm len ferrite cylinder.
Total current = 6 Amps
Voltage Drop = 0.958053+I*170.002 Volts
Flux Linkage = 0.00270566-I*1.48236e-006 Webers
Flux/Current = 0.000450944-I*2.47061e-007 Henries
Voltage/Current = 0.159676+I*28.3336 Ohms
Real Power = 2.87416 Watts
Reactive Power = 510.005 VAr
Apparent Power = 510.013 VA

Linkage/power = 941u

8 June: not sure how FEMM is reckoning fill, but 100% clearly isn't realistic
for Litz.  Actual wound dimensions of 33ga x 40:
  1.86mm width by 1.5mm height

Extrapolating for 33ga x 30:
  1.61mm x 1.30mm
Extrapolating for 33ga x 20:
  1.32mm x 1.06
Extrapolating for 33ga x 14:
  1.1mm x 0.89


72 turns 33 ga x 30 strand litz, on 38mm dia x 38mm len ferrite cylinder.
Total current = 4.5 Amps
Voltage Drop = 1.06793+I*167.743 Volts
Flux Linkage = 0.00266972-I*1.26457e-006 Webers
Flux/Current = 0.00059327-I*2.81016e-007 Henries
Voltage/Current = 0.237319+I*37.2763 Ohms
Real Power = 2.40285 Watts
Reactive Power = 377.422 VAr
Apparent Power = 377.43 VA
Linkage/power = 1,125u

Allowing for equal low-carrier current in power budget (treating as DC)
Note that DC/RMS current is 1.17A.
110 turns 33 ga x 20 strand litz, on 38mm dia x 38mm len ferrite cylinder.
In 30mm x 5mm WA, this is 5 layers of 22 turns.

Total current = 1.66 Amps
Voltage Drop = 0.905305+I*144.431 Volts
Flux Linkage = 0.00229869-I*1.10892e-006 Webers
Flux/Current = 0.00138475-I*6.68023e-007 Henries
Voltage/Current = 0.545364+I*87.0066 Ohms
Real Power = 0.751403 Watts
Reactive Power = 119.878 VAr
Apparent Power = 119.88 VA
Linkage/1.5W = 1.5m

There's a lot of wire buildup (20 mm, because two layers cross on all sides.)
Window can't be more than 3mm thick to stay inside 50mm.  Let's try 34mm x
3mm.  According to FEM our fill factor keeps getting worse, which may be why
we're diverging.  According to:
    http://www.hmwire.com/calculations.html
    33ga x10 = 0.774 mm, so:
158 Turns 33ga x 10 in four layers.  We actually aren't using the full WA
according to this reckoning.  FEMM says gets us back up to 40% fill, which is
where we were after the initial 40 strand build.  Their litz may be a little
tighter than mine, though the difference isn't big.

A RMS = 0.707

Total current = 1 Amps
Voltage Drop = 1.47876+I*168.976 Volts
Flux Linkage = 0.00268934-I*1.05483e-006 Webers
Flux/Current = 0.00268934-I*1.05483e-006 Henries
Voltage/Current = 1.47876+I*168.976 Ohms
Real Power = 0.73938 Watts
Reactive Power = 84.4881 VAr
Apparent Power = 84.4914 VA
Linkage/1.5W = 1.8m

Huh, interesting that we ended up at the same flux for half the power.  Or
really, less than half.  I guess for one thing, we reduced eddy current losses
by reducing copper, now only 33 mW.  With less current, losses elsewhere
should be a lot less too.  Our Q is 114, down from 176.  VA is way less too,
even allowing for moving part of the current to DC.  That seems a bit
puzzling, because the process of adding turns makes VA blow up.  Inductance is
way up, from 450 uH to 2.7 mH.  The overall size is smaller too, because of
the reduction in wire build.  And

### I wonder if the change in boundary condition is having an effect.  If so,
### who's right?  But even if we're off by a factor of two, it's quite decent.
### It does seem like the boundary is discouraging the flux from reconnecting,
### but it doesn't seem like the effect would be too big at 200mm.

Point: r=0, z=200
Flux= 0 Wb
|B| = 1.82541e-005 T
Br  = 0 T
Bz  = 1.82541e-005-I*3.16719e-009 T
|H| = 14.5262 A/m
Hr  = 0 A/m
Hz  = 14.5262-I*0.00252037 A/m
mu_r 1 (rel)
mu_z= 1 (rel)
J= 0 MA/m^2

But we really need to aim for more like 5W due to surface area, so:
174 Turns 33x10, 4 layer:

Total current = 0.73 Amps
Voltage Drop = 1.20012+I*149.598 Volts
Flux Linkage = 0.00238093-I*1.02804e-006 Webers
Flux/Current = 0.00326155-I*1.40827e-006 Henries
Voltage/Current = 1.644+I*204.929 Ohms
Real Power = 0.438044 Watts
Reactive Power = 54.6034 VAr
Apparent Power = 54.6052 VA
Flux/0.83W = 2.9e-3

Point: r=0, z=200
Flux= 0 Wb
|B| = 1.46749e-005 T
Br  = 0 T
Bz  = 1.46749e-005-I*2.8015e-009 T
|H| = 11.6779 A/m
Hr  = 0 A/m
Hz  = 11.6779-I*0.00222936 A/m
mu_r 1 (rel)
mu_z= 1 (rel)
J= 0 MA/m^


Cranking boundary out to 1m, flux at 200mm is 2% less, and basically no difference
in circuit paramters.

Point: r=0, z=200
Flux= 0 Wb
|B| = 1.43134e-005 T
Br  = 0 T
Bz  = 1.43134e-005-I*2.72799e-009 T
|H| = 11.3903 A/m
Hr  = 0 A/m
Hz  = 11.3903-I*0.00217086 A/m
mu_r 1 (rel)
mu_z= 1 (rel)
J= 0 MA/m^2



Huh.  It's seeming almost like no matter how little power I put in, I always
get the same flux?  What happens if we halve the wire size:
348 turns 33ga x 5
Total current = 0.355 Amps
Voltage Drop = 2.33448+I*291 Volts
Flux Linkage = 0.0046314-I*1.99975e-006 Webers
Flux/Current = 0.0130462-I*5.63309e-006 Henries
Voltage/Current = 6.576+I*819.718 Ohms
Real Power = 0.41437 Watts
Reactive Power = 51.6525 VAr
Apparent Power = 51.6541 VA

Point: r=0, z=-200
Flux= 0 Wb
|B| = 1.42832e-005 T
Br  = 0 T
Bz  = 1.42832e-005-I*2.72749e-009 T
|H| = 11.3662 A/m
Hr  = 0 A/m
Hz  = 11.3662-I*0.00217047 A/m
mu_r 1 (rel)
mu_z= 1 (rel)
J= 0 MA/m^2


Voltage is over our limit, of course, but flux is almost doubled?  But B at
r=200 hasn't changed, in fact it's dropped a bit.  So I guess the problem is
that flux linkage doesn't mean what I thought.  It isn't the magnetic moment,
whatever it is.  Maybe it needs to be normalized by the number of turns or
something. Or number of parallel wires?

[22 Sep 14]

Going for much lower impedance for COTS audio amp: 80 ohm.

109 turns 33x10:
Total current = 1.16 Amps
Voltage Drop = 1.15564+I*93.2873 Volts
Flux Linkage = 0.00148471-I*4.02421e-007 Webers
Flux/Current = 0.00127993-I*3.46915e-007 Henries
Voltage/Current = 0.99624+I*80.4201 Ohms
Real Power = 0.67027 Watts
Reactive Power = 54.1067 VAr
Apparent Power = 54.1108 VA

Uh, even lower:
55 turns 33x20:

Total current = 2.32 Amps
Voltage Drop = 0.583355+I*47.5033 Volts
Flux Linkage = 0.000756039-I*2.06788e-007 Webers
Flux/Current = 0.000325879-I*8.91326e-008 Henries
Voltage/Current = 0.251446+I*20.4756 Ohms
Real Power = 0.676692 Watts
Reactive Power = 55.1039 VAr
Apparent Power = 55.108 VA

Point: r=6, z=-200
Flux= 1.61529e-009-I*1.95663e-013 Wb
|B| = 1.42817e-005 T
Br  = -6.22345e-007+I*7.23877e-011 T
Bz  = 1.42681e-005-I*1.72851e-009 T
|H| = 11.365 A/m
Hr  = -0.495246+I*5.76043e-005 A/m
Hz  = 11.3542-I*0.0013755 A/m
mu_r 1 (rel)
mu_z= 1 (rel)
J= 0 MA/m^2


45 turns 33x20, with slightly larger core more accurately representing the
40mm cube I've got.  Max voltage with this amp is something like 42V, so
having a few turns less gives me the possibility of cranking it up to 11.  The
larger core boosts flux per watt, of course.

That's 302 uH.  As built, I measure 250 uH. I ended up with 2.6 layers, spread
over almost the full core width.  In FEMM the winding has larger margins.
Then there's the issue of approximation of the square section.  Possibly
effects due to the non-solid pieced together core.  Reasonably good agreement,
I would say.  Equivalent to 4.5 turns, or 10% of turn count.

Total current = 1.93 Amps
Voltage Drop = 0.468058+I*36.5424 Volts
Flux Linkage = 0.000581591-I*9.38712e-008 Webers
Flux/Current = 0.000301343-I*4.86379e-008 Henries
Voltage/Current = 0.242517+I*18.9339 Ohms
Real Power = 0.451676 Watts
Reactive Power = 35.2635 VAr
Apparent Power = 35.2664 VA

Point: r=4, z=-205
Flux= 6.87606e-010-I*3.50068e-014 Wb
|B| = 1.36893e-005 T
Br  = -3.88825e-007+I*1.85885e-011 T
Bz  = 1.36838e-005-I*6.96672e-010 T
|H| = 10.8936 A/m
Hr  = -0.309417+I*1.47923e-005 A/m
Hz  = 10.8892-I*0.000554394 A/m
mu_r 1 (rel)
mu_z= 1 (rel)
J= 0 MA/m^2



Pickup:

Simulated alone with field generated by boundary conditions (pickup_only.fem).
540 turns 40 ga, 12mm OD, wiring section 2mm x 3mm (r, z), 60% fill

With current to generate ciruit parameters:
Total current = 0.01 Amps
Voltage Drop = 0.496774+I*2.10017 Volts
Flux Linkage = 3.34252e-005-I*7.88471e-009 Webers
Flux/Current = 0.00334252-I*7.88471e-007 Henries
Voltage/Current = 49.6774+I*210.017 Ohms

So 3.3 mH and 50 ohms.

Open circuit:
Voltage Drop = 5.3427e-006+I*0.041957 Volts
Flux Linkage = 6.67766e-007-I*8.50316e-011 Webers

At flux:
Point: r=0, z=0
Flux= 0 Wb
|B| = 1.47721e-005 T
Br  = 0 T
Bz  = 1.47721e-005+I*2.02542e-009 T
|H| = 11.7552 A/m
Hr  = 0 A/m
Hz  = 11.7552+I*0.00161178 A/m

So 42 mV @ 14.8 uT and 10 kHz  k = V/B = 2.9e3

V=2*pi*f*N*A*B

N=570, A = pi*5mm^2 = 78e-6 m^2, so NA=44e-3.  Then omega=10e3*2*pi=63e3
That works out to 41.4 mV, which is excellent agreement.

Noise analysis, etc.

Don't have time to do a detailed analysis of the differential amp, and effect
of current noise, DC bias resistors at input, etc., but the LT1028 has 0.85
nV/rt Hz, voltage noise, and there are two in each LNA, and three LNA's per
sensor, so total noise about 0.85 * sqrt(6), or 2 nV/ rt Hz, or 46 nV. For 50
ohm coil, |n| (for 3-vector voltage) is 36 nV in 500 Hz.  (The vector signal
amplitude isn't increased by spreading it across three axes: the magnitude is
the same as the single-axis maximum.)  This is somewhat less than the
amplifier noise.  Total noise would be about 58 nV, which is -151 dB re. 2
Vrms (the ADC 0DB level).

Trying to figure out the relevant noise for the PCM4222. [We don't really want
to use this part because it has an internal reference.  We want a single
reference for all channels to get ratiometic operation.]  I think maybe THD+N
is easier to interpret, but we have to allow for the measurement bandwidth.
For example, at Fs 192 kHz, at -20dB input, THD+N is -91 dB in 80 kHz BW, but
correcting to 1 kHz bandwidth (assuming noise dominates) gives an additional
19 dB, or -110 dB, which is -130 dB re. full scale.  At -1 dB input, the THD+N
is -126 dB in 1 kHz BW.  But in the FFT plots the noise floor looks like -150
dB at 10 kHz.  If it's really that good, then we'd be better off without an
LNA at all.  Ah, and the full scale differential input is 5.6 Vpp, ~= 2 Vrms.

If the ADC dynamic range is DR, the LNA must have at least 151 - DR + 10 dB
gain in order for the ADC noise to contribute negligibly, which puts us
somewhere in the range of 11 to 35 dB.  At 26 dB (20x), this limits the peak
coil voltage to less than 140 mV peak, significantly less per carrier when we
allow for the superimposed carriers and add margin to avoid clipping.  If we
divide by 5, this would give 28 mV peak or 20 mV RMS.  Seems like we either
need to reduce the sensor gain or increase the range to justify the source
strength :-).

Our best-case SNR is then 20 mV/58 nV = 111 dB, which is 18.4 bits of
resolution.  This is 2.8 microradian RMS resolution, which would be 0.56
microns RMS at r=200mm.  So we meet our goal with room to spare as long as we
can achieve full-scale input at 200mm (and we can more than do this with the
12 mm sensor coil.)
[### re-check these numbers.  Something changed 4x in the last iteration.
 ==> keeps getting better?
]
 

On the other hand, if we reduced the LNA gain somewhat then we could make use
of higher fields, which would improve the maximum SNR.  Since the LNA voltage
range is almost 10x larger than the ADC input range, one possibility would be
a switchable attenuator at the LNA output, for example 5x.

Fully differential amplifiers are an interesting alternative to a LT1028
pair.  There are high speed RF/baseband ones, but the OPA1632 is designed for
high performance audio ADC drive.  It looks like it would have lower noise
than the LT1028 pair, and is designed for AC applications.  I notice that they
suggest putting 1K resistors in series with the input, which kind of defeats
the low noise, doesn't it?  I'm not sure whether these amps have a high input
impedance either, because we don't want to load down the sensor.  The input
structure looks like an inverting amp, but I'm not sure how it actually works.

Instrumentation amplifiers like the INA103 also have good noise, but only at
high gains.


440 turns 36 ga, 10mm OD, wiring section 2mm x 4mm (r, z), 90% fill

Total current = -3.75e-007 Amps
Voltage Drop = 1.8581e-006+I*0.0151608 Volts
Flux Linkage = 2.41292e-007-I*1.39225e-010 Webers
Flux/Current = -0.643446+I*0.000371268 Henries
Voltage/Current = -4.95494-I*40428.9 Ohms
Real Power = -3.48394e-013 Watts
Reactive Power = -2.84266e-009 VAr
Apparent Power = 2.84266e-009 VA

20 mm dia, 2x3 mm section, 300t 36 ga, 
1 mH, Z = 63 ohm @ 10 kHz, R = 124 ohm.
1.4 nV/Hz, 45 nV in 1 kHz
Flux at pickup center is 7 uT, at r = 200mm
[Real part of flux/current is inductance?  643 mH.  Yes.
AC resistance is real part of voltage/current?  5 ohms.  Evidently not.

At DC:
Total current = -3.75e-007 Amps
Voltage Drop = -6.88979e-006 Volts
Flux Linkage = 2.41293e-007 Webers
Flux/Current = -0.643448 Henries
Voltage/Current = 18.3728 Ohms
Power = 2.58367e-012 Watts

Ah, but at 1 mA DC, I get totally different numbers:
Total current = 0.001 Amps
Voltage Drop = 0.0183728 Volts
Flux Linkage = 2.10756e-006 Webers
Flux/Current = 0.00210756 Henries
Voltage/Current = 18.3728 Ohms
Power = 1.83728e-005 Watts

##########

Simulating pickup alone:
Total current = 0.01 Amps
Voltage Drop = 0.183728 Volts
Flux Linkage = 1.86617e-005 Webers
Flux/Current = 0.00186617 Henries
Voltage/Current = 18.3728 Ohms
Power = 0.00183728 Watts

Inductance via A.J integration closely agrees, 1.87 mH.  That's a lot more
plausible than 643 mH.



I think I may be running into issues with solver tolerance.  At least, numbers
related to current don't seem to be very stable.  Maybe should do the integral
method to find inductance, whatever that is.



[25 May 13:
Probably FEMM operator error, but I'm getting significantly different numbers
now for coil properties, though the noise density ended up approximately
right. Maybe I got the other numbers from integral analysis at AC?  L/R seem
easier to interpret from circuit properties in DC analysis.
    R = 21 ohm, L = 3 mH.

At 10 kHz, inductive reactance would be 188 ohm.  At these impedances
amplifier voltage noise is limiting, so it won't actually be possible to
achieve the coil Johnson noise limit.  Only the resistance generates Johnson
noise, but the current noise across the inductive reactance does contribute.
This means we can't directly use the datasheet tables for resistive source
impedance, but 1.5 or 2 nv/rtHz is about right for LT1028.

Of course finer wire is possible, but a pain to work with.  Q remains roughly
constant as wire gage as is changed, so relative contribution from voltage
vs. current noise won't change. (?)  The intrinsic SNR of the coil won't
change, but increasing both R and V may make a better match for the amp.
Since we're running the amp fairly close to its ideal source impedance, it'd
be hard to get as much as factor of 2 this way.  There is also the argument
that EMI may make it difficult to achieve this level of performance anyway, so
chasing that last bit of noise may not be worthwhile.  But for a sufficiently
small coil, thermal noise will dominate any amount of EMI.]


For 100k SNR, signal voltage is then 4.5 mV.

Let's say that we're going for a 40 mm outer dimension on the source, so the
coil OD is 30 mm dia and 5 mm thick.  That leaves some room for 3 coils and
the enclosure itself.  Then let's say the workspace extends 200 mm from the
coil center.  

If we can vary the output to keep the RX level at a good point then that helps
to ease the tradeoffs a bit.  We don't always have to run the source hot, and
we could run closer to the source without RX gain switching.

FEMM says that with this source coil at 0.7A @ 10 kHz we dissipate 1.8 W with
171 V drive, Q = 33 w/ 300t 28ga wire.  Then the open circuit voltage at the
pickup is 6 mV when r = 200 mm.  H is 1.2 A/m.  The corresponding B strength
at the pickup is 1.46 uT.  At 100 mm the signal is about 10x stronger.

The nonlinearity of the range response helps the instantaneous dynamic range
we need to achieve a given resolution.  It does also increase the total
dynamic range over the workspace, but that can be minimized by varying the
transmit level.

r = 200 mm
Voltage Drop = 8.06758e-006+I*0.00588961 Volts
Flux Linkage = 9.37361e-008-I*1.284e-010 Webers

r = 199 mm
Voltage Drop = 8.34633e-006+I*0.0060924 Volts
Flux Linkage = 9.69635e-008-I*1.32836e-010 Webers

That's a 3.4% increase in voltage for a 0.5% decrease in distance, almost 7x
better than linear.  So we do have some margin, which is good, because it
would be nice to have more like 2 um peak-to-peak rather than 2 um RMS (worse
than current ASAP res at light), and have some room for interence rejection.

[25 May 13:
See micron/research/tools/magnetic/B_vs_r.cfit

According to FEMM, response is approximately proportional to 1/(r^3), as is the
rule of thumb.  I fitted models using Matlab cftool.  I use 1/B as the weight,
since we are interested in the relative error, not absolute.  Otherwise we get
huge relative errors at the edge of the workspace.  I guess what we should
really weight by is the derivitive, dr/dB (or 1/(dB/dr)), as that give
constant radius error.

General model:
     f(x) = a/(x^3)
Coefficients (with 95% confidence bounds):
       a =   2.019e+07  (1.976e+07, 2.061e+07)

Goodness of fit:
  SSE: 144.6
  R-square: 0.9844
  Adjusted R-square: 0.9844
  RMSE: 1.26


Fitting for the exponent, I get:
General model:
     f(x) = a/(x^b)
Coefficients (with 95% confidence bounds):
       a =   6.349e+07  (5.011e+07, 7.688e+07)
       b =       3.256  (3.209, 3.304)

Goodness of fit:
  SSE: 59.15
  R-square: 0.9936
  Adjusted R-square: 0.9935
  RMSE: 0.8107


Though the value of fitting based on the simple FEMM model isn't clear, I got
a much better SSE with a nice crunchy residual by adding an exponential term.
The optimization was rather brittle, and the confidence bounds aren't terribly
tight.  The parameters have been scaled to be about +1, which helps.  Finding
the symbolic Jacobian would surely help.

General model:
     f(x) = (1e7*a)/(x^3)+(400*c)*exp(-(0.04*d)*x)
Coefficients (with 95% confidence bounds):
       a =      0.9785  (0.9355, 1.022)
       c =       1.065  (1.033, 1.096)
       d =      0.9078  (0.899, 0.9167)

Goodness of fit:
  SSE: 2.775
  R-square: 0.9997
  Adjusted R-square: 0.9997
  RMSE: 0.1766

Models of the form a/(r^b) + c/(r^d) did not seem to work well.


It isn't clear how much better these fits would work in practice than just
a/r^3.  Some of the residual may be due to the boundary conditions in FEMM.
One advantage of having a closed form is that we can find the derivitive for
sensitivity analysis.  The effects that these tweaks have on the derivitive is
quite small.  The derivitive is in good agreement with the perterbuation test
I did in FEMM.


There will be some current in the pickup due to capacitive load,
self-capacitance, if nothing else.  This is pretty small, though, because both
the voltage and the capacitance are small.  If we figure c = 1 nF, which seems
conservative, then Xc = 16k ohm @10 kHz, so I = 6 mV/16k = 375 nA.  This gives
hardly any effect in the vicinity of the pickup.  I haven't closely modelled
the effect of shorting the pickup (eg. transimpedance amp), but currents in
that range give clear effects out to a couple pickup coil diameters.
Depending on packaging, that might be acceptable in Micron.  O.T.O.H., it's
not clear what the advantage would be, since it doesn't seem likely the
self-resonance is going to fall near the carrier frequency.  I guess it would
minimize the slight phase shift that the capacitance is going to create, but
that phase shift isn't a big deal as long as it's small and stable, which I
think it would be.
 ==> I added a shunt capacitance in amp as diff mode filter, > 1nf, so current
     will be a bit higher. 
]


What about angular noise, and so lateral tip noise?  Does the resolution vary
with orientation?  

[25 May 13:

Maybe a bit.  The SNR is sqrt(2) less favorable at pi/4 on each channel, but
this may be offset by the equal contribution from both, vs. all coming from
the sine channel at theta=0.

See notes for drawing, but the angular resolution (in radians) approximately
equals the SNR.  One way to understand the superiority of angular noise
vs. translation, is that the magnetic sensor is dual to ASAP.  ASAP computes
angle from position, while magnetic computes position from angle.  The
tangential component of the position fix is a polar measurement, so the
relative contribution to tip noise from translation noise (at the pickup)
vs. rotation relates to the ratio of the pickup to tip moment vs. the source
to pickup moment.  The source->pickup moment is always going to be bigger.

The Polhemus resolution specs bear out my idea that the contribution from
angular noise should be significantly less than with ASAP.  That means that
the translation accuracy can be somewhat worse than ASAP and still achieve the
same resolution at the tip.  

I guess another possibility would be to abandon the dipole assumption and have
a large source, perhaps by using ferrite rods in a corner arrangement, so
that the rods partly enclose a quadrant of the workspace.  Dropoff is faster
near the source.

What is the relative contribution to position resolution from radial
vs. tangential noise?  
    A(r)= amplitude = k*B(r) = a*r^-3
    A'(r) = -3 a*r^-4
    Tangential noise = r/SNR
       = r/(A(r)/n)
       = r*n/A(r)
       = r*n/(a*r^-3)
       = (n/a)r^4

    Radial noise = n/A'(r)
       = -(1/3)(n/a)r^4

Tangential noise is 3x greater than radial, independent of r.  This is due to
the factor of 3 in B' vs B.  3D tangential noise has an additional degree of
freedom too, so that will increase the dominance of the tangential component
by sqrt(2).  The amplitude of each source vector is a vector sum of three
uncorrelated noises, but we also have three redundant amplitude measurements,
which seems to cancel.

So:
    Tangential/Radial = 3 * sqrt(2) = 4.2 (?)

Well, maybe that's not exactly right.  Would have to think about how the noise
on the 9 measurements contribute to the angular error.  Some averaging happens
there too.  It might be just 3, even in the 3D case.

Position resolution drops off faster than angular resolution.  In Polhemus
specs it is r^5.5 vs. r^4.5.  Exponential also fits their data well.  In my
understanding it should be more like r^4 and r^3, which is worrying.  0.5
could be due to dipole approximation, but that leaves another r factor.
Possibly this is a consequence of the adaptive noise filter, which must in
general exhibit a sort of noise instability, where as noise increases it
eventually becomes difficult to tell whether the sensor actually is static or
not, so the filter must either widen the bandwidth or slow down its adaptation
rate.


Eddy current compensation:

It seems like the high-low dual frequency approach would work, though 100x
lower frequency is probably too low for inductive pickup to get reasonable
bandwidth.  But somewhere between 10x and 100x should reduce eddy current
error by that amount, which is quite significant.  And it may be possible to
extrapolate the compensation.

NDI has some patents related to eddy current compensation in AC systems, but
they seem fairly different.  In other words, high-low may well be novel,
though I can't get too excited about doing a detailed search.  FDM high-low
isn't consistent with either Polhemus or Ascension product technology, so I
can see why this space might not have been comprehensively explored.

High-low is more compatible with the Polhemus AC approach, but in product they
still seem to be using no FDM and wedded to the idea of narrowband
transmit/receive.  Even so, they seem to have been aggressively patenting any
approach that comes to mind in order to discourage possible competitors.
Magnetic sensing also seems to have generated a considerable volume of patents
from others with no clear stake in the market, and with companies that seem to
have been exploring developing custom magnetic trackers for their proprietary
use. 

The original Ascension patent mentions the idea of using high/low rate
measurement to (statically) map the error, then apply this correction to
continuous high rate measurement.  This is an argument in favor of non-novelty
from the standpoint of defense against any other patents that may be out
there.  But due to their time-domain approach, Ascension can't
*simultaneously* make both low-rate low-error and high-rate high-error
measurements.

Ascension patent 6,172,499 says that patent 5,347,289 (Elhardt) uses a
dual-path approach, and is now lapsed.  Points out problems with crossover
glitches, but I'm not convinced those are fatal.  Perfectly complementary
filters are straightforward in DSP, not so easy in analog.

Polhemus has patents for eddy-current compensation involving "witness
sensors", which is clearly related to the error compensation benefits of a
reference pickup on the patient.  But the general idea of a matched reference
was in an earlier patent (that I have misplaced), and is anyway IMO pretty
obvious to one skilled in the art.

I think that the precise patent situation isn't crucial w.r.t. the proposal,
but it might be best to avoid citing patents still in effect.


Modulation scheme:

At 60 Hz power line, suppose we used 150, 210, 270 Hz low carriers, spaced
midway between line harmonics.  There's no getting around the fact that any
harmonics of the each low-rate carrier are going to splatter across all of the
high-rate channels.  This means that we want the driver to have pretty low
distortion at the low carrier frequencies, and low IMD.

We can go to lengths to minimize driver distortion, but the bottom line is
that we don't want low-carrier ripple on the high-carrier amplitude
measurement, and this can be guaranteed more effectively on the input
side. The distortion will be strictly repeating, so it would be possible to
accurately notch out just those frequencies.

The low carrier filter would use the same mechanism as the hum filter.  We
don't need a full adaptive noise canceller because we know the repeat rates.
We just keep a buffer whose length is the interference period (or a multiple),
and keep a running average of the demodulated signal, repeating at that block
size so the interference is in-phase, while the signal isn't.  Then each new
demodulated sample has the appropriate interference prediction subtracted from
it.  This is conceptually a high-order FIR filter, but no convolution is
needed, so the filter size doesn't affect the runtime.

How does this mesh with the idea of locating the carriers offset eg. 30 Hz
from the harmonics?  Note that there are two different sorts of likely
interference here, somewhat corresponding to hum and low carriers.

Hum is mainly additive noise, and it is also prone to noisy variation.  At the
high carrier measurement rate we can't resolve the spacing between the hum
harmonic and the carrier, even if there is 30 Hz spacing.  But if there is
that spacing, then because the interference is additive, it looks more or less
like the sidebands of a 30 Hz modulation on the carrier.  The amplitude is
similar (?sideband phase?) to that from landing square on the carrier, but the
frequency is different.  It seems it would be simpler to locate the carrier on
a hum harmonic, and then all the interference would appear at DC and 60 Hz
multiples.  This is partly a question of whether it's preferable to have
misadaptatation of the filter show up as 30 Hz buzzing or a mix of 60 Hz and
LF twitching.  I think the latter is preferable, especially when we consider
that the low carrier path is in a position to help us with respect to any
variation near DC.

For the low carriers, the most problematic interaction is intermodulation
between the high/low pair in the driver.  This will create sum and difference
sidebands, which show up as output ripple at the low carrier frequency.  It
doesn't matter what the low and high carriers are, the ripple will always be
at the low carrier frequency, since this is an intermodulation between the
fundamentals.  There can also be additive interference from low carrier
harmonics, and if these are at odd offsets from the carrier then the output
ripple will be at those odd frequencies.  Because the carrier amplitude is
stable (unlike hum) it is clearly preferable that the high carriers be common
multiples of all the low carriers, since it's not going to create any
twitching, and any ripple should be cancelled very well.

So far, I've just described the filtering of the demodulated high carrier in
the full rate output.  For the low carrier, performance is much more heavily
dependent on hum suppresion because the signal is much smaller and the nearby
hum harmonics are much larger.  In the shop I measured 120 uV p-p hum
(including spikes), which is roughly the same amplitude that the low carrier
would be at full range.  But the bandwidth of the low carrier demodulation is
well below line frequency, in the low Hz, or even below.  This means we *can*
resolve the low carrier amplitude as distinct from hum harmonics, and are not
even dependent on the notch filter's assumption of repetitive interference
(though the notch filter will only help.)

So the low carriers should be off hum harmonics, while the high carriers
should be on them.  If we make the low carriers be multiples of 1/2 hum (30
Hz) and the output rate be an even hum multiple (x18 = 1080Hz), then we can
make the hum filter length 1/30 sec (exactly 36 samples), and any effects
related to low carriers will also be canceled.  If the high carriers are in
channels 7, 9, 10, then the frequencies would be 9.72 kHz, 10.8 kHz and 11.88
kHz. The decimation factor should be a power of two for FFT (64x = 65,280
samples/sec).  The carrier pairs are then:
 <150 Hz, 10.8 kHz>
 <210 Hz, 7.56 kHz>
 <270 Hz, 9.72 kHz>

In this case, the high/low ratio is fixed at 72, but it wouldn't have to be.
The high carrier just has to be a multiple of both the low carrier and the
output sample rate.

So the whole scheme is based on half hum.  In the 60 Hz case,
 30 = 2 * 3 * 5
 18 = 2 * 3^2 
 150 = 5 * 30
 210 = 7 * 30
 270 = 9 * 30


The simplest way to demodulate the low carrier is to take the DC component of
the output-rate FFT and use that as the input to the low carrier demodulator,
but the low carriers have to be well below the 500 Hz Nyquist frequency.
Having low carriers that actually are low is the basis of the eddy current
compensation scheme, so keeping them below 250 Hz seems reasonable.
 
Since we want to smoothly update the overall position estimate at the output
rate, it doesn't seem to make sense to decimate the low-carrier signal before
demodulation.  This means that we're back to a point-by-point lock-in
amplifier simulation.  Note that while the low carrier amplitude drops
proportional to the frequency reduction, in order to buy back the lost SNR we
to have to narrow the bandwidth by the *square* of the reduction.  With 1 ksps
output (and 500 Hz bandwidth), to have the same noise on a 1 Hz bandwidth low
path, then the low carrier could only be reduced by sqrt(500), or around 22x.

But making the LF noise a bit higher would probably be acceptable, and given a
point-by-point implemenation of the slow path it's easy to tune the slow path
bandwidth at runtime according to different tradeoffs of LF noise
vs. tolerance to rapid changes in eddy current interference or to adjust the
high/low separation according to different tradeoffs of static eddy current
rejection vs. low path noise.  And we can also can make the relative transmit
amplitudes of the two carriers be whatever we please, which gives us more
wiggle room.  If, for example, we maintained the same total current, but
changed the amplitude ratio by sqrt(2), then we could lower the low carrier 2x
while keeping the path noises balanced, though at the cost of degrading
overall SNR by sqrt(2), decreasing range by 11%.  Or you could increase the
high carrier frequency to buy back the lost signal, or bla bla bla.  The
possibilities are endless.

The notch filter does theoretically distort the measurement by removing any
truly periodic motion components that fall in these notches, but the notches
should be well less than 1 Hz wide, and can be made arbitarily narrow at the
cost of slower noise adaptation.  (The frequency response of the coefficient
adaptation determines the notch width.)  This means that in practice it would
be hard to see any notch effects outside of a contrived engineering test.
Note also that the lowest notch will be at 60 Hz, which is a region were we
don't expect very large motion amplitudes.

We don't need to phase-lock our clocks to the power line because the low path
is basically immune to hum, and is in charge of output in the sub-Hz region
where any beating with line frequency would appear.

[15 June 13: 

For FPGA implementation it may make more sense to do the straightforward DFT
implementation, since we have plenty of cycles to do it the dumb way, it it's
much more thrifty with hardware.  We only need some 100 millions of MACs a
second, which is only a couple DSP blocks.  The main question is how you
structure it so the necessary data resources of current sum(s) and
coefficients are available  It also means that we can easily
process cycle-by-cycle, which is better for latency.  The FFT may have a
higher bandwidth, but you need to buffer the entire block, then *start* to
process.

With ASAP, we're demodulating 6x8 = 48 channels on the PC, and that's pretty
fast.  Sampling rates will probably be similar, but we'll be demodulating:
  (4 sensors + amp feedback = 5) * 3 axes * 3 carriers = 60

That's actually not more for magnetic.  I thought it was.  That's assuming we
demod all carriers on each amp feedback axis.  There *shouldn't* be any
significant off-axis signal, but if there is, it would be interesting to know.
Plus if we demodulate I and Q on all axes, then we have phase info.  Rather
than making phase-tweaked sine tables, as we do now, it might use less FPGA
resources (or be less coding) to just pass out the complex results and let the
software sort out the in-phase component.  This is really just a polyphase
decimating FIR filter, so there should be lots of guidance and IP for that.

Basically, each sample runs to a small fixed set of computations, since it is
destined for a certain slot in the data block.  
  2 window positions * 3 carriers * 2 IQ = 12

Then there's also the DC channel that we need for the low carriers, which is I
guess the windowed mean.  Is it real then?

Every sample (of 15 channels) that comes in on a given cycle is going to run
by the same 12 coefficients, and every one of those 12 * 15 = 180 MACs goes to
a different partial sum.  Then we have all kinds of time until the next
sample, 10.4 microseconds. 416 cycles, even at 40 MHz.  And when we're done,
we store half of the window for the next cycle, sum with the previous window
half, and we're down to 45 complex amplitudes, which can be scaled, maybe even
turned into floats, and passed out to Labview.

If we update the low carrier every Fd cycle, rather than further decimating,
then the low carrier demodulation actually requires about the same cycles as
the high carrier.

]




Source:

So far as increasing the SNR in general, we don't want to increase the
source diameter too much because then we get more into the aperture
nonlinearity region, but it would be possible to increase the amount of
copper.  If we increased the copper 4x to 10x10mm, then keep the turns the
same, the resistance is 1/4, but we can only double the current to keep the
power the same, because resistive losses go as I^2.  It's much more efficient
to increase the loop size, since resistance increases linear to r, but moment
as r^2.
 ==> That is, Q increases linearly with r.

Note that reactive power to achieve a given field strength at a fixed loop
area is independent of the number of turns.  Doubling the number of turns
quadruples the inductance, but we only need to drive half as much current
so the voltage only doubles, keeping VA constant.
 ==> Reactive power doesn't depend on amount of copper or wire size.

Dual to the "you can't win" SNR scaling law for wire size in an antenna, the Q
of a given air-core coil geometry (loop size and winding area) at a given
frequency is also fixed, regardless of wire size (ignoring eddy current copper
losses.)
 ==> A corrolary is that the losses to create a given field strength
     don't depend on the wire size.

A ferromagnetic core can significantly increase Q and emission efficency, but
unless you allow significant size increase the benefits are moderate (2-3x).
(For non-antenna inductors the Q benefit of a toroid core can be large.)  If
you add a core, you increase the inductance by the effective permeability
(requring higher drive voltage), but if you do increase the voltage to keep
current constant, then the emitted flux increases by the effective
permeability, with little increase in losses.

What changing the wire size does is change the impedance, and therefore the
voltage/current needed to drive a given field strength.  Bigger wire needs
more current at less voltage.  So for direct drive the wire size (and
corresponding turn count) should be adjusted to suit the driver impedance.

The energy stored in the magnetic field at a given amp-turns and loop area is
independent of frequency, but the apparent power (VA) is proportional to
frequency, both from the circuit perspective and also from the physical
understanding that modulating the field requires taking that energy in and out
on every cycle.  This means that in the high/low scheme it is crucial to
recycle energy at the high frequency (requiring a moderately high Q at that
frequency), but it is going to be both futile and unnecessary at the low
frequency because Q is low at that frequency (probably 1 or less.)  To get the
same field strength at either frequency, approximately the same amount of
power needs to be dissipated in the coil.

Q can be controlled by adjusting the winding shape.  Q increases linearly with
loop diameter (as does copper), while it increase as the square root of the
winding cross section.  So make the diameter as large as possible, then adjust
the cross section to get the needed Q.  It's somewhat preferable to make the
winding be a band (wider than it is thick) because this increases the mean
winding diameter for a given winding cross section.  This effect is only large
when the winding thickness becomes a significant fraction of the radius.  A
thin winding helps heat dissipation and proximity effect losses too.


Source driver:

[14 Mar 14: One possible issue is cross-coupling between the output channels
in the source.  If the coils aren't perfectly orthogonal, then there's
inductive coupling.  Some coupling will also happen due to nonlinearity in the
source core, which will create IMD.  These effects should be measurable, but I
don't know how significant they are.  Would make sense to choose core with
best linearity under operating flux and temperature.  Hysteresis is an issue
too, I guess.  Existing trackers don't have to worry about this since they
aren't running all coils at the same time.  A brute-force solution to reducing
source cross-coupling would be to go back to the non-concentric coils idea,
rods or air-core.

Inductive coupling could be cancelled using the current feedback distortion
control idea.  Induced voltage creates some potential across the coil so that
our voltage doesn't drive in the desired current.  Creates an additive
cross-coupling.  Induced voltage is less at the low carrier, but so is drive
voltage, so on balance the coupling from low carrier to low carrier would be
similar to high vs. high.  

Does it really matter?  Dunno. Cross-coupling between axes could create
apparent axis misalignment, but is itself caused by misalignment.  Not sure
whether it increases or decreases misalignment, but we need to calibrate for
misalignment anway.

Between-axis IMD (due to core) isn't too big a concern either, since the
second order spurs (at least) won't fall on any signal channels, and the
magnitude should be small.  Large cross-coupling in the driver could be a
problem, hence the preregulator idea.

A simple idea for dealing with driver error is to use negative feedback with
separate integral error for each sample in the 1/30 sec repeat period (only a
few thousand words per channel, not a big deal in an FPGA, even using onboard
block RAM).  We are computing the average error at that sample in the repeat
pattern, not the instantaneous error, so processing delay isn't an issue.
This would just be subtracted from the modulator input.  Since all carriers
repeat at that rate, this would deal with additive cross-coupling too.  I
thought this idea was in here was in here somewhere else, but didn't see it today.

]

In principle all you need is a 0.7A @ 180V amplifier.  Oops, that's 126W,
isn't it?

Clearly one of the annoyances of the dual-frequency scheme is how to drive the
source.  The LF and HF fields need to be about the same strength, which means
the amp-turns need to be about the same.  One thought is to use two similar
windings.  The HF winding can be resonated to overcome the inductive
reactance, but the LF winding will have such a low Q that it is basically
resistive.  The LF winding could be driven from a voltage source if it is
sufficiently loosely coupled, or it could be driven from a controlled current
source.  We do need the LF and HF fields to be fairly similar, which somewhat
limits how loosely the windings can be coupled.  If the LF winding is driven
from a voltage source then it will tend to spoil the Q of the HF winding, both
by reducing inductance and increasing losses.  If we have sine drive we don't
need Q for signal purity, but the losses are still annoying.

The driver should have fairly low noise too.  This may point away from using
commercial class-D audio amps, because they likely add dither to mask other
artifacts, and also won't be synchronized with our modulation, possibly
creating intermodulation effects within our passband.

Resonating helps with driver noise, though I'm not sure how big this effect is
at realistic Q levels.  Mostly it reduces the amount we emit outside of the
used channels, which is nice for EMI, but doesn't help resolution.  I guess it
may modestly help by eliminating adjacent-channel interference.  

One thing that may help with source noise is that it is common-mode in a
number of situations.  In particular, random amplitude fluctuations don't
affect the field vector direction (used to find orientation), so this noise
also only affects the radial component of the translation, whereas the
tangential component dominates.

It would also be possible to normalize out amplitude fluctuations using a
(high amplitude) reading from a reference coil in the source.

Having a reference sensor on the patient would also help, in that the
amplitude noise would be interpreted as change in the source->patient
distance, but not so much in the patient->tool distance.

It would be possible to create a custom switch-mode driver for a single coil
that creates both the high and low frequency fields (or any arbitrary signal).
Rather than continuously shuttling current between capacitor and inductor,
regeneration works by shuttling current to and from the bus in discrete
pulses.  This would probably be the ideal approach from the viewpoint of
source simplicity, and the great freedom of choice in modulation is also
quite attractive.

PWM could be done open-loop on a cycle-by cycle basis with digital (FPGA)
generation.  This would insure synchronization, and noise would also be low as
long as the supply rails are kept quiet.  The PWM duty variation comes from
the desired output voltage, which is (to first order) the derivitive of the
desired current.  Really we need an LR model of the output, or I guess we
could easily use a transfer function from Spice AC analysis to create a
shaping filter.  Since the stimulus is repetitive it's easy to phase-shift
things to get a causal implementation.

The supplies for the separate source channels could be isolated by using
per-channel linear sub-regulators to prevent the carrier-frequency ripple from
making it back onto the shared bus.  These sub-regulators could also be used
to bring down the rail voltage for low output levels so that the PWM duty
cycle doesn't become too low.  A very low duty will increase the relative
content of harmonics in the output, and may also create quantization noise due
to the PWM resolution.  At 100 kHz PWM, we only have about 9 bits resolution
with 10 ns clock period.  With the inverse cubic range response, we may want
to vary level 100x or more.  If we only bring down the bus voltage when the
mean current is already fairly low, then losses in the sub-regulators won't be
much.  The sub-regulator doesn't need to have a low output impedance, in fact
ideally it would act more like current source, not passing ripple at the high
carrier and above back onto the main bus, but rather allowing the regeneration
capacitor to absorb it.  This could be done using an N-type current source
referenced to the sub-rail, even just a source follower with a big RC on the
gate and source degeneration.  Sub-rail variation during the fast carrier
cycle will create distortion, but we don't care too much about the high
carrier distortion, since those products are out of our passband.  But we want
the sub-rail to be stiff at the low carrier frequency to minimize distortion
of the low carrier.  Stiff is relative, since we're talking about a fairly
high impedance load.
 => 14 Mar 14: The concern is with distortion causing apparent amplitude
    modulation in the high carrier.  This will create low-carrier ripple in
    the output. In the frequency domain this can be seen as intermodulation
    creating sidebands, but the idea of gain compression is perhaps more
    intuitive.  If the supply droops when we are at a low carrier peak, this
    will cause a decrease in high carrier amplitude.  As noted elsewhere, this
    can probably also be controlled well by filtering this ripple out in the
    receiver, since there is no actual signal at those frequencies.  The only
    other effect of the distortion is slight gain error which can be corrected
    by the level servoing feedback.

It may make sense to for the sub-regulators to be switching regulators with a
DCM boost or flyback topology, so that they can step up a moderate supply bus
(24V) to the high voltage needed.  This keeps the high voltage more contained,
and eliminates the need to find an off-the-shelf 200V supply, as well as
eliminating concern with power dissipation in the sub-regulator.  For best
efficiency you'd use a flyback transformer, but you could use a stock inductor
for the boost.  The sub-regulator could be driven by FPGA PWM too, though
you'd want hard overvoltage and overcurrent shutdowns.  Or they could be
analog regulators synchronized to the sampling rate.  I guess you could also
get away with just one regulator, since the output diode will prevent the
regeneration currents from flowing between channels (though there would still
be some interaction, because current would be steered to whichever channel
happened to be lowest.  You would regulate the minimum channel bus voltage.
Basically what's going to happen is that each channel is only going to draw
supply current when its bus is bottoming out, which happens at peak output
current.  Even when the ripple is sinusoidal, it's going to cause some
harmonic distortion.  The crunchies at the bottom of the ripple will magnify
this a bit, but (as noted), harmonic distortion of the high carriers isn't a
major concern.  Second order IMD between high carriers is also not a huge
problem, because the products fall elsewhere.  The biggest concern is IMD
between high and low carriers, which can be minimized by keeping the bus stiff
at the low carrier freqiencies.

You would want feedback from current sensing to keep the source level
constant at the timescale of the measurent rate.  This would compensate for
changes in Q or bus voltage.  This measurement would be on a signal where the
PWM-rate ripple is filtered out, but the carrier is recovered fairly
accurately, and demodulated digitally, in the same way that the sensor signals
are.  This would eliminate the need for a reference pickup in the source.

If we make sure our excitation is balanced, then DC current in the source will
be self-limiting, because the IR drop due to the DC current creates a voltage
that opposes the current.  Balanced excitation is intrinsic in full-bridge
drive as long as the switching waveform is symmetrical.  So we don't need DC
feedback, but we'd like the response to be flat between the low and high
carriers.

This feedback measurement would be basically the only thing determining the
source noise level, so it should be a low-noise measurement.  The regulation
bandwidth can be considerably below the high carrier frequency, though, which
reduces the noise requirement.  We do care about noise at the low carrier.  It
is possible that an acceptable tradeoff between noise and loss might be found
for a sense resistor, especially with a high bus voltage, but the current
needs to be measured at the bridge output (so we measure the circulating
current), which is awkward with a sense resistor.  Probably a current
transformer is the highest performance solution, since losses can be
minimized, and in comparison to GMR, etc., its closed magnetic geometry
prevents hum pickup.  The current sense should be after the output filter.
With a current transformer, we can measure just the differential mode current
by running both leads through (in opposite direction.)  A common-mode CT could
be used for fault detection (ground fault interruptor.)

Since the excitation is repeating, it would also be possible to tweak the PWM
cycle-by-cycle to minimize waveform distortion, not by using fast feedback,
but by using the averaged waveform across multiple datablocks.  It's not clear
what use this would be, though, since all the high carrier harmonics fall
outside our passband.  Probably our amplitude servo loop would run at the
input block rate (and output rate, 1 ksps), which would result in some
feedback at the low carrier.  This is a feedback system, of course, so the
loop does need to be compensated.  Since the response from amplitude parameter
to current is nominally flat, we'd use an integral gain.

On the generation side, the FPGA would have tables at the PWM rate of a full
block of each high carrier and a cycle of each low carrier.  These would be
added up according to adjustable gains (based on feedback) and used to drive
the modulator.  While it wouldn't be hard to put the output shaping into the
FPGA as an IIR filter, it actually isn't necessary because the model is
linear.  We can pre-shape the waveform tables (which, as long as they are
fixed frequency sines, is just gain/phase shift.)  As noted, we don't need to
control DC explicitly (because full bridge drive is balanced), so the only
feedback is this multiplicitive scaling of the carriers.  Since the SNR is
high in the feedback, even at the low carrier, we can update the low carrier
amplitude several times a cycle, but the correction factor is computed as
scaling rather than as an offset.


In the switch-mode driver, the desirable bus voltage would determine the coil
impedance, since the peak coil voltage needs to be below the bus voltage.  The
bus voltage doesn't need to be particularly low, though.  A medium-high
voltage up to 200V is expedient for the electronics because it reduces
current, with numerous benefits, notably reducing the size of a high-Q energy
storage capacitor. (e.g 4.7uf polypro @ 250 VDC.) Keeping the coil turns count
up also reduces the relative emissions from the driver and lead-wire vs. the
coil itself.

How high does the PWM rate need to be?  The cleanest frequency-domain
result would come from modulating at a multiple of the carrier frequency, as
this would insure that all artifacts due to PWM are harmonic.  PWM
could be at the input sampling rate, or any convenient multiple.  This would
insure that PWM has no effect on the measurement.  Some low-pass
filtration at the output will be necessary to minimize E-field EMI emission,
and this will also reduce higher harmonics in the H-field.  

The H-bridge needs to have +, - and 0 output voltages, but the half-bridges
don't need an "off" state, because when we're holding current constant we
would drive both ends low, shorting the coil, and allowing the current to
recirculate with minimum droop.  Since the coil always has current (except at
zero crossings) it is meaningless to leave the end floating.

The carrier periods don't have to be multiples of the input sample period (and
PWM rate), so PWM cycles won't fall at the same place on each cycle in a
block, which results in the cycles looking slightly different, but we can't
actually observe this intermodulation because the PWM cycle is synchronous
with the input sample rate.  With only five PWM cycles or so per high carrier
cycle, the emitted waveform is going to be pretty crunchy, but I think it
really doesn't matter.  After the waveform is digitized, it looks that way
anyway.
[14 Mar 14: The current waveform will have alternate ramp and flat sections.]

Unlike in the case of a SMPS, there is not much pushing us to higher
frequencies, because the main magnetic is the coil, which is fixed.  Using the
lowest possible PWM rate (the input sample rate) will reduce switching losses
and EMI.  However, we do have output filter inductors, and since we want the
output filter corner set below the PWM rate, a low PWM pushes us toward larger
filter inductors.  We may want double or triple the PWM rate to reduce the
demands on the output filter.  A PWM of 100 kHz to 200 kHz puts us well within
the sweet spot for switch-mode MOSFETs.

The switching voltage waveform at the output will excite the coil's
self-resonance.  The coil is effectively shorted, but a higher resonance still
appears in the H-field output.  For example, with the prototype coil and
driver, after I undid my slew limiting, there is a pronounced ringing at 350
kHz, while my note says the SRF (open circuit, presumably) is 220 kHz.  The
ringing is approx 20x down from the output at the 5 kHz resonance, and clearly
visible on the scope, which I guess is why I had the slew limiting.  Since we
need a much higher switching rate with the PWM approach, we can't limit the
slew rate that severely without unreasonable losses.  The situation can be
helped by placing the coil SRF safely between PWM harmonics, which prevents
the self-resonance from gaining energy across multiple PWM cycles.  Since the
SRF is in the same ballpark as the PWM frequency, this seems a plausible coil
design consideration.  But even an isolated step creates ringing, so the
output filter should significantly suppress PWM harmonic power at the SRF.

I guess that though you can reduce the size of the output filter inductors by
pushing the corner frequency up, it would be more sensible to set the corner
at least an octave below the PWM frequency.  As well as attenuating EMI
better, it also avoids the problem of the PWM causing the output filter itself
to ring.  The output filter inductors would be toroidal (to minimize
emission), likely stock powdered iron components. This is pretty similar to a
SMPS output filter application, though our "DC" current is varying at 10 kHz.
Powdered iron is naturally lossy in the 100's of kHz unless exotic RF
materials are used, which is o.k. as long as the loss is low at the high
carrier frequency.  Because of the 1/r^3 dropoff of field strength, emission
from the driver isn't a large concern.
[16 Sep 14]
 => From class-D audio app notes, it seems the typical approach is to set the
    output filter cutoff about an octave below the PWM frequency, and allow it
    to be under-damped.  Low freq avoids ringing due to PWM, and moderate to
    high Q minimizes power dissipation.  Main concern is that modulation must
    be limited to frequencies well below the filter resonance so that the
    output isn't resonantly pumped up to high voltages when the load itself is
    low loss.  In audio, driver resonances are typically at low frequencies,
    and are largely resistive in the ultrasonic range, which would tend to
    damp the resonance anyway.  For us, the resonance is a concern, but we can
    just decline to excite it.



[14 Mar 14: 

Switching servo amps for motors don't normally use such heavy output filtering
(if any).  The coil sees a wave which is decidedly square.  It is true that
motors are designed with a closed magnetic geometry, rather than as emitters.
But a more typical approach would be to set the output filter corner well
above the PWM rate, and rely on filter damping to prevent ringing in the
filter.  I guess one point is that servo amps tend to use lower PWM
frequencies to reduce losses, since (as for us) the output inductance is built
in, and there is no benefit to moving to higher freqencies that would allow
smaller inductance.  Even the sample rate with no multiplier (~70 kHz) is a
high chopping rate for motor drive.  Once you realize you are filtering the
edges and not the PWM fundamental then it is clear there is no EMI advantage
to moving to a higher PWM (even keeping the same filter), since that's just
more edges.

The filter has to do far more work if you're going to try to apply quasi-DC to
the source coil.  For one thing, you're dealing with much more ripple current
as well as having a lower corner.  You could do that, but I don't think it's
necessary for EMI, and is also at odds with the need for the output to
alternate at the high carrier rate, maybe a decade below than the PWM
frequency.  That output capacitor is going to be shunting the signal and
reducing the coil resonance.

A shielded source cable would be a good idea to keep down the E-field emission.
]

We want the output filter to have a high Z_0 so that the switching transients
don't create unreasonable AC currents in the LC components.  This means a high
inductance (while the "DC" current is fixed), pushing us toward larger
inductors.  This relates to the observation that our design load impedance is
high, perhaps 100 ohms.  Note that for tank design we are primarily concerned
with the PWM and (to some degree) the high carrier.  The high carrier
determines the interaction between coil design and driver impedance.  The low
carrier is DC, creating I^2R loss in the switches and coil, and VI loss in the
sub-regulator, but note that the power loss due to the low carrier is
approximately the same, even though the VA is much less.  We drive the same DC
current into the coil as AC, but at a much lower voltage.  This means that the
current out of the bus is much less than coil current, the reduction being
approximately the coil Q.  The bus VA is of course real watts, so we can
derive the bus current from voltage and design constraints.  If we limit the
power per channel to 7W (21W in all), then at 150V bus, the current is 47 mA,
quite low indeed. It might be, given the relatively low power, that we want to
push down to a lower impedance.

Hmmn, I guess that at least so far as exciting the SRF goes, that is actually
a differential mode problem, whereas EMI is more of a common-mode problem.
Need to do some simulations of the output filter and the coil, and look at
class-D amplifier application notes.



GMR sensor?

1.2 A/m = 14 mOe, even for NVE AAH002 (highest sensitivity) output would be
~200 uV.  This is 30x less even though the voltage noise is 10x more.  A 300x
hit is going to be pretty hard to absorb.  And due to the 1/f noise
dominating, the LF supremacy of the GMR sensor is also not too exciting.  We
only lose one order of magnitude SNR instead of two by dropping from 10kHz to
100 Hz, but at that rate we'd still be 30x better with the inductive pickup at
100 Hz.  The issue of needing bias for bipolar response is also a drag, and the
nonlinearity can be a problem for frequency domain, especially if we are doing
fast/slow modulation.  The loose specs on sensivity and the temperature drift
are also problematic for any kind of accuracy.  To be fair, 20mm is a pretty
big coil, but 10mm only changes the picture 4x.

It seems like the noise advantage for GMR vs. inductive really only
materializes for extremely small sensors, such as in a sensor linear
array. Ah, but if you use lock-in amplification then you can get out of the
1/f noise regime, even for DC measurements, because the sensor is ratiometric.
At that level of implementation complexity the superiority vs. fluxgate
seems somewhat elusive, but if you cut the coil to 10mm and used lock-in, then
at 100 Hz coil vs. GMR would be a wash.  At 10 kHz it would still be ~100x
worse, though.
]


Structural and mass materials:

One criterion for the effect of nonferromagnetic conductors on AC fields is
the skin depth.  If the thickness is well less than the skin depth, then there
is not much effect on the field.  Skin depth is proportional to the sqrt of
restivity, inversely to root of frequency.  Stainless steel resistivity is
approx 40x higher than copper, but this still gives a skin depth of ~0.5mm
[[***wrong, more like 4.2mm]] at 10 kHz.  Using a metal snout on the handpiece for
mass would likely result in detectable field distortion, and other large
chunks of metal (manipulator base, housing) would need to be avoided.

[14 March 14:

Still true, given that material even well less than skin depth will have
effects that are "large" from our perspective of precision measurement.  We'd
like chunks of metal to be several diameters away from the sensor, and
certainly don't want a metal shell wrapping around the sensor.

The "one skin depth" rule applies to sheilding, which is not exactly the same
thing as tracker interference.  One reason is that a "small" sheilding effect
may still be a big tracking error.  I'm still slightly puzzled here, because I
manged to convince myself that eddy current distortion would increase more
slowly than linear when thickness is above the skin depth.  But this is also
the very same range in which the conductor becomes effective in blocking the
flux.  I guess that's the point, though, isn't it?  Below the skin depth it
can't exclude the field, above it can.  Additional thickness only further
affects the field insofar that it creates a larger volume of field exclusion
(?).  I would still expect the eddy current to increase, though, since flux
derivitive has increased, and resistance has only increased by sqrt().
External field modification would still increase.

Also the depths for real aluminum alloys and copper are higher because of
higher resitivity.  e.g. 6061-T4 is 4.3 uOhm/cm, = 1.04mm, or about 2mm if you
double the thickness (as for wire).  The rule of thumb for sheilding (>one
skin depth) is based on a box with overall 2x thickness (so far as the
external field is concerned.)  Influence does depend on shape: a ring creates a
larger field disruption than a ball of the same mass.


Machinable ceramic (mica/glass) has almost the density (2.2 g/cm^3) of
aluminum.  Another possibility for weighting is to use finely divided tungsten
based materials (powder or shot). Tungsten is approximately twice as dense as
steel, ~8x as dense as aluminum, somewhat denser than lead.  Tungsten carbide
is nearly as dense as metallic tungsten, but has somewhat higher restivity
(not high as stainless.)  Pure tungsten has low permeability, but alloys and
composites may have modest permeabilities of perhaps 6 due to nickel and iron
(or cobalt in cemented tungsten carbide.)

It would be possible to make a plastic housing with rapid prototype or
machining, then add tungsten shot or tungsten filled epoxy to recesses.

See http://www.tungsten-heavy-powder.com/ There are machinable tungsten
alloys, some of which are non-magnetic.  Tungsten powder filled resin can
exceed 10 g/cm^3.  They say that it's non-conductive.

300 series (304, 18/8) stainless is largely non-magnetic, though work hardening
may give it modest permeability (perhaps 6).


References:


Classen11 is interested in stabilization for orthopedic bone cutting, and
identify the one of the problems we have with commercial products, which is
that the latency is too high for closed loop control.  They propose fusing
inertial with a commercial optical tracker.

Welch02 also has a box discussing latency problems, primarily from the
viewpoint of VR head tracking.

raab79 has authors from Polhemus, which is one of the two main vendors of
magnetic trackers.  Anything in this paper is safely off-patent by now.
Ascension (now owned by Northern Digital, makers of the Optotrak) uses a
different pulse modulation principle which they say has greater immunity of
conductive metal effects.

There is a great deal of patent literature on magnetic tracking, using all
feasible combinations of fixed and moving coils, co-located or not.  US6073043
contains a review of some of these patents.  Untypically for a patent, the
text is human-readable.


Requirements for Micron closed loop control:
  Two 6DOF measurements, 1 kHz update rate, 1ms latency max.  We also want
  resolution at least as good as ASAP, though characterizing that is a bit
  complex due to the mix of contributions from angular and position noise.
  The size of the tip noise distribution in the 3DOF paper was 6 microns RMS
  (36 p-p) in 1 kHz bandwidth.  The noise at the light was 0.74 um RMS.
  Because the control loop bandwidth is less than the full measurement
  bandwidth, the tip noise caused by measurement noise is approximately 3x
  less than the measurement noise.

The Polhemus FasTrack would have adequate resolution at 12", but it only reads
120 samples/sec (divided by number of sensors), and the latency is too high (4
ms) and the pickup coil is way too big.  Likely they also use a ferrite core,
which would make two pickups interfere with each other.  It does seem that the
FasTrack resolution specs bear out my intuition about angular vs. position
resolution.  In ASAP the tip-referred noise is mainly due to angular noise,
whereas with the FasTrak the noise ratio is approximately reversed.

There is a fundamental difficulty with respect to interference rejection as
the measurement rate is increased.  I'm sure the use of the 120 Hz rate is
partly motivated by rejecting power-line harmonics.  Repetitive and impulse
noise filtering would likely be necessary to get acceptable performance.  The
modulation should be synchronized to the power line.  I guess the carriers
should be not be placed on a line frequency multiple.

On a sample-by-sample basis this isn't particularly helpful, because you'd
have to observe for many samples to be able to resolve the frequency
separation between the carrier and the hum, also the full measurement
bandwidth necessarily includes many hum harmonics.  What it would do, however,
it let you precisely resolve any purely repetitive hum components without
being confused by the carrier presence.  If we placed carriers midway between
hum harmonics, then the max separation would be 1/2 the line frequency (30 or
25 Hz), so the hum model couldn't be updated more frequently that that.
Having a roughly matched reference pickup would be useful for measuring the
hum field.  Moving pickups will see varying hum pickup, especially w.r.t. to
orientation, so it would be useful to have some idea of the fixed hum field so
that it can be subtracted out in a vector sense according to the approximately
known moving coil orientation.  In general, having a disturbance measurement
is quite valuable for noise cancellation.


Could also consider magnetic/inertial hybrids.  In the HF regieme inertial
sensors are relatively favorable.  You could do inertial + LF magnetic in the
handle, and then some local measurement of manipulator position.  This could
be based on a local magnetic principle, with onboard source, but avoiding
interference between the two is likely to be rather challenging.  Another
possibility would be some sort of capacitive or optical internal measurement.
That's a lot of crap to put in the handle, though.  The idea of using inertial
to measure the output motion seems kind of perverse, but given the
availability of compact multiaxial MEMS sensors it isn't completely insane.


The plan A would certainly be to press ahead with the all-magnetic 12-DOF
approach, and just assume that interference filtering is going to be a big
part of the job.  What we don't want is to be sending a lot of hum out to the
actuators.

Note that if you have a matched reference coil, then insofar as the
interference field is uniform you can get rejection without relying on any
assumption of periodicity.  Possibly the reference coil can be head mounted,
serving the dual function of gross head motion measurement (w.r.t. the fixed
source) and precise relative motion measurement of Micron w.r.t. the head.
This could help to get the reference coil close to Micron, where the fields
are more nearly identical.  

One way to look at this is that the purpose of the source is to stir up the
magnetic field to make sure there's enough info, but especially from the
viewpoint of orientation measurement we don't entirely care who's contributing
to those field vectors we are measuring.  To measure translation we do have to
make some assumptions about the field gradient, but error in the gradient due
to hum contribution is going to be a second-order effect.  We can also
estimate the hum field at the handpiece at a fairly high rate, and so have an
idea of any hum gradient.

Note also that for pose stabilization we aren't that sensitive to scale (or
general nonlinearity) and offset errors, since our goal is to drive the tip
back to where it was.  We are somewhat sensitive to the derivitives, since
that affects what direction we go in and how fast, but small errors will only
have a small effect on dynamics.

It seems it would be worth looking at solutions to the differential pose of
two pickups, rather than finding the poses w.r.t. the source and then
subtracting.  This could be used to find both the output -> head transform and
the handle -> output transform


RF vs. ELF:

Most of what I found related to RF localization is for wireless sensor nets or
indoor navigation for robots, firefighters, etc.  There is also a literature
on trying to localize using RF for capsule endoscopy.  To some degree with
sensor nets, and particularly with the capsule endoscopy, the choice of RF is
being driven primarily by the practical consideration that there is already an
RF link.  See for example Pahlavan12, attached.

Strictly speaking, AC magnetic sensors are not entirely disjoint from RF,
since it's all electromagnetic.  The question is what advantage is gained by
operating at a higher or lower frequency.  Antennas are definitely smaller at
higher frequencies, and the situation with respect to interference from active
sources (lightning, power line harmonics improves.)  However interference from
the propagation environment deteriorates as frequency increases.  That is, you
start having to deal with field distortions from conductive media and
eventually reflections (multipath.)  Although we are not an inside-body
application, we need to operate accurately near the patient's head and the
surgeon's hands.  One approach that as been proposed for the RF capsule
endoscopy is to measure only the magnetic field, as that is less distorted by
the body.  But for best penetration you want the wavelength to be long,
preferably much greater than the measurement distance.

Once you get into the far field (more than a wavelength away) the signal
strength drops off as r^2 rather than r^3 or r^4 (in the near field).  This
makes RF better suited to long-distance applications (many wavelengths), but
also reduces the usefulness of Signal Strength (SS) for determining range.  At
RF a different localization approch becomes reasonable: measure distance by
Time Of Arrival (TOA), as opposed to just SS as used in magnetic sensing.
However there is acute difficulty in achieving micron scale accuracy using
time measurements.  :)  To first order, you have to consider the distance
resolution to resemble the wavelength, which would push us out of the RF
regime entirely, and back into optical.  Using interferometric techniques
(Maroti05) you can get resolution to a small fraction of the wavelength,
though at the cost of lower bandwidth and some complexity.  Ultra Wideband
(UWB) (see Gezici05) is another approach to better resolution, simply to
moving to high frequencies.  Either way, a "high" resolution is in the
centimeter range.


Existing magnetic trackers:

In contrast, the Polhemus Liberty magnetic sensor actually provides adequate
position and angular resolution for our purposes, but the pickup is too big,
the update rate too low (240samples/sec) and the latency too high (4 ms).
Its specs bear out my intuition that with the magnetic approach has
significantly different properties than ASAP for angular vs. position
resolution.  In ASAP the tip-referred noise is mainly due to angular noise,
whereas with the Liberty the noise ratio is approximately reversed.

The standard Fastrak operating frequency is 12 kHz, and options from 8-14 kHz
are available for concurrent operation.  The channel spacing is 2 kHz, which
would be enough room for three carriers (at their 120 s/s output bandwidth.)
The precise carrier frequencies are not a 60 Hz multiple.
 => I was surprised to find that in Raab79, the Polhemus system described is
    based on a single carrier and time-domain multiplexing.  This is
    somewhat simpler on the hardware side, and perhaps they tune the
    pickup coils.  They also use this scheme where the coils are
    proportionally excited so that the vectors are aligned with the last
    tracked orientation.  I'm a little fuzzy on the virtues of that.  I'm
    guessing it was just to make computation a easier (in '70s tech), but
    means that you can only track one pickup at a time.  There might also be
    some virtue with respect to mitigating the effect of various
    approximations, core nonlinearity, etc.  Kuipers80 says that having the
    source vectors track the pickup is advantageous for field distortion
    calibration.  I notice that in Liberty they lost the restriction that
    speed is divided by number of sensors, but FasTrak still had it. Of course
    time-multiplexing is highly unfavorable for high-rate measurement.

Other Polhemus patents mention the advantage of having a narrowband response
for "noise" reasons.  I'm guessing they mean EMI, and this supports the idea
that they've used tuned sensors. 

Ernest B. Blood, author on Raab79 (then at Polhemus) is CTO of
Ascension.  The basic Ascension patent is 1990, so also expired.  The
Ascension technology relies on DC field sensing.  Evidently their pickups are
fluxgate sensors.  The DC technology is certainly simpler on the transmit
side, and the receive signal processing in the '90 patent is pretty minimal
too.  I can certainly see why line interference is a big issue for them, given
the DC sensitivity and lack of any frequency-domain selectivity.  It sounds
like recent versions use DSP filtration.

While Ascension may have chosen the DC technology partly because it was
clearly outside Polhemus patents, the eddy current interference issue with AC
trackers does seem to be serious, given the amount of patent activity
generated related to that.  On the other hand, there also seems to be a lot of
patent activity related to the theory of position solution which is not quite
so convincing.  Is an iterative solution really that bad?  A good bit of this
is off-patent too, and could be mined related to the issue of accuracy near
the source.

The eddy current time constant of a 1" dia aluminum rod seems to be around
1-1.5ms at room temperature (can be seconds in liquid helium!).  In a long
cylindrical bar, tau = 2.17e-9 * mu r^2 / rho, tau = time constant, mu
permeability, rho resistivity, r bar radius.  If you need to wait multiple
time constants, then with aluminum you're going to be having a big problem
with ksps measurement rates, since the DC approach requires time-domain
multiplexing of the excitation (3 axes.)  The '90 patent does mention fitting
an exponential decay to the measurement, which would eliminate need to wait
multiple decay times.

Polhemus patents start quoting use of a "witness" sensor for eddy current
compensation around '02.  There was some other patent that used a reference
coil primarily to compensate for source level variation which I'm having
trouble finding again.

The brute-force way to deal with eddy currents in an AC approach is to push
down to lower frequencies, which was my original idea.  But I didn't realize
how low the Polhemus frequencies already were.  10 kHz seems to be the
nominal operating frequency for AC trackers with induction pickup.  By using
DC-responding sensors (like GMR) we could push down to a lower frequency, but
it would be hard to push the modulation frequency below the readout rate, and
even approaching that is going to forego some of the EMI rejection benefits of
AC, both in reducing the selectivity and in moving into a region where
interference is higher.  But the interference situation is no worse than the
pulsed-DC approach, and probably better.

With frequency domain multiplexing, with high dynamics there does start to be
an issue of the nonlinear response (to distance) distorting the dynamic
signal, creating harmonics of the motion, and therefore sidebands of the
actual dynamic signal.  These sidebands could create adjacent channel
interference.  It'd be best to leave an empty channel between carriers, though
my guess is that this effect is small, mainly because for us the dynamic
signal is such a small portion of the channel power.  Possibly another issue
with FDM is that the eddy current distortion varies depending on frequency,
making the axes somewhat less comparable.

Polhemus and Ascension numbers should be taken with a grain of salt.  IMO
neither company is particularly exemplary in saying in their literature how
any given number is measured, though Ascension does have a paper on this that
seems reasonable.

Their accuracies are RMS, not max (as for ASAP).  Also, their accuracy is over
an unclear workspace, probably larger than the 300 mm one that I picked.

Ascension is more or less lying about their update rate, which may be why they
don't like to talk about latency. They may update the fix that often, but it
seems that the data will be correlated over a three fix window.

Ascension also doesn't like to feature the resolution spec (noise), perhaps
because they look bad.  This number was extracted from their response to
Polhemus' r.e. comparing AC and pulsed DC trackers.

Both Polhemus and Ascension have a rate-adaptive noise filter ("DC adaptive"),
and their resolution specs are most likely not at full bandwidth.  Latency
specs do not reflect group delay caused by the adaptive filter.  The filter
adapts within a given range, and despite its frequency domain implementation,
neither vendor reveals the correspondence between the parameters and
bandwidth.  Inferring from the Ascension manual, the default "alpha min" is
0.02, which would correspond to a 1 Hz bandwidth at the default 240 s/s output
rate.  That seems reasonable based on our experience with cancellation
filters.

In the Polhemus "teardrop mini" sensor, you can see the ball shape of the
sensor.  This is presumably an air-core sphere, both because of the low
density and also because a sphere doesn't make sense with a ferrite core.

For speed and resolution NDI Aurora is worse than Polhemus or Ascension, and
accuracy is comparable.  It's most interesting for discussion of eddy current
compensation, and their paper can also be cited for methodology.  NDI has a
true 40 samples/second, and accuracy is similar to Liberty.  Noise floor seems
even worse than Ascension.  In [19] they give a noise density (30 microns/rt
hertz), but distance isn't specified.  That would be 670 in 500 Hz.  Though it
is an AC system, it has eddy current compensation, and performance is more
similar to Ascension.  In the paper they show a rather large tetrahedral
source (presumably air-core), but have since changed to a more compact source
(presumably ferrite core).  Their system seems to be optimized for shorter
range use.  Their graphs only go up to 600 mm, because RMS total error then
exceeds 1 mm RMS in 1 Hz.  They also give a minimum range (50 mm).  High noise
may be from weak drive due to not using resonant drive, because of multiple
frequencies per channel used by their eddy current compensation scheme.


For Micron, we're primarily interested in the inner part, where there is
single-digit-micron resolution, but in fig. 1, the resolution at 500mm is 20
microns RMS, and 500mm is the Aurora max specified range.  With EMTs, there is
no hard edge to the workspace, noise performance just keeps deteriorating at
r^4, and then suddenly it becomes noticeable, and is a problem.  The ILEMT
source is as powerful (for its size) as Polhemus, and this is their standard
source size.  The sensor is presumably also as sensitive as their sensor.  So
we should be able to get at least as good noise performance as them at any
range, and better dynamic behavior due to FDM and simply being optimized for
latency.  If there's any interest, it would be easy to add a larger source for
ILEMT and get longer range.  The driver will be quite capable of driving large
sources to high levels.

My current take on it is that, despite the marketing literate and stats
emphasizing speed and latency, neither Polhemus or Ascension has really tried
hard for speed, which is a niche within a niche.  This may be partly because
both of their technologies are up against a wall, and they haven't been willing
to undertake any significant departure from their initial technology.  This is
more obviously the cause with pulsed DC, where higher rates forgo eddy current
compensation, and rapid axis switching creates increasing problems with high
drive voltages.  With Polhemus, it seems they aren't using FDM, and that
creates a speed limitation, because each time you switch axes for TDM it takes
tens to 100 cycles for the carrier to build up to full amplitude, and you have
to do that three times a measurement cycle.  At 240 samples/sec, they only
have 14 cycles per axis, which probably isn't enough time for the carrier to
reach full amplitude, let alone sit there for a while.  In fact, now that I
run these numbers, that seems pretty problematic.  If they really are still
using TDM, then they're really squished down into that corner.

So there are technical difficulties with increasing speed, it would cost more,
and most customers wouldn't notice.  They're mostly running with the default
adaptive filter settings, where their bandwidths are in the 1-20 Hz range.

It's also notable that Polhemus, despite all of their patent activity related
to eddy current compensation, does not claim to have any compensation in their
current product, and therefore presumably doesn't.  Although Polhemus seems to
have a bunch of smart people thinking about trackers and filing patents, as
far as I can tell the operating principle of their current trackers is almost
identical to the original concept.  So either all their ideas don't work, or
management has decided not to pursue them.  I kind of get the impression that
Polhemus management isn't all that great.  It is hard to do eddy current
compensation without a multi-frequency source, and maybe they don't want to
take that hit.



Performance characterization:

It's hard to hear myself saying this, but I think the best of the tracker
accuracy papers (Frantz) is lacking in rigor related to the possibility and
nature of extreme outliers.  They also seem rather keen on disregarding the
outlier values, on the basis that max isn't robust. They don't report
min/max.  The use of 95% confidence bounds is rather suspicious, too,
especially that given in fig 1(c), that seems to be the place where the central
distribution gives way to fat tails.  In ASAP I used the 99.9% percentile to
characterize the peak-to-peak resolution (as my own way to sweep impulse noise
outliers under the rug.)

I think the "not robust" argument is wrong, especially as to characterizing
static accuracy.  The argument is basically that the maximum value grows
without bound (fails to converge) as you increase sample size, so the number
is meaningless.  But in my experience, for example sampling interpolation
errors in the ASAP LUT, the max does converge, and rather rapidly.  I think
the argument of failure to converge depends on a truly white
(non-band-limited) noise process, which is non-physical (has infinite power.)

The "not robust" argument also poo-poos the user's legimite concern about
extreme outliers, sweeping them all into the 95%'th percentile.  If the
maximum truly does fail to converge as sample size increases, then that's
saying something bad about the measurement system.

I think part of the problem is the unwarranted assumption that the error
surface across the pose space is a "statistical" object at all.  As long as
averaging is sufficient to minimize time-domain noise, measurements should be
repeatable and local linearity should be good.  When you're trying to estimate
integral non-linearity, that's a rather benign problem.  INL is a good
starting point, because some work has been done on measuring INL using
statistical methods.  It's also interesting to characterize the DNL.

The business of determining extreme values in a high-dimensional space is
clearly related to the global optimization problem, but is somewhat more
tractable, because you can guess at the presence of what might be "out there"
without actually knowing where to find it.
[14 Mar 14: I think what I'm referring to is that you can see the extreme
tending to an asymptote is the search effort increases.  But optimization
techniques ("pessimization") seem like a much more efficient method of finding
extreme values that random search.]

There's obviously some correspondance between the resolution of the extreme
and the sample size.  For example, it would be silly to talk about the 95'th
percentile when you only have five samples.  If you're seeking to estimate the
start of the top 1/e outlier bin, then your sample size must be >>e.

Of course it requires some sort of induction to infer the error of all poses
without testing each one of them individually, but this really is rather
plausible given the smooth (if nonlinear) nature of the underlying error
sources.  Although having obvious appeal, and worthwhile as a sanity check,
random samples of the pose space are probably not the most efficient way to
get at extreme values.  Instead, it would be better to look at the calibration
residuals for source and sensor, and test-test repeatability of these
measures, and then look at the sensitivity of the pose to these errors.

There may also be relevant statistics such as extreme value theory where you
can infer the likelyhood of coming across significantly worse bad values by
peeking at the data.  Intuitively, as you add samples, the sample max
converges toward the true maximum.  Sensitivity can be investigated by
derivitives (which we need anyway) and also by monte-carlo analysis.


Signal processing:

It can't be a new idea, but it just occurred to me that you can estimate noise
with any frequency using fixed size FFT.  The bin centers are all
synchronously sampled, and will average out fine.  The issue is with
frequencies that fall off a bin center.  The idea is to sort of do a
phase-lock loop for each bin, using the FFT phase for a phase detector.  This
will probably work best for low frequencies.  It's another Kalman filter like
thingie.  As well as the complex response (including current phase) we also
estimate the phase increment per FFT block.  This should fall in the +/- 180
range.  Frequencies on the center have shift of 0 (or 360).  The phase
increment implies the precise off-center frequency.  Given this shift we can
predict the next FFT block, and IFFT that to generate an interference model.

It's not clear this would work as well as time-domain averaging for the known
line frequency repeat components, but it could predict any frequency.  The
time domain average also has the advantage of giving time domain variance for
each sample, which is what we want for weighted demodulation.  The main
advantage of a frequency domain approach (as for the distortion filter) is
that we can notch out the known carrier frequencies.  For the time domain,
we'd probably rely on using the time-domain carrier model to subtract the
carriers before averaging.  I guess we could also run the time domain
interference model through a comb filter with the carriers in it.  This is
really only an issue for the high carriers, since they are on hum harmonics.
The low carriers will tend to smear out in the averaging.  Since the averaging
filter is a delay line, we can take the notch tap right out of it, at +/- the
carrier period from the current output.
 ==> This is an evolution of an idea that didn't work.  Don't know about this
     idea.


Impulse filter: Subtract averaged carrier from signal, then highpass filter
with small FIR near 20 kHz. Subtracting most of carrier reduces demands on
high pass.  Filter brings out impulses from actual carrier modulation and
lower frequency noise.  Threshold to detect impulse, then zero DFT weights in
small surrounding window.  Tell caller impulse stats.

For periodic impulse, DFT weight is inverse of variance (or root?) at that
position in 10 Hz repeat.

For DFT, use carrier model to get phase.  Rotate complex result and take real
part.

We really do want to use FFT for low carrier.  The easiest way to make sure
low carrier is not corrupted by sporadic impulse is to just drop that low rate
sample (duplicate previous).

Probably useful to update low carrier faster than full block rate.  This
reduces latency and avoids undesirable step changes.  Whatever the low carrier
bandwidth, we want the update rate to be at least 10 Hz.  This would work
together with overlapped windowing.

Could suppress impulse noise in frequency domain by exploiting its broadband
character.  This would be for high carrier, where there is no hum, so we have
lots of unused bandwidth that usually has a consistent level of noise.  We
just watch the unused channels, noticing the mean power.  If there is a big
jump, subtract that amount from the signal bins.  This is indifferent to the
exact time domain structure, whether a single impulse or a burst of digital
hash.  This could be done in addition to the time domain impulse filter.  This
jump in the noise floor can also be passed through to the block level
measurement noise for input to Kalman filter.

Some interference is well localized in frequency.  This can be rejected by
frequency domain techniques.  Impulses are spectrally broad, but by time
frequency duality are well localized temporarily.  This suggests time domain
approaches.

Detection of sporadic impulses implicitly relies on motion dynamic model.  At
10 mm/s, max motion possible in 1 ms is 10 um, and with acceleration limits
even smaller.  Periodic hum impulses also an issue.  Best approach here is not
to rely on precise prediction, since amplitude is variable, but to find sample
locations that are highly variable in a 10 Hz repeat.

A better solution than repairing the time domain signal by interpolation is to
modify DFT to weight samples according to relative SNR.  This works for
periodic and sporadic impulses.  That is, in the limit, just ignore the
sample, dropping it out of the block sum, but keeping the reference phase
unchanged.  Our sample noise estimate can be based on both the sample
innovation and on a periodic noise estimate.

Adaptive noise cancellation naturally fits the Kalman filter innovation model.
The state can be either a FFT block or a time signal.  For repetitive impulse,
time domain approach is more obvious.  Update hum model at low carrier sample
rate.  Notch and low high carriers out of the FFT, inverse transform, then
collect per sample statistics in 10Hz repeat.  For the deterministic
component, add this to the noise model to subtract out, but also compute
variance.  This can be used to weight DFT, and we can also compute a block
level measurement noise which can be used in Kalman filter estimating the high
carrier amplitude.

Use constant acceleration KF with acceleration and velocity limits.
Eventually rate limits and process noise should be adaptive to recent motion.
KF also fuses low rate estimate.  Eventually this is a 6DOF KF.

There should be a window on low rate FFT, and we can use overlap if we update
at a higher rate than the bandwidth.  This gets around the problem with non
overlapped windows that the data is not fully used because a given sample sees
only a single window coefficient.  Chebyshev, or maybe a custom FIR filter.
Acceptable to broaden central lobe somewhat in order to avoid being confused
by gross motion.

Also need gross motion detection in order to inhibit filter adaptation in
response.  This is a different kind of transient, affecting low rate.  Since
we demodulate high carrier first, we can detect gross motion directly from
that.  As with high carrier, monitoring of low rate noise floor can be used
for measurement noise estimate in KF.

Noise floor estimate comes from total noise power level across FFT bins that
have been identified as noise.  Some nuance in determining noise floor bins.
Simplest would be all non-carrier, but this will include hum and any other
phase coherent interference.  These peaks can't affect our result except when
base broadening due to time variation overlaps our bandwidth.  We don't want
to estimate noise based on channel bandwidth though, since this discards most
info and also risks mistaking actual motion for noise.  The simplest way to
select noise bins is to take the bottom quantile according to average power,
masking out the actual channel bandwidth.

For low rate noise estimate, we only consider bins that are reasonably near
the carrier.  Bins far outside the channel bandwidth are not representative.
It is ok for the noise estimate to be somewhat noisy.

High rate noise estimate needs to be done using high rate FFT, so there is not
a lot of resolution.  We probably want to subtract the carrier model before
FFT so that we can use levels from the adjacent guard channels.  With guard
channels there is just not all that much room for three carriers in the 20 kHz
bandwidth.  Bins above the antialias cutoff are not representative.  We could
use just the two adjacent guard channels.  Can do the same weighted DFT on
these guard channels, rather than FFT.  That way our noise floor estimate
isn't corrupted by the identified impulses, but will still reflect sub
threshold disturbances.

Driver idea: get around the potential problem with direct digital PWM of
having limited resolution, especially at low duty.  Use a current output DAC
to drive a timing integrator.  Higher codes give shorter pulses.  In order to
allow arbitrarily short pulses, use digital delay at output so that the
leading part of the integration time up through the shortest possible is
blanked out.  This has the interesting property that duty is the reciprocal of
digital code, which means we have the highest resolution for the shortest
pulses, and low codes would be unusable because of too low resolution and
eventually for generating too-long pulses.  You could increase the usable code
range by adding a fixed current, not just codes with the high bit set.  Max
duty would be limited by the blanking period, but could exceed 50%.  Or
alternately, as would make more sense with two state PWM, reduce or eliminate
the blanking and accept a minimum duty.  You can also use hybrid PWM timing
with coarse digital and then a fine analog trim.  That would risk creating
non-monotonic output, and starts to seem complicated.

Note that you don't need arbitrarily low duty to get LOW output, what it does
is limit the max output, since you're always partially pulling the other way.
That would be fine for us, since we don't have to be able to generate big peak
power.  We have to be able to run at full output all day, and we can increase
bus voltage a bit to make up.  Our PWM rate could be on the lower side, so we
can use higher voltage non audio FETs.

Three state PWM would be more efficient, I think.  Have fixed start time for
high side and low side, but when neither is on, then the input to the output
filter (paired inductors) is shorted to ground.  In two state mode we never
have any time where the output current isn't flowing either into or out of one
of the rails.  This creates higher average ripple on the output filter
inductors and the bus capacitors, increasing losses there.  Should also reduce
output ripple.

Instead of using a single low carrier, it might be interesting to use as
multi-sine waveform.  This would allow more detailed frequency response, and
would not require any more power from the source, especially if substantially
higher components are included.  Regression could be used to estimate
frequency responses from a curvilinear family without requiring high SNR at
any given frequency.  For example 250, 500, 1k, 2k Hz (but not actually a
precise harmonic series). This would mean we wouldn't be so dependent on any
single low carrier, which might be crapped on by a motor, or wherever, and
would open the possibility of updating the metallic interference model at a
higher rate.  Think frequency hopping spread spectrum, but without any actual
hopping, though we could abandon noisy channels.
